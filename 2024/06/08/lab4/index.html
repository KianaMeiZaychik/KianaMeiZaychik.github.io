<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/cr32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/cr16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"Available values":"tabs | buttons","style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="感知机简介感知机（英语：Perceptron）是Frank Rosenblatt在1957年就职于康奈尔航空实验室（Cornell Aeronautical Laboratory）时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。 Frank Rosenblatt给出了相应的感知机学习算法，常用的有感知机学习、最小二乘法和梯度下降法。譬如，感知机利用梯度下">
<meta property="og:type" content="article">
<meta property="og:title" content="基于前馈神经网络的姓氏分类">
<meta property="og:url" content="http://example.com/2024/06/08/lab4/index.html">
<meta property="og:site_name" content="Lilin">
<meta property="og:description" content="感知机简介感知机（英语：Perceptron）是Frank Rosenblatt在1957年就职于康奈尔航空实验室（Cornell Aeronautical Laboratory）时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。 Frank Rosenblatt给出了相应的感知机学习算法，常用的有感知机学习、最小二乘法和梯度下降法。譬如，感知机利用梯度下">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image.png?raw=true">
<meta property="og:image" content="http://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-1.png?raw=true">
<meta property="og:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-2.png?raw=true">
<meta property="og:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-4.png?raw=true">
<meta property="og:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-7.png?raw=true">
<meta property="og:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-10.png?raw=true">
<meta property="og:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-5.png?raw=true">
<meta property="og:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-11.png?raw=true">
<meta property="og:image" content="http://example.com/%5Cphoto%5Cimage-6.png">
<meta property="og:image" content="http://example.com/%5Cphoto%5Cimage-12.png">
<meta property="og:image" content="http://example.com/%5Cphoto%5Cimage-13.png">
<meta property="og:image" content="https://github.com/PaddlePaddle/awesome-DeepLearning/raw/master/docs/images/deep_learning/loss_functions/CrossEntropy.png">
<meta property="og:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-15.png?raw=true">
<meta property="og:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-16.png?raw=true">
<meta property="og:image" content="http://example.com/%5Cphoto%5Cf144f-2019-06-19-juanji.gif">
<meta property="og:image" content="http://example.com/%5Cphoto%5Cimage-14.png">
<meta property="og:image" content="http://example.com/%5Cphoto%5C3fd53-2019-06-19-chihua.gif">
<meta property="og:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/c1a6d-2019-06-19-quanlianjie.png.webp?raw=true">
<meta property="og:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/a8f0b-2019-06-19-lenet.png.webp?raw=true">
<meta property="og:image" content="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596033.png">
<meta property="og:image" content="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596054.png">
<meta property="og:image" content="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596071.png">
<meta property="og:image" content="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596080.png">
<meta property="og:image" content="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596142.png">
<meta property="og:image" content="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596131.png">
<meta property="og:image" content="http://example.com/%5Cphoto%5Cimage-17.png">
<meta property="og:image" content="http://example.com/%5Cphoto%5Cimage-18.png">
<meta property="og:image" content="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596120.png">
<meta property="og:image" content="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596110.png">
<meta property="og:image" content="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596102.png">
<meta property="article:published_time" content="2024-06-08T06:00:00.000Z">
<meta property="article:modified_time" content="2024-06-08T07:44:27.004Z">
<meta property="article:author" content="闲云">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image.png?raw=true">

<link rel="canonical" href="http://example.com/2024/06/08/lab4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>基于前馈神经网络的姓氏分类 | Lilin</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Lilin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/06/08/lab4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/Christina.jpg">
      <meta itemprop="name" content="闲云">
      <meta itemprop="description" content="雄关漫道真如铁 而今迈步从头越">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lilin">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          基于前馈神经网络的姓氏分类
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-06-08 14:00:00 / 修改时间：15:44:27" itemprop="dateCreated datePublished" datetime="2024-06-08T14:00:00+08:00">2024-06-08</time>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>33k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>30 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>感知机（英语：Perceptron）是Frank Rosenblatt在1957年就职于康奈尔航空实验室（Cornell Aeronautical Laboratory）时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。</p>
<p>Frank Rosenblatt给出了相应的感知机学习算法，常用的有感知机学习、最小二乘法和梯度下降法。譬如，感知机利用梯度下降法对损失函数进行极小化，求出可将训练数据进行线性划分的分离超平面，从而求得感知机模型。</p>
<p>感知机是生物神经细胞的简单抽象。神经细胞结构大致可分为：树突、突触、细胞体及轴突。单个神经细胞可被视为一种只有两种状态的机器——激活时为‘是’，而未激活时为‘否’。神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激活，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。</p>
<p>在人工神经网络领域中，感知机也被指为单层的人工神经网络，以区别于较复杂的多层感知机（Multilayer Perceptron）。作为一种线性分类器，（单层）感知机可说是最简单的前向人工神经网络形式。尽管结构简单，感知机能够学习并解决相当复杂的问题。感知机主要的本质缺陷是它不能处理线性不可分问题。</p>
<h2 id="单层感知机"><a href="#单层感知机" class="headerlink" title="单层感知机"></a>单层感知机</h2><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>单层感知机的模型可以简单表示为：<br>$$ f(x) &#x3D; sign(w*x+b) $$<br>对于具有 $n$ 个输入 $x_{i}$ 以及对应连接权重系数为 $w_j$ 的感知机，首先通过线性加权得到输入数据的累加结果 $z$：$z&#x3D;w_1 x_1+w_2 x_2+ … +b$。这里 $x_1,x_2,…,x_n$ 为感知机的输入，$w_1,w_2,…,w_n$为网络的权重系数，$b$ 为偏置项（$bias$）。然后将 $z$ 作为激活函数 $\varPhi(\cdot)$ 的输入，这里激活函数 $\varPhi(\cdot)$为 $sign$ 函数，其表达式为：</p>
<p>$$<br>sign(x) &#x3D;<br>\begin{cases}<br>+1 \qquad &amp; x \geq 0 \\<br>-1 \qquad &amp; x \lt 0<br>\end{cases}<br>$$<br> $\varPhi(\cdot)$会将 $z$ 与某一阈值（此例中，阈值为$0$）进行比较，如果大于等于该阈值则感知器输出为 $1$，否则输出为 $-1$。通过这样的操作，输入数据被分类为 $1$ 或 $-1$ 这两个不同类别。<br> 其结构为<img src="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image.png?raw=true" alt="alt text"></p>
<h3 id="单层感知机存在的问题"><a href="#单层感知机存在的问题" class="headerlink" title="单层感知机存在的问题"></a>单层感知机存在的问题</h3><p><img src="http://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-1.png?raw=true" alt="alt text"><br>单层感知机可被用来区分线性可分数据。在图中，逻辑与(AND)、逻辑与非(NAND)和逻辑或(OR)为线性可分函数，所以可利用单层感知机来模拟这些逻辑函数。但是，由于逻辑异或（XOR）是非线性可分的逻辑函数，因此单层感知机无法模拟逻辑异或函数的功能。</p>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><h3 id="多层感知机的结构"><a href="#多层感知机的结构" class="headerlink" title="多层感知机的结构"></a>多层感知机的结构</h3><p>由于无法模拟诸如异或以及其他复杂函数的功能，使得单层感知机的应用较为单一。一个简单的想法是，如果能在感知机模型中增加若干隐藏层，增强神经网络的非线性表达能力，就会让神经网络具有更强拟合能力。因此，由多个隐藏层构成的多层感知机被提出。</p>
<p>多层感知机由输入层、输出层和至少一层的隐藏层构成。网络中各个隐藏层中神经元可接收相邻前序隐藏层中所有神经元传递而来的信息，经过加工处理后将信息输出给相邻后续隐藏层中所有神经元。<br><img src="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-2.png?raw=true" alt="alt text"><br>在多层感知机中，相邻层所包含的神经元之间通常使用“全连接”方式进行连接。所谓“全连接”是指两个相邻层之间的神经元相互成对连接，但同一层内神经元之间没有连接。多层感知机可以模拟复杂非线性函数功能，所模拟函数的复杂性取决于网络隐藏层数目和各层中神经元数目。</p>
<h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前L-1层看作表示，把最后一层看作线性预测器。 这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。 下面，我们以图的方式描述了多层感知机<br><img src="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-4.png?raw=true" alt="alt text"><br>这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。因此，这个多层感知机中的层数为2。注意，这两个层都是全连接的。每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><h4 id="RELU函数"><a href="#RELU函数" class="headerlink" title="RELU函数"></a>RELU函数</h4><p>函数定义：</p>
<p>$$f(x)&#x3D;\begin{cases} \begin{matrix} 0 &amp; x&lt;0 \end{matrix} \\ \begin{matrix} x &amp; x\ge 0 \end{matrix} \end{cases}$$</p>
<p>导数：</p>
<p>$${ f }^{ ‘ }(x)&#x3D;\begin{cases} \begin{matrix} 0 &amp; x&lt;0 \end{matrix} \\ \begin{matrix} 1 &amp; x\ge 0 \end{matrix} \end{cases}$$<br>函数图如图所示：<img src="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-7.png?raw=true"><br>导函数图如图所示：<img src="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-10.png?raw=true" alt="alt text"></p>
<h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>函数定义：</p>
<p>$${ f }(x)&#x3D;\sigma (x)&#x3D;\frac { 1 }{ 1+{ e }^{ -x } } $$</p>
<p>导数：</p>
<p>$${ f }^{ ‘ }(x)&#x3D;f(x)(1-f(x))$$<br>函数图形如图所示：<img src="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-5.png?raw=true" alt="alt text"></p>
<p>导函数图如图所示：<img src="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-11.png?raw=true" alt="alt text"><br>优点：</p>
<ol>
<li>$sigmoid$ 函数的输出映射在 $(0,1)$ 之间，单调连续，输出范围有限，优化稳定，可以用作输出层；</li>
<li>求导容易；</li>
</ol>
<p>缺点：</p>
<ol>
<li>由于其软饱和性，一旦落入饱和区梯度就会接近于0，根据反向传播的链式法则，容易产生梯度消失，导致训练出现问题；</li>
<li>Sigmoid函数的输出恒大于0。非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢；</li>
<li>计算时，由于具有幂运算，计算复杂度较高，运算速度较慢。</li>
</ol>
<h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><p>函数定义：</p>
<p>$${ f }(x)&#x3D;tanh(x)&#x3D;\frac { { e }^{ x }-{ e }^{ -x } }{ { e }^{ x }+{ e }^{ -x } }$$</p>
<p>导数：</p>
<p>$${ f }^{ ‘ }(x)&#x3D;1-f(x)^{ 2 }$$<br>函数图形如图所示：<br><img src="/%5Cphoto%5Cimage-6.png" alt="alt text"><br>导函数图如图所示：<img src="/%5Cphoto%5Cimage-12.png" alt="alt text"><br>优点：</p>
<ol>
<li>$tanh$ 比 $sigmoid$ 函数收敛速度更快；</li>
<li>相比 $sigmoid$ 函数，$tanh$ 是以 $0$ 为中心的；</li>
</ol>
<p>缺点：</p>
<ol>
<li>与 $sigmoid$ 函数相同，由于饱和性容易产生的梯度消失；</li>
<li>与 $sigmoid$ 函数相同，由于具有幂运算，计算复杂度较高，运算速度较慢。</li>
</ol>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><h4 id="均方差损失（Mean-Square-Error，MSE）"><a href="#均方差损失（Mean-Square-Error，MSE）" class="headerlink" title="均方差损失（Mean Square Error，MSE）"></a>均方差损失（Mean Square Error，MSE）</h4><p>均方误差损失又称为二次损失、L2损失，常用于回归预测任务中。均方误差函数通过计算预测值和实际值之间距离（即误差）的平方来衡量模型优劣。即预测值和真实值越接近，两者的均方差就越小。</p>
<h5 id="计算方式"><a href="#计算方式" class="headerlink" title="计算方式"></a>计算方式</h5><p>假设有 $n$ 个训练数据 $x_i$，每个训练数据 $x_i$ 的真实输出为 $y_i$，模型对 $x_i$ 的预测值为 $\hat{y}_i$。该模型在 $n$ 个训练数据下所产生的均方误差损失可定义如下： </p>
<p>$$<br>MSE&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^n{\left( y_i-\hat{y}_i \right) ^2}<br>$$<br>假设真实目标值为100，预测值在-10000到10000之间，我们绘制MSE函数曲线如图所示。可以看到，当预测值越接近100时，MSE损失值越小。MSE损失的范围为0到$\infty$ 。<br><img src="/%5Cphoto%5Cimage-13.png" alt="alt text"></p>
<h4 id="交叉熵损失（cross-entropy-loss-）"><a href="#交叉熵损失（cross-entropy-loss-）" class="headerlink" title="交叉熵损失（cross-entropy loss ）"></a>交叉熵损失（cross-entropy loss ）</h4><p>在物理学中，“熵”被用来表示热力学系统所呈现的无序程度。香农将这一概念引入信息论领域，提出了“信息熵”概念，通过对数函数来测量信息的不确定性。</p>
<p>交叉熵（cross entropy）是信息论中的重要概念，主要用来度量两个概率分布间的差异。假定 $p$ 和 $q$ 是数据 $x$ 的两个概率分布，通过 $q$ 来表示 $p$ 的交叉熵可如下计算：</p>
<p>$$<br>H\left( p,q \right) &#x3D;-\sum_x{p\left( x \right) \log q\left( x \right)}<br>$$</p>
<p>交叉熵刻画了两个概率分布之间的距离，旨在描绘通过概率分布 $q$ 来表达概率分布 $p$ 的困难程度。根据公式不难理解，交叉熵越小，两个概率分布 $p$ 和 $q$ 越接近。</p>
<p>这里仍然以三类分类问题为例，假设数据 $x$ 属于类别 $1$。记数据x的类别分布概率为 $y$，显然 $y&#x3D;(1,0,0)$代表数据 $x$ 的实际类别分布概率。记 $\hat{y}<br>$ 代表模型预测所得类别分布概率。</p>
<p>那么对于数据 $x$ 而言，其实际类别分布概率 $y$ 和模型预测类别分布概率 $\hat{y}$ 的交叉熵损失函数定义为：</p>
<p>$$<br>cross\ entropy&#x3D;-y\times \log \left( \hat{y} \right)<br>$$</p>
<p>很显然，一个良好的神经网络要尽量保证对于每一个输入数据，神经网络所预测类别分布概率与实际类别分布概率之间的差距越小越好，即交叉熵越小越好。于是，可将交叉熵作为损失函数来训练神经网络。</p>
<p><img src="https://github.com/PaddlePaddle/awesome-DeepLearning/raw/master/docs/images/deep_learning/loss_functions/CrossEntropy.png">三类分类问题中输入x的交叉熵损失示意图（x 属于第一类）</p>
<p>上图给出了一个三个类别分类的例子。由于输入数据 $x$ 属于类别 $1$，因此其实际类别概率分布值为 $y&#x3D;(y_1,y_2,y_3)&#x3D;(1,0,0)$。经过神经网络的变换，得到了输入数据 $x$ 相对于三个类别的预测中间值 $(z1,z2,z3)$。然后，经过 $Softmax$ 函数映射，得到神经网络所预测的输入数据 $x$ 的类别分布概率 $\hat{y}&#x3D;\left( \hat{y}_1,\hat{y}_2,\hat{y}_3 \right)$。根据前面的介绍，$\hat{y}_1$、$\hat{y}_2$ 和 $\hat{y}_3$ 为 $(0,1)$ 范围之间的一个概率值。由于样本 $x$ 属于第一个类别，因此希望神经网络所预测得到的 $\hat{y}_1$取值要远远大于 $\hat{y}_2$ 和 $\hat{y}_3$ 的取值。为了得到这样的神经网络，在训练中可利用如下交叉熵损失函数来对模型参数进行优化：<br>$$<br>cross\ entropy&#x3D;-\left( y_1\times \log \left( \hat{y}_1 \right) +y_2\times \log \left( \hat{y}_2 \right) +y_3\times \log \left( \hat{y}_3 \right) \right)<br>$$</p>
<p>在上式中，$y_2$ 和 $y_3$ 均为 $0$、$y_1$ 为 $1$，因此交叉熵损失函数简化为：<br>$$<br>-y_1\times \log \left( \hat{y}_1 \right) &#x3D;-\log \left( \hat{y}_1 \right)<br>$$</p>
<p>在神经网络训练中，要将输入数据实际的类别概率分布与模型预测的类别概率分布之间的误差（即损失）从输出端向输入端传递，以便来优化模型参数。下面简单介绍根据交叉熵计算得到的误差从 $\hat{y}_1$ 传递给 $z_1$ 和 $z_2$（$z_3$ 的推导与 $z_2$ 相同）的情况。</p>
<p>$$<br>\frac{\partial \hat{y}_1}{\partial z_1}&#x3D;\frac{\partial \left( \frac{e^{z_1}}{\sum_k{e^{z_k}}} \right)}{\partial z_1}&#x3D;\frac{\left( e^{z_1} \right) ^{‘}\times \sum_k{e^{z_k}-e^{z_1}\times e^{z_1}}}{\left( \sum_k{e^{z_k}} \right) ^2}&#x3D;\frac{e^{z_1}}{\sum_k{e^{z_k}}}-\frac{e^{z_1}}{\sum_k{e^{z_k}}}\times \frac{e^{z_1}}{\sum_k{e^{z_k}}}&#x3D;\hat{y}_1\left( 1-\hat{y}_1 \right)<br>$$</p>
<p>由于交叉熵损失函数 $-\log \left( \hat{y}_1 \right)$ 对 $\hat{y}_1$ 求导的结果为 $-\frac{1}{\hat{y}_1}$，$\hat{y}_1\left( 1-\hat{y}_1 \right)$ 与 $-\frac{1}{\hat{y}_1}$ 相乘为 $\hat{y}_1-1$。这说明一旦得到模型预测输出 $\hat{y}_1$，将该输出减去1就是交叉损失函数相对于 $z_1$ 的偏导结果。</p>
<p>$$<br>\frac{\partial \hat{y}_1}{\partial z_2}&#x3D;\frac{\partial \left( \frac{e^{z_1}}{\sum_k{e^{z_k}}} \right)}{\partial z_2}&#x3D;\frac{0\times \sum_k{e^{z_k}-e^{z_1}\times e^{z_2}}}{\left( \sum_k{e^{z_k}} \right) ^2}&#x3D;-\frac{e^{z_1}}{\sum_k{e^{z_k}}}\times \frac{e^{z_2}}{\sum_k{e^{z_k}}}&#x3D;-\hat{y}_1\hat{y}_2<br>$$</p>
<p>同理，交叉熵损失函数导数为 $-\frac{1}{\hat{y}_1}$，$-\hat{y}_1\hat{y}_2$ 与 $-\frac{1}{\hat{y}_1}$ 相乘结果为 $\hat{y}_2$。这意味对于除第一个输出节点以外的节点进行偏导，在得到模型预测输出后，只要将其保存，就是交叉损失函数相对于其他节点的偏导结果。在 $z_1$、$z_2$ 和 $z_3$得到偏导结果后，再通过链式法则（后续介绍）将损失误差继续往输入端传递即可。</p>
<p>在上面的例子中，假设所预测中间值 $(z_1,z_2,z_3)$ 经过 $Softmax$ 映射后所得结果为 $(0.34,0.46,0.20)$。由于已知输入数据 $x$ 属于第一类，显然这个输出不理想而需要对模型参数进行优化。如果选择交叉熵损失函数来优化模型，则 $(z_1,z_2,z_3)$ 这一层的偏导值为 $(0.34-1,0.46,0.20)&#x3D; (-0.66,0.46,0.20)$。</p>
<p>可以看出，$Softmax$ 和交叉熵损失函数相互结合，为偏导计算带来了极大便利。偏导计算使得损失误差从输出端向输入端传递，来对模型参数进行优化。在这里，交叉熵与$Softmax$ 函数结合在一起，因此也叫 $Softmax$ 损失（Softmax with cross-entropy loss）。</p>
<h1 id="基于多层感知机的姓氏分类网络"><a href="#基于多层感知机的姓氏分类网络" class="headerlink" title="基于多层感知机的姓氏分类网络"></a>基于多层感知机的姓氏分类网络</h1><p>我们将MLP应用于将姓氏分类到其原籍国的任务。从公开观察到的数据推断人口统计信息(如国籍)具有从产品推荐到确保不同人口统计用户获得公平结果的应用。人口统计和其他自我识别信息统称为“受保护属性”。“在建模和产品中使用这些属性时，必须小心。”我们首先对每个姓氏的字符进行拆分，并像对待“示例:将餐馆评论的情绪分类”中的单词一样对待它们。除了数据上的差异，字符层模型在结构和实现上与基于单词的模型基本相似.我们将通过描述姓氏分类器模型及其设计背后的思想过程来继续。</p>
<h2 id="The-Surname-Dataset"><a href="#The-Surname-Dataset" class="headerlink" title="The Surname Dataset"></a>The Surname Dataset</h2><p>姓氏数据集，它收集了来自18个不同国家的10,000个姓氏，这些姓氏是作者从互联网上不同的姓名来源收集的。该数据集将在本课程实验的几个示例中重用，并具有一些使其有趣的属性。第一个性质是它是相当不平衡的。排名前三的课程占数据的60%以上:27%是英语，21%是俄语，14%是阿拉伯语。剩下的15个民族的频率也在下降——这也是语言特有的特性。第二个特点是，在国籍和姓氏正字法(拼写)之间有一种有效和直观的关系。有些拼写变体与原籍国联系非常紧密(比如“O ‘Neill”、“Antonopoulos”、“Nagasawa”或“Zhu”)。</p>
<p>为了创建最终的数据集，我们从一个比课程补充材料中包含的版本处理更少的版本开始，并执行了几个数据集修改操作。第一个目的是减少这种不平衡——原始数据集中70%以上是俄文，这可能是由于抽样偏差或俄文姓氏的增多。为此，我们通过选择标记为俄语的姓氏的随机子集对这个过度代表的类进行子样本。接下来，我们根据国籍对数据集进行分组，并将数据集分为三个部分:70%到训练数据集，15%到验证数据集，最后15%到测试数据集，以便跨这些部分的类标签分布具有可比性。</p>
<p>SurnameDataset的实现与“Example: classification of Sentiment of Restaurant Reviews”中的ReviewDataset几乎相同，只是在getitem方法的实现方式上略有不同。回想一下，本课程中呈现的数据集类继承自PyTorch的数据集类，因此，我们需要实现两个函数:<code>__getitem</code>方法，它在给定索引时返回一个数据点;以及len方法，该方法返回数据集的长度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SurnameDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="comment"># Implementation is nearly identical to Section 3.5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;PyTorch 数据集的主要入口方法</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            index (int): 数据点的索引 </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            一个包含数据点的字典:</span></span><br><span class="line"><span class="string">                特征 (x_surname)</span></span><br><span class="line"><span class="string">                标签 (y_nationality)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        row = self._target_df.iloc[index]</span><br><span class="line"></span><br><span class="line">        surname_vector = self._vectorizer.vectorize(row.surname)</span><br><span class="line"></span><br><span class="line">        nationality_index = self._vectorizer.nationality_vocab.lookup_token(row.nationality)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;x_surname&#x27;</span>: surname_vector,</span><br><span class="line">                <span class="string">&#x27;y_nationality&#x27;</span>: nationality_index&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Vocabulary-Vectorizer-and-DataLoader"><a href="#Vocabulary-Vectorizer-and-DataLoader" class="headerlink" title="Vocabulary, Vectorizer, and DataLoader"></a>Vocabulary, Vectorizer, and DataLoader</h2><p>为了使用字符对姓氏进行分类，我们使用词汇表、向量化器和DataLoader将姓氏字符串转换为向量化的minibatches。这些数据结构与“Example: Classifying Sentiment of Restaurant Reviews”中使用的数据结构相同，它们举例说明了一种多态性，这种多态性将姓氏的字符标记与Yelp评论的单词标记相同对待。数据不是通过将字令牌映射到整数来向量化的，而是通过将字符映射到整数来向量化的。</p>
<h3 id="THE-VOCABULARY-CLASS"><a href="#THE-VOCABULARY-CLASS" class="headerlink" title="THE VOCABULARY CLASS"></a>THE VOCABULARY CLASS</h3><p>本例中使用的词汇类与“example: Classifying Sentiment of Restaurant Reviews”中的词汇完全相同，该词汇类将Yelp评论中的单词映射到对应的整数。简要概述一下，词汇表是两个Python字典的协调，这两个字典在令牌(在本例中是字符)和整数之间形成一个双射;也就是说，第一个字典将字符映射到整数索引，第二个字典将整数索引映射到字符。add_token方法用于向词汇表中添加新的令牌，lookup_token方法用于检索索引，lookup_index方法用于检索给定索引的令牌(在推断阶段很有用)。与Yelp评论的词汇表不同，我们使用的是one-hot词汇表，不计算字符出现的频率，只对频繁出现的条目进行限制。这主要是因为数据集很小，而且大多数字符足够频繁。</p>
<h3 id="THE-SURNAMEVECTORIZER"><a href="#THE-SURNAMEVECTORIZER" class="headerlink" title="THE SURNAMEVECTORIZER"></a>THE SURNAMEVECTORIZER</h3><p>虽然词汇表将单个令牌(字符)转换为整数，但SurnameVectorizer负责应用词汇表并将姓氏转换为向量。实例化和使用非常类似于“示例:对餐馆评论的情绪进行分类”中的ReviewVectorizer，但有一个关键区别:字符串没有在空格上分割。姓氏是字符的序列，每个字符在我们的词汇表中是一个单独的标记。然而，在“卷积神经网络”出现之前，我们将忽略序列信息，通过迭代字符串输入中的每个字符来创建输入的收缩one-hot向量表示。我们为以前未遇到的字符指定一个特殊的令牌，即UNK。由于我们仅从训练数据实例化词汇表，而且验证或测试数据中可能有惟一的字符，所以在字符词汇表中仍然使用UNK符号。</p>
<p>虽然我们在这个示例中使用了收缩的one-hot，但是在后面的实验中，将了解其他向量化方法，它们是one-hot编码的替代方法，有时甚至更好。具体来说，在“示例:使用CNN对姓氏进行分类”中，将看到一个热门矩阵，其中每个字符都是矩阵中的一个位置，并具有自己的热门向量。然后，在实验5中，将学习嵌入层，返回整数向量的向量化，以及如何使用它们创建密集向量矩阵。看一下示例4-6中SurnameVectorizer的代码。</p>
<p>Example 4-6. Implementing SurnameVectorizer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SurnameVectorizer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;协调词汇表并将它们应用到使用的向量化器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, surname_vocab, nationality_vocab, max_surname_length</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            surname_vocab (Vocabulary): 将字符映射到整数的词汇表</span></span><br><span class="line"><span class="string">            nationality_vocab (Vocabulary): 将国籍映射到整数的词汇表</span></span><br><span class="line"><span class="string">            max_surname_length (int): 最长姓氏的长度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.surname_vocab = surname_vocab</span><br><span class="line">        self.nationality_vocab = nationality_vocab</span><br><span class="line">        self._max_surname_length = max_surname_length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">vectorize</span>(<span class="params">self, surname</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            surname (str): 姓氏</span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            one_hot_matrix (np.ndarray): 一个one-hot向量的矩阵</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        one_hot_matrix_size = (<span class="built_in">len</span>(self.surname_vocab), self._max_surname_length)</span><br><span class="line">        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)</span><br><span class="line">                               </span><br><span class="line">        <span class="keyword">for</span> position_index, character <span class="keyword">in</span> <span class="built_in">enumerate</span>(surname):</span><br><span class="line">            character_index = self.surname_vocab.lookup_token(character)</span><br><span class="line">            one_hot_matrix[character_index][position_index] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> one_hot_matrix</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_dataframe</span>(<span class="params">cls, surname_df</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;从数据集DataFrame实例化向量化器</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            surname_df (pandas.DataFrame): 姓氏数据集</span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            SurnameVectorizer的实例</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        surname_vocab = Vocabulary(unk_token=<span class="string">&quot;@&quot;</span>)</span><br><span class="line">        nationality_vocab = Vocabulary(add_unk=<span class="literal">False</span>)</span><br><span class="line">        max_surname_length = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> index, row <span class="keyword">in</span> surname_df.iterrows():</span><br><span class="line">            max_surname_length = <span class="built_in">max</span>(max_surname_length, <span class="built_in">len</span>(row.surname))</span><br><span class="line">            <span class="keyword">for</span> letter <span class="keyword">in</span> row.surname:</span><br><span class="line">                surname_vocab.add_token(letter)</span><br><span class="line">            nationality_vocab.add_token(row.nationality)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(surname_vocab, nationality_vocab, max_surname_length)</span><br></pre></td></tr></table></figure>


<h2 id="The-Surname-Classifier-Model"><a href="#The-Surname-Classifier-Model" class="headerlink" title="The Surname Classifier Model"></a>The Surname Classifier Model</h2><p>SurnameClassifier是本实验前面介绍的MLP的实现(示例4-7)。第一个线性层将输入向量映射到中间向量，并对该向量应用非线性。第二线性层将中间向量映射到预测向量。</p>
<p>在最后一步中，可选地应用softmax操作，以确保输出和为1;这就是所谓的“概率”。它是可选的原因与我们使用的损失函数的数学公式有关——交叉熵损失。我们研究了“损失函数”中的交叉熵损失。回想一下，交叉熵损失对于多类分类是最理想的，但是在训练过程中软最大值的计算不仅浪费而且在很多情况下并不稳定。</p>
<p>Example 4-7. The SurnameClassifier as an MLP</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SurnameClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, initial_num_channels, num_classes, num_channels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        创建一个姓氏分类器。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            initial_num_channels (int): 输入特征向量的大小</span></span><br><span class="line"><span class="string">            num_classes (int): 输出预测向量的大小</span></span><br><span class="line"><span class="string">            num_channels (int): 网络中要始终使用的恒定通道大小</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SurnameClassifier, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.convnet = nn.Sequential(</span><br><span class="line">            nn.Conv1d(in_channels=initial_num_channels, </span><br><span class="line">                      out_channels=num_channels, kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class="line">                      kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class="line">                      kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class="line">                      kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.ELU()</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Linear(num_channels, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_surname, apply_softmax=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;分类器的前向传播</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            x_surname (torch.Tensor): 输入数据张量。</span></span><br><span class="line"><span class="string">                x_surname.shape 应为 (batch, initial_num_channels, max_surname_length)</span></span><br><span class="line"><span class="string">            apply_softmax (bool): softmax 激活的标志</span></span><br><span class="line"><span class="string">                如果与交叉熵损失一起使用，则应为 False</span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            结果张量。tensor.shape 应为 (batch, num_classes)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        features = self.convnet(x_surname).squeeze(dim=<span class="number">2</span>)</span><br><span class="line">       </span><br><span class="line">        prediction_vector = self.fc(features)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> apply_softmax:</span><br><span class="line">            prediction_vector = F.softmax(prediction_vector, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> prediction_vector</span><br></pre></td></tr></table></figure>


<h2 id="The-Training-Routine"><a href="#The-Training-Routine" class="headerlink" title="The Training Routine"></a>The Training Routine</h2><p>虽然我们使用了不同的模型、数据集和损失函数，但是训练例程是相同的。因此，在例4-8中，我们只展示了args以及本例中的训练例程与“示例:餐厅评论情绪分类”中的示例之间的主要区别。</p>
<p>Example 4-8. The args for classifying surnames with an MLP</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">args = Namespace(</span><br><span class="line">    <span class="comment"># 数据和路径信息</span></span><br><span class="line">    surname_csv=<span class="string">&quot;data/surnames/surnames_with_splits.csv&quot;</span>,</span><br><span class="line">    vectorizer_file=<span class="string">&quot;vectorizer.json&quot;</span>,</span><br><span class="line">    model_state_file=<span class="string">&quot;model.pth&quot;</span>,</span><br><span class="line">    save_dir=<span class="string">&quot;model_storage/ch4/cnn&quot;</span>,</span><br><span class="line">    <span class="comment"># 模型超参数</span></span><br><span class="line">    hidden_dim=<span class="number">100</span>,</span><br><span class="line">    num_channels=<span class="number">256</span>,</span><br><span class="line">    <span class="comment"># 训练超参数</span></span><br><span class="line">    seed=<span class="number">1337</span>,</span><br><span class="line">    learning_rate=<span class="number">0.001</span>,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    num_epochs=<span class="number">100</span>,</span><br><span class="line">    early_stopping_criteria=<span class="number">5</span>,</span><br><span class="line">    dropout_p=<span class="number">0.1</span>,</span><br><span class="line">    <span class="comment"># 运行时选项</span></span><br><span class="line">    cuda=<span class="literal">False</span>,</span><br><span class="line">    reload_from_files=<span class="literal">False</span>,</span><br><span class="line">    expand_filepaths_to_save_dir=<span class="literal">True</span>,</span><br><span class="line">    catch_keyboard_interrupt=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>训练中最显著的差异与模型中输出的种类和使用的损失函数有关。在这个例子中，输出是一个多类预测向量，可以转换为概率。正如在模型描述中所描述的，这种输出的损失类型仅限于CrossEntropyLoss和NLLLoss。由于它的简化，我们使用了CrossEntropyLoss。</p>
<p>在例4-9中，我们展示了数据集、模型、损失函数和优化器的实例化。这些实例应该看起来与“示例:将餐馆评论的情绪分类”中的实例几乎相同。事实上，在本课程后面的实验中，这种模式将对每个示例进行重复。</p>
<p>Example 4-9. Instantiating the dataset, model, loss, and optimizer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)</span><br><span class="line">vectorizer = dataset.get_vectorizer()</span><br><span class="line"></span><br><span class="line">classifier = SurnameClassifier(input_dim=<span class="built_in">len</span>(vectorizer.surname_vocab),</span><br><span class="line">                               hidden_dim=args.hidden_dim,</span><br><span class="line">                               output_dim=<span class="built_in">len</span>(vectorizer.nationality_vocab))</span><br><span class="line"></span><br><span class="line">classifier = classifier.to(args.device)    </span><br><span class="line"></span><br><span class="line">loss_func = nn.CrossEntropyLoss(dataset.class_weights)</span><br><span class="line">optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)</span><br></pre></td></tr></table></figure>


<h3 id="THE-TRAINING-LOOP"><a href="#THE-TRAINING-LOOP" class="headerlink" title="THE TRAINING LOOP"></a>THE TRAINING LOOP</h3><p>与“Example: Classifying Sentiment of Restaurant Reviews”中的训练循环相比，本例的训练循环除了变量名以外几乎是相同的。具体来说，示例4-10显示了使用不同的key从batch_dict中获取数据。除了外观上的差异，训练循环的功能保持不变。利用训练数据，计算模型输出、损失和梯度。然后，使用梯度来更新模型。</p>
<p>Example 4-10. A snippet of the training loop</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the training routine is these 5 steps:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------------</span></span><br><span class="line"><span class="comment"># step 1. zero the gradients</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># step 2. compute the output</span></span><br><span class="line">y_pred = classifier(batch_dict[<span class="string">&#x27;x_surname&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># step 3. compute the loss</span></span><br><span class="line">loss = loss_func(y_pred, batch_dict[<span class="string">&#x27;y_nationality&#x27;</span>])</span><br><span class="line">loss_batch = loss.to(<span class="string">&quot;cpu&quot;</span>).item()</span><br><span class="line">running_loss += (loss_batch - running_loss) / (batch_index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step 4. use loss to produce gradients</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># step 5. use optimizer to take gradient step</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>


<h2 id="Model-Evaluation-and-Prediction"><a href="#Model-Evaluation-and-Prediction" class="headerlink" title="Model Evaluation and Prediction"></a>Model Evaluation and Prediction</h2><p>要理解模型的性能，应该使用定量和定性方法分析模型。定量测量出的测试数据的误差，决定了分类器能否推广到不可见的例子。定性地说，可以通过查看分类器的top-k预测来为一个新示例开发模型所了解的内容的直觉。</p>
<h3 id="EVALUATING-ON-THE-TEST-DATASET"><a href="#EVALUATING-ON-THE-TEST-DATASET" class="headerlink" title="EVALUATING ON THE TEST DATASET"></a>EVALUATING ON THE TEST DATASET</h3><p>评价SurnameClassifier测试数据,我们执行相同的常规的routine文本分类的例子“餐馆评论的例子:分类情绪”:我们将数据集设置为遍历测试数据,调用<code>classifier.eval()</code>方法,并遍历测试数据以同样的方式与其他数据。在这个例子中，调用<code>classifier.eval()</code>可以防止PyTorch在使用测试&#x2F;评估数据时更新模型参数。</p>
<p>该模型对测试数据的准确性达到50%左右。如果在附带的notebook中运行训练例程，会注意到在训练数据上的性能更高。这是因为模型总是更适合它所训练的数据，所以训练数据的性能并不代表新数据的性能。如果遵循代码，你可以尝试隐藏维度的不同大小，应该注意到性能的提高。然而，这种增长不会很大(尤其是与“用CNN对姓氏进行分类的例子”中的模型相比)。其主要原因是收缩的onehot向量化方法是一种弱表示。虽然它确实简洁地将每个姓氏表示为单个向量，但它丢弃了字符之间的顺序信息，这对于识别起源非常重要。</p>
<h3 id="CLASSIFYING-A-NEW-SURNAME"><a href="#CLASSIFYING-A-NEW-SURNAME" class="headerlink" title="CLASSIFYING A NEW SURNAME"></a>CLASSIFYING A NEW SURNAME</h3><p>示例4-11显示了分类新姓氏的代码。给定一个姓氏作为字符串，该函数将首先应用向量化过程，然后获得模型预测。注意，我们包含了apply_softmax标志，所以结果包含概率。模型预测，在多项式的情况下，是类概率的列表。我们使用PyTorch张量最大函数来得到由最高预测概率表示的最优类。</p>
<p>Example 4-11. A function for performing nationality prediction</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_nationality</span>(<span class="params">surname, classifier, vectorizer</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测姓氏的国籍</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        surname (str): 要分类的姓氏</span></span><br><span class="line"><span class="string">        classifier (SurnameClassifer): 分类器的实例</span></span><br><span class="line"><span class="string">        vectorizer (SurnameVectorizer): 相应的向量化器</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dict: 包含最可能的国籍及其概率的字典</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 向量化姓氏</span></span><br><span class="line">    vectorized_surname = vectorizer.vectorize(surname)</span><br><span class="line">    <span class="comment"># 将向量化的姓氏转换为PyTorch张量，并添加批次维度</span></span><br><span class="line">    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 使用分类器进行预测，并对输出进行softmax处理</span></span><br><span class="line">    result = classifier(vectorized_surname, apply_softmax=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取概率最高的国籍及其对应的索引</span></span><br><span class="line">    probability_values, indices = result.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">    index = indices.item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过索引查找预测的国籍，并获取其概率值</span></span><br><span class="line">    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)</span><br><span class="line">    probability_value = probability_values.item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回预测结果的字典</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;nationality&#x27;</span>: predicted_nationality, <span class="string">&#x27;probability&#x27;</span>: probability_value&#125;</span><br></pre></td></tr></table></figure>
<p>示例：<br><img src="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-15.png?raw=true" alt="alt text"></p>
<h3 id="RETRIEVING-THE-TOP-K-PREDICTIONS-FOR-A-NEW-SURNAME"><a href="#RETRIEVING-THE-TOP-K-PREDICTIONS-FOR-A-NEW-SURNAME" class="headerlink" title="RETRIEVING THE TOP-K PREDICTIONS FOR A NEW SURNAME"></a>RETRIEVING THE TOP-K PREDICTIONS FOR A NEW SURNAME</h3><p>不仅要看最好的预测，还要看更多的预测。例如，NLP中的标准实践是采用k-best预测并使用另一个模型对它们重新排序。PyTorch提供了一个torch.topk函数，它提供了一种方便的方法来获得这些预测，如示例4-12所示。</p>
<p>Example 4-12. Predicting the top-k nationalities</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_topk_nationality</span>(<span class="params">surname, classifier, vectorizer, k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测姓氏的前K个可能的国籍</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        surname (str): 要分类的姓氏</span></span><br><span class="line"><span class="string">        classifier (SurnameClassifer): 分类器的实例</span></span><br><span class="line"><span class="string">        vectorizer (SurnameVectorizer): 相应的向量化器</span></span><br><span class="line"><span class="string">        k (int): 要返回的前K个国籍数量</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: 字典的列表，每个字典包含一个国籍和其对应的概率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 向量化姓氏</span></span><br><span class="line">    vectorized_surname = vectorizer.vectorize(surname)</span><br><span class="line">    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 使用分类器进行预测，并对输出进行softmax处理，获取前K个国籍及其概率</span></span><br><span class="line">    prediction_vector = classifier(vectorized_surname, apply_softmax=<span class="literal">True</span>)</span><br><span class="line">    probability_values, indices = torch.topk(prediction_vector, k=k)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取预测结果中概率最高的K个国籍及其概率值</span></span><br><span class="line">    probability_values = probability_values[<span class="number">0</span>].detach().numpy()</span><br><span class="line">    indices = indices[<span class="number">0</span>].detach().numpy()</span><br><span class="line">    </span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> kth_index <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        <span class="comment"># 通过索引查找国籍，并获取其对应的概率值，将结果存储为字典列表</span></span><br><span class="line">        nationality = vectorizer.nationality_vocab.lookup_index(indices[kth_index])</span><br><span class="line">        probability_value = probability_values[kth_index]</span><br><span class="line">        results.append(&#123;<span class="string">&#x27;nationality&#x27;</span>: nationality, </span><br><span class="line">                        <span class="string">&#x27;probability&#x27;</span>: probability_value&#125;)</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<p>示例：<br><img src="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-16.png?raw=true" alt="alt text"></p>
<h2 id="Regularizing-MLPs-Weight-Regularization-and-Structural-Regularization-or-Dropout"><a href="#Regularizing-MLPs-Weight-Regularization-and-Structural-Regularization-or-Dropout" class="headerlink" title="Regularizing MLPs: Weight Regularization and Structural Regularization (or Dropout)"></a>Regularizing MLPs: Weight Regularization and Structural Regularization (or Dropout)</h2><p>在实验3中，我们解释了正则化是如何解决过拟合问题的，并研究了两种重要的权重正则化类型——L1和L2。这些权值正则化方法也适用于MLPs和卷积神经网络，我们将在本实验后面介绍。除权值正则化外，对于深度模型(即例如本实验讨论的前馈网络，一种称为dropout的结构正则化方法变得非常重要。</p>
<h3 id="DROPOUT"><a href="#DROPOUT" class="headerlink" title="DROPOUT"></a>DROPOUT</h3><p>简单地说，在训练过程中，dropout有一定概率使属于两个相邻层的单元之间的连接减弱。这有什么用呢?我们从斯蒂芬•梅里蒂(Stephen Merity)的一段直观(且幽默)的解释开始：“Dropout，简单地说，是指如果你能在喝醉的时候反复学习如何做一件事，那么你应该能够在清醒的时候做得更好。这一见解产生了许多最先进的结果和一个新兴的领域。”</p>
<p>神经网络——尤其是具有大量分层的深层网络——可以在单元之间创建有趣的相互适应。“Coadaptation”是神经科学中的一个术语，但在这里它只是指一种情况，即两个单元之间的联系变得过于紧密，而牺牲了其他单元之间的联系。这通常会导致模型与数据过拟合。通过概率地丢弃单元之间的连接，我们可以确保没有一个单元总是依赖于另一个单元，从而产生健壮的模型。dropout不会向模型中添加额外的参数，但是需要一个超参数——“drop probability”。drop probability，它是单位之间的连接drop的概率。通常将下降概率设置为0.5。例4-13给出了一个带dropout的MLP的重新实现。</p>
<p>Example 4-13. MLP with dropout</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultilayerPerceptron</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dim, output_dim</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            input_dim (int): the size of the input vectors</span></span><br><span class="line"><span class="string">            hidden_dim (int): the output size of the first Linear layer</span></span><br><span class="line"><span class="string">            output_dim (int): the output size of the second Linear layer</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultilayerPerceptron, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_in, apply_softmax=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;The forward pass of the MLP</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x_in (torch.Tensor): an input data tensor.</span></span><br><span class="line"><span class="string">                x_in.shape should be (batch, input_dim)</span></span><br><span class="line"><span class="string">            apply_softmax (bool): a flag for the softmax activation</span></span><br><span class="line"><span class="string">                should be false if used with the Cross Entropy losses</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            the resulting tensor. tensor.shape should be (batch, output_dim)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        intermediate = F.relu(self.fc1(x_in))</span><br><span class="line">        output = self.fc2(F.dropout(intermediate, p=<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> apply_softmax:</span><br><span class="line">            output = F.softmax(output, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>请注意，dropout只适用于训练期间，不适用于评估期间。作为练习，可以尝试带有dropout的SurnameClassifier模型，看看它如何更改结果。</p>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>在本实验的第一部分中，我们深入研究了MLPs、由一系列线性层和非线性函数构建的神经网络。mlp不是利用顺序模式的最佳工具。例如，在姓氏数据集中，姓氏可以有(不同长度的)段，这些段可以显示出相当多关于其起源国家的信息(如“O’Neill”中的“O”、“Antonopoulos”中的“opoulos”、“Nagasawa”中的“sawa”或“Zhu”中的“Zh”)。这些段的长度可以是可变的，挑战是在不显式编码的情况下捕获它们。</p>
<p>在本节中，我们将介绍卷积神经网络(CNN)，这是一种非常适合检测空间子结构(并因此创建有意义的空间子结构)的神经网络。CNNs通过使用少量的权重来扫描输入数据张量来实现这一点。通过这种扫描，它们产生表示子结构检测(或不检测)的输出张量。</p>
<p>在本节的其余部分中，我们首先描述CNN的工作方式，以及在设计CNN时应该考虑的问题。我们深入研究CNN超参数，目的是提供直观的行为和这些超参数对输出的影响。最后，我们通过几个简单的例子逐步说明CNNs的机制。在“示例:使用CNN对姓氏进行分类”中，我们将深入研究一个更广泛的示例。</p>
<h2 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h2><p>卷积神经网络（英语：convolutional neural network，缩写：CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。</p>
<p>CNNs的名称和基本功能源于经典的数学运算卷积。卷积已经应用于各种工程学科，包括数字信号处理和计算机图形学。一般来说，卷积使用程序员指定的参数。这些参数被指定来匹配一些功能设计，如突出边缘或抑制高频声音。事实上，许多Photoshop滤镜都是应用于图像的固定卷积运算。然而，在深度学习和本实验中，我们从数据中学习卷积滤波器的参数，因此它对于解决当前的任务是最优的。</p>
<p>卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。</p>
<p>卷积神经网络的灵感来自于动物视觉皮层组织的神经连接方式。单个神经元只对有限区域内的刺激作出反应，不同神经元的感知区域相互重叠从而覆盖整个视野</p>
<h2 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h2><p>典型的 CNN 由3个部分构成：</p>
<p>1.卷积层</p>
<p>2.池化层</p>
<p>3.全连接层</p>
<p>如果简单来描述的话：</p>
<p>卷积层负责提取图像中的局部特征；池化层用来大幅降低参数量级(降维)；全连接层类似传统神经网络的部分，用来输出想要的结果。</p>
<h3 id="Convolutional-layer"><a href="#Convolutional-layer" class="headerlink" title="Convolutional layer"></a>Convolutional layer</h3><p>卷积层的运算过程如下图，用一个卷积核扫完整张图片：<br><img src="/%5Cphoto%5Cf144f-2019-06-19-juanji.gif" alt="alt text"></p>
<p>这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。</p>
<p>在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：<br><img src="/%5Cphoto%5Cimage-14.png" alt="alt text"></p>
<h3 id="Pooling-Layer"><a href="#Pooling-Layer" class="headerlink" title="Pooling Layer"></a>Pooling Layer</h3><p>池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：<br><img src="/%5Cphoto%5C3fd53-2019-06-19-chihua.gif" alt="alt text"><br>上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。</p>
<p>之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。</p>
<p>池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。</p>
<h3 id="Fully-connected-layer"><a href="#Fully-connected-layer" class="headerlink" title="Fully connected layer"></a>Fully connected layer</h3><p>这个部分就是最后一步了，经过卷积层和池化层处理过的数据输入到全连接层，得到最终想要的结果。</p>
<p>经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。<br><img src="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/c1a6d-2019-06-19-quanlianjie.png.webp?raw=true" alt="alt text"><br>典型的 CNN 并非只是上面提到的3层结构，而是多层结构，例如 LeNet-5 的结构就如下图所示：</p>
<p>卷积层 – 池化层- 卷积层 – 池化层 – 卷积层 – 全连接层<br><img src="https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/a8f0b-2019-06-19-lenet.png.webp?raw=true" alt="alt text"></p>
<h3 id="DIMENSION-OF-THE-CONVOLUTION-OPERATION"><a href="#DIMENSION-OF-THE-CONVOLUTION-OPERATION" class="headerlink" title="DIMENSION OF THE CONVOLUTION OPERATION"></a>DIMENSION OF THE CONVOLUTION OPERATION</h3><p>首先要理解的概念是卷积运算的维数。在本节，我们使用二维卷积进行说明，但是根据数据的性质，还有更适合的其他维度的卷积。在PyTorch中，卷积可以是一维、二维或三维的，分别由Conv1d、Conv2d和Conv3d模块实现。一维卷积对于每个时间步都有一个特征向量的时间序列非常有用。在这种情况下，我们可以在序列维度上学习模式。NLP中的卷积运算大多是一维的卷积。另一方面，二维卷积试图捕捉数据中沿两个方向的时空模式;例如，在图像中沿高度和宽度维度——为什么二维卷积在图像处理中很流行。类似地，在三维卷积中，模式是沿着数据中的三维捕获的。例如，在视频数据中，信息是三维的，二维表示图像的帧，时间维表示帧的序列。我们主要使用Conv1d。</p>
<h3 id="CHANNELS"><a href="#CHANNELS" class="headerlink" title="CHANNELS"></a>CHANNELS</h3><p>非正式地，通道(channel)是指沿输入中的每个点的特征维度。例如，在图像中，对应于RGB组件的图像中的每个像素有三个通道。在使用卷积时，文本数据也可以采用类似的概念。从概念上讲，如果文本文档中的“像素”是单词，那么通道的数量就是词汇表的大小。如果我们更细粒度地考虑字符的卷积，通道的数量就是字符集的大小(在本例中刚好是词汇表)。在PyTorch卷积实现中，输入通道的数量是in_channels参数。卷积操作可以在输出(out_channels)中产生多个通道。您可以将其视为卷积运算符将输入特征维“映射”到输出特征维。下图说明了这个概念。<br><img src="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596033.png"><br><font color='grey'><center> 卷积运算用两个输入矩阵（两个输入通道）表示相应的核也有两层，它将每层分别相乘，然后对结果求和。参数配置：input_channels&#x3D;2, output_channels&#x3D;1, kernel_size&#x3D;2, tride&#x3D;1, padding&#x3D;0, and dilation&#x3D;1.</center></font><br><img src="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596054.png"><br><font color='grey'><center> 一种具有一个输入矩阵（一个输入通道）和两个卷积的卷积运算核（两个输出通道）。这些核分别应用于输入矩阵，并堆叠在输出张量。参数配置：input_channels&#x3D;1, output_channels&#x3D;2, kernel_size&#x3D;2, tride&#x3D;1,	padding&#x3D;0, and dilation&#x3D;1.</center></font></p>
<p>很难立即知道有多少输出通道适合当前的问题。为了简化这个困难，我们假设边界是1,1,024——我们可以有一个只有一个通道的卷积层，也可以有一个只有1,024个通道的卷积层。现在我们有了边界，接下来要考虑的是有多少个输入通道。一种常见的设计模式是，从一个卷积层到下一个卷积层，通道数量的缩减不超过2倍。这不是一个硬性的规则，但是它应该让您了解适当数量的out_channels是什么样子的。</p>
<h3 id="KERNEL-SIZE"><a href="#KERNEL-SIZE" class="headerlink" title="KERNEL SIZE"></a>KERNEL SIZE</h3><p>核矩阵的宽度称为核大小(PyTorch中的<code>kernel_size</code>)。我们分别给出一个2x2和3x3的核。卷积将输入中的空间(或时间)本地信息组合在一起，每个卷积的本地信息量由内核大小控制。然而，通过增加核的大小，也会减少输出的大小(Dumoulin和Visin, 2016)。这就是为什么当核大小为3时，输出矩阵2x2，而当核大小为2时，输出矩阵3x3。</p>
<p><img src="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596071.png"><br><font color='grey'><center> 将kernel_size&#x3D;3的卷积应用于输入矩阵。结果是一个折衷的结果：在每次将内核应用于矩阵时，都会使用更多的局部信息，但输出的大小会更小.</center></font></p>
<p>此外，可以将NLP应用程序中核大小的行为看作类似于通过查看单词组捕获语言模式的n-gram的行为。使用较小的核大小，可以捕获较小的频繁模式，而较大的核大小会导致较大的模式，这可能更有意义，但是发生的频率更低。较小的核大小会导致输出中的细粒度特性，而较大的核大小会导致粗粒度特性。</p>
<h3 id="STRIDE"><a href="#STRIDE" class="headerlink" title="STRIDE"></a>STRIDE</h3><p>Stride控制卷积之间的步长。如果步长与核相同，则内核计算不会重叠。另一方面，如果跨度为1，则内核重叠最大。输出张量可以通过增加步幅的方式被有意的压缩来总结信息，如下图所示。</p>
<p><img src="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596080.png"><br><font color='grey'><center> 应用于具有超参数步长的输入的kernel_size&#x3D;2的卷积核等于2。这会导致内核采取更大的步骤，从而产生更小的输出矩阵。对于更稀疏地对输入矩阵进行二次采样非常有用。</center></font></p>
<h3 id="PADDING"><a href="#PADDING" class="headerlink" title="PADDING"></a>PADDING</h3><p>即使stride和kernel_size允许控制每个计算出的特征值有多大范围，它们也有一个有害的、有时是无意的副作用，那就是缩小特征映射的总大小(卷积的输出)。为了抵消这一点，输入数据张量被人为地增加了长度(如果是一维、二维或三维)、高度(如果是二维或三维)和深度(如果是三维)，方法是在每个维度上附加和前置0。这意味着CNN将执行更多的卷积，但是输出形状可以控制，而不会影响所需的核大小、步幅或扩展。下图展示了正在运行的填充。</p>
<p><img src="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596142.png"><br><font color='grey'><center> 应用于高度和宽度等于的输入矩阵的kernel_size&#x3D;2的卷积2。但是，由于填充（用深灰色正方形表示），输入矩阵的高度和宽度可以被放大。这通常与大小为3的内核一起使用，这样输出矩阵将等于输入矩阵的大小。</center></font></p>
<h3 id="DILATION"><a href="#DILATION" class="headerlink" title="DILATION"></a>DILATION</h3><p>膨胀控制卷积核如何应用于输入矩阵。在下图中，我们显示，将膨胀从1(默认值)增加到2意味着当应用于输入矩阵时，核的元素彼此之间是两个空格。另一种考虑这个问题的方法是在核中跨跃——在核中的元素或核的应用之间存在一个step size，即存在“holes”。这对于在不增加参数数量的情况下总结输入空间的更大区域是有用的。当卷积层被叠加时，扩张卷积被证明是非常有用的。连续扩张的卷积指数级地增大了“接受域”的大小；即网络在做出预测之前所看到的输入空间的大小。</p>
<p><img src="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596131.png"><br><font color='grey'><center> 应用于超参数dilation&#x3D;2的输入矩阵的kernel_size&#x3D;2的卷积。从默认值开始膨胀的增加意味着核矩阵的元素在与输入矩阵相乘时进一步分散开来。进一步增大扩张会加剧这种扩散。</center></font></p>
<h1 id="基于卷积神经网络的姓氏分类"><a href="#基于卷积神经网络的姓氏分类" class="headerlink" title="基于卷积神经网络的姓氏分类"></a>基于卷积神经网络的姓氏分类</h1><h2 id="Implementing-CNNs-in-PyTorch"><a href="#Implementing-CNNs-in-PyTorch" class="headerlink" title="Implementing CNNs in PyTorch"></a>Implementing CNNs in PyTorch</h2><p>在本节中，我们将通过端到端示例来利用上一节中介绍的概念。一般来说，神经网络设计的目标是找到一个能够完成任务的超参数组态。我们再次考虑在“示例:带有多层感知器的姓氏分类”中引入的现在很熟悉的姓氏分类任务，但是我们将使用CNNs而不是MLP。我们仍然需要应用最后一个线性层，它将学会从一系列卷积层创建的特征向量创建预测向量。这意味着目标是确定卷积层的配置，从而得到所需的特征向量。所有CNN应用程序都是这样的:首先有一组卷积层，它们提取一个feature map，然后将其作为上游处理的输入。在分类中，上游处理几乎总是应用线性(或fc)层。</p>
<p>本课程中的实现遍历设计决策，以构建一个特征向量。我们首先构造一个人工数据张量，以反映实际数据的形状。数据张量的大小是三维的——这是向量化文本数据的最小批大小。如果你对一个字符序列中的每个字符使用onehot向量，那么onehot向量序列就是一个矩阵，而onehot矩阵的小批量就是一个三维张量。使用卷积的术语，每个onehot(通常是词汇表的大小)的大小是”input channels”的数量，字符序列的长度是“width”。</p>
<p>在例4-14中，构造特征向量的第一步是将PyTorch的Conv1d类的一个实例应用到三维数据张量。通过检查输出的大小，你可以知道张量减少了多少。</p>
<p>Example 4-14. Artificial data and using a Conv1d class</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SurnameDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;PyTorch数据集的主要入口方法</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            index (int): 数据点的索引 </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            一个字典，保存数据点的特征（x_data）和标签（y_target）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        row = self._target_df.iloc[index]</span><br><span class="line"></span><br><span class="line">        surname_matrix = \</span><br><span class="line">            self._vectorizer.vectorize(row.surname)</span><br><span class="line"></span><br><span class="line">        nationality_index = \</span><br><span class="line">            self._vectorizer.nationality_vocab.lookup_token(row.nationality)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;x_surname&#x27;</span>: surname_matrix,</span><br><span class="line">                <span class="string">&#x27;y_nationality&#x27;</span>: nationality_index&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Vocabulary-Vectorizer-and-DataLoader-1"><a href="#Vocabulary-Vectorizer-and-DataLoader-1" class="headerlink" title="Vocabulary, Vectorizer, and DataLoader"></a>Vocabulary, Vectorizer, and DataLoader</h2><p>在本例中，尽管词汇表和DataLoader的实现方式与“示例:带有多层感知器的姓氏分类”中的示例相同，但Vectorizer的vectorize()方法已经更改，以适应CNN模型的需要。具体来说，正如我们在示例4-18中的代码中所示，该函数将字符串中的每个字符映射到一个整数，然后使用该整数构造一个由onehot向量组成的矩阵。重要的是，矩阵中的每一列都是不同的onehot向量。主要原因是，我们将使用的Conv1d层要求数据张量在第0维上具有批处理，在第1维上具有通道，在第2维上具有特性。</p>
<p>除了更改为使用onehot矩阵之外，我们还修改了矢量化器，以便计算姓氏的最大长度并将其保存为max_surname_length</p>
<p>Example 4-18. Implementing the Surname Vectorizer for CNNs</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SurnameVectorizer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;协调词汇表并将它们应用到使用的向量化器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, surname_vocab, nationality_vocab, max_surname_length</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            surname_vocab (Vocabulary): 将字符映射到整数的词汇表</span></span><br><span class="line"><span class="string">            nationality_vocab (Vocabulary): 将国籍映射到整数的词汇表</span></span><br><span class="line"><span class="string">            max_surname_length (int): 最长姓氏的长度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.surname_vocab = surname_vocab</span><br><span class="line">        self.nationality_vocab = nationality_vocab</span><br><span class="line">        self._max_surname_length = max_surname_length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">vectorize</span>(<span class="params">self, surname</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            surname (str): 姓氏</span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            one_hot_matrix (np.ndarray): 一个one-hot向量的矩阵</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        one_hot_matrix_size = (<span class="built_in">len</span>(self.surname_vocab), self._max_surname_length)</span><br><span class="line">        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)</span><br><span class="line">                               </span><br><span class="line">        <span class="keyword">for</span> position_index, character <span class="keyword">in</span> <span class="built_in">enumerate</span>(surname):</span><br><span class="line">            character_index = self.surname_vocab.lookup_token(character)</span><br><span class="line">            one_hot_matrix[character_index][position_index] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> one_hot_matrix</span><br></pre></td></tr></table></figure>
<h2 id="Reimplementing-the-SurnameClassifier-with-Convolutional-Networks"><a href="#Reimplementing-the-SurnameClassifier-with-Convolutional-Networks" class="headerlink" title="Reimplementing the SurnameClassifier with Convolutional Networks"></a>Reimplementing the SurnameClassifier with Convolutional Networks</h2><p>我们在本例中使用的模型是使用我们在“卷积神经网络”中介绍的方法构建的。实际上，我们在该部分中创建的用于测试卷积层的“人工”数据与姓氏数据集中使用本例中的矢量化器的数据张量的大小完全匹配。正如在示例4-19中所看到的，它与我们在“卷积神经网络”中引入的Conv1d序列既有相似之处，也有需要解释的新添加内容。具体来说，该模型类似于“卷积神经网络”，它使用一系列一维卷积来增量地计算更多的特征，从而得到一个单特征向量。</p>
<p>然而，本例中的新内容是使用sequence和ELU PyTorch模块。序列模块是封装线性操作序列的方便包装器。在这种情况下，我们使用它来封装Conv1d序列的应用程序。ELU是类似于实验3中介绍的ReLU的非线性函数，但是它不是将值裁剪到0以下，而是对它们求幂。ELU已经被证明是卷积层之间使用的一种很有前途的非线性(Clevert et al.， 2015)。</p>
<p>在本例中，我们将每个卷积的通道数与num_channels超参数绑定。我们可以选择不同数量的通道分别进行卷积运算。这样做需要优化更多的超参数。我们发现256足够大，可以使模型达到合理的性能。</p>
<p>Example 4-19. The CNN-based SurnameClassifier</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SurnameClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, initial_num_channels, num_classes, num_channels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        创建一个姓氏分类器。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            initial_num_channels (int): 输入特征向量的大小</span></span><br><span class="line"><span class="string">            num_classes (int): 输出预测向量的大小</span></span><br><span class="line"><span class="string">            num_channels (int): 网络中要始终使用的恒定通道大小</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SurnameClassifier, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.convnet = nn.Sequential(</span><br><span class="line">            nn.Conv1d(in_channels=initial_num_channels, </span><br><span class="line">                      out_channels=num_channels, kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class="line">                      kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class="line">                      kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.ELU(),</span><br><span class="line">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class="line">                      kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.ELU()</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Linear(num_channels, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_surname, apply_softmax=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;分类器的前向传播</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            x_surname (torch.Tensor): 输入数据张量。</span></span><br><span class="line"><span class="string">                x_surname.shape 应为 (batch, initial_num_channels, max_surname_length)</span></span><br><span class="line"><span class="string">            apply_softmax (bool): softmax 激活的标志</span></span><br><span class="line"><span class="string">                如果与交叉熵损失一起使用，则应为 False</span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            结果张量。tensor.shape 应为 (batch, num_classes)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        features = self.convnet(x_surname).squeeze(dim=<span class="number">2</span>)</span><br><span class="line">       </span><br><span class="line">        prediction_vector = self.fc(features)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> apply_softmax:</span><br><span class="line">            prediction_vector = F.softmax(prediction_vector, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> prediction_vector</span><br></pre></td></tr></table></figure>
<h2 id="The-Training-Routine-1"><a href="#The-Training-Routine-1" class="headerlink" title="The Training Routine"></a>The Training Routine</h2><p>训练程序包括以下似曾相识的的操作序列:实例化数据集,实例化模型,实例化损失函数,实例化优化器,遍历数据集的训练分区和更新模型参数,遍历数据集的验证分区和测量性能,然后重复数据集迭代一定次数。此时，这是本书到目前为止的第三个训练例程实现，应该将这个操作序列内部化。对于这个例子，我们将不再详细描述具体的训练例程，因为它与“示例:带有多层感知器的姓氏分类”中的例程完全相同。但是，输入参数是不同的，可以在示例4-20中看到。</p>
<p>Example 4-20. Input arguments to the CNN surname classifier</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">args = Namespace(</span><br><span class="line">    <span class="comment"># 数据和路径信息</span></span><br><span class="line">    surname_csv=<span class="string">&quot;data/surnames/surnames_with_splits.csv&quot;</span>,</span><br><span class="line">    vectorizer_file=<span class="string">&quot;vectorizer.json&quot;</span>,</span><br><span class="line">    model_state_file=<span class="string">&quot;model.pth&quot;</span>,</span><br><span class="line">    save_dir=<span class="string">&quot;model_storage/ch4/cnn&quot;</span>,</span><br><span class="line">    <span class="comment"># 模型超参数</span></span><br><span class="line">    hidden_dim=<span class="number">100</span>,</span><br><span class="line">    num_channels=<span class="number">256</span>,</span><br><span class="line">    <span class="comment"># 训练超参数</span></span><br><span class="line">    seed=<span class="number">1337</span>,</span><br><span class="line">    learning_rate=<span class="number">0.001</span>,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    num_epochs=<span class="number">100</span>,</span><br><span class="line">    early_stopping_criteria=<span class="number">5</span>,</span><br><span class="line">    dropout_p=<span class="number">0.1</span>,</span><br><span class="line">    <span class="comment"># 运行时选项</span></span><br><span class="line">    cuda=<span class="literal">False</span>,</span><br><span class="line">    reload_from_files=<span class="literal">False</span>,</span><br><span class="line">    expand_filepaths_to_save_dir=<span class="literal">True</span>,</span><br><span class="line">    catch_keyboard_interrupt=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="Model-Evaluation-and-Prediction-1"><a href="#Model-Evaluation-and-Prediction-1" class="headerlink" title="Model Evaluation and Prediction"></a>Model Evaluation and Prediction</h2><p>要理解模型的性能，需要对性能进行定量和定性的度量。下面将描述这两个度量的基本组件。</p>
<h3 id="Evaluating-on-the-Test-Dataset"><a href="#Evaluating-on-the-Test-Dataset" class="headerlink" title="Evaluating on the Test Dataset"></a>Evaluating on the Test Dataset</h3><p>正如“示例:带有多层感知器的姓氏分类”中的示例与本示例之间的训练例程没有变化一样，执行评估的代码也没有变化。总之，调用分类器的<code>eval()</code>方法来防止反向传播，并迭代测试数据集。与 MLP 约 50% 的性能相比，该模型的测试集性能准确率约为56%。尽管这些性能数字绝不是这些特定架构的上限，但是通过一个相对简单的CNN模型获得的改进应该足以让您在文本数据上尝试CNNs。</p>
<h3 id="Classifying-or-retrieving-top-predictions-for-a-new-surname"><a href="#Classifying-or-retrieving-top-predictions-for-a-new-surname" class="headerlink" title="Classifying or retrieving top predictions for a new surname"></a>Classifying or retrieving top predictions for a new surname</h3><p>在本例中，<code>predict_nationality()</code>函数的一部分发生了更改，如例4-21:我们没有使用视图方法重塑新创建的数据张量以添加批处理维度，而是使用PyTorch的<code>unsqueeze()</code>函数在批处理应该在的位置添加大小为1的维度。相同的更改反映在<code>predict_topk_nationality()</code>函数中。</p>
<p>Example 4-21. Using the trained model to make predictions</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_nationality</span>(<span class="params">surname, classifier, vectorizer</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测姓氏的国籍</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        surname (str): 要分类的姓氏</span></span><br><span class="line"><span class="string">        classifier (SurnameClassifer): 分类器的实例</span></span><br><span class="line"><span class="string">        vectorizer (SurnameVectorizer): 相应的向量化器</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dict: 包含最可能的国籍及其概率的字典</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 向量化姓氏</span></span><br><span class="line">    vectorized_surname = vectorizer.vectorize(surname)</span><br><span class="line">    <span class="comment"># 将向量化的姓氏转换为PyTorch张量，并添加批次维度</span></span><br><span class="line">    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 使用分类器进行预测，并对输出进行softmax处理</span></span><br><span class="line">    result = classifier(vectorized_surname, apply_softmax=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取概率最高的国籍及其对应的索引</span></span><br><span class="line">    probability_values, indices = result.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">    index = indices.item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过索引查找预测的国籍，并获取其概率值</span></span><br><span class="line">    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)</span><br><span class="line">    probability_value = probability_values.item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回预测结果的字典</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;nationality&#x27;</span>: predicted_nationality, <span class="string">&#x27;probability&#x27;</span>: probability_value&#125;</span><br></pre></td></tr></table></figure>
<p>示例：<br><img src="/%5Cphoto%5Cimage-17.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_topk_nationality</span>(<span class="params">surname, classifier, vectorizer, k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测姓氏的前K个可能的国籍</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        surname (str): 要分类的姓氏</span></span><br><span class="line"><span class="string">        classifier (SurnameClassifer): 分类器的实例</span></span><br><span class="line"><span class="string">        vectorizer (SurnameVectorizer): 相应的向量化器</span></span><br><span class="line"><span class="string">        k (int): 要返回的前K个国籍数量</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: 字典的列表，每个字典包含一个国籍和其对应的概率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 向量化姓氏</span></span><br><span class="line">    vectorized_surname = vectorizer.vectorize(surname)</span><br><span class="line">    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 使用分类器进行预测，并对输出进行softmax处理，获取前K个国籍及其概率</span></span><br><span class="line">    prediction_vector = classifier(vectorized_surname, apply_softmax=<span class="literal">True</span>)</span><br><span class="line">    probability_values, indices = torch.topk(prediction_vector, k=k)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取预测结果中概率最高的K个国籍及其概率值</span></span><br><span class="line">    probability_values = probability_values[<span class="number">0</span>].detach().numpy()</span><br><span class="line">    indices = indices[<span class="number">0</span>].detach().numpy()</span><br><span class="line">    </span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> kth_index <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        <span class="comment"># 通过索引查找国籍，并获取其对应的概率值，将结果存储为字典列表</span></span><br><span class="line">        nationality = vectorizer.nationality_vocab.lookup_index(indices[kth_index])</span><br><span class="line">        probability_value = probability_values[kth_index]</span><br><span class="line">        results.append(&#123;<span class="string">&#x27;nationality&#x27;</span>: nationality, </span><br><span class="line">                        <span class="string">&#x27;probability&#x27;</span>: probability_value&#125;)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">new_surname = <span class="built_in">input</span>(<span class="string">&quot;Enter a surname to classify: &quot;</span>)</span><br><span class="line"></span><br><span class="line">k = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;How many of the top predictions to see? &quot;</span>))</span><br><span class="line"><span class="keyword">if</span> k &gt; <span class="built_in">len</span>(vectorizer.nationality_vocab):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Sorry! That&#x27;s more than the # of nationalities we have.. defaulting you to max size :)&quot;</span>)</span><br><span class="line">    k = <span class="built_in">len</span>(vectorizer.nationality_vocab)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 调用函数进行预测并打印结果</span></span><br><span class="line">predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Top &#123;&#125; predictions:&quot;</span>.<span class="built_in">format</span>(k))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===================&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> prediction <span class="keyword">in</span> predictions:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; -&gt; &#123;&#125; (p=&#123;:0.2f&#125;)&quot;</span>.<span class="built_in">format</span>(new_surname,</span><br><span class="line">                                        prediction[<span class="string">&#x27;nationality&#x27;</span>],</span><br><span class="line">                                        prediction[<span class="string">&#x27;probability&#x27;</span>]))</span><br></pre></td></tr></table></figure>
<p>示例：<br><img src="/%5Cphoto%5Cimage-18.png"></p>
<h2 id="Miscellaneous-Topics-in-CNNs"><a href="#Miscellaneous-Topics-in-CNNs" class="headerlink" title="Miscellaneous Topics in CNNs"></a>Miscellaneous Topics in CNNs</h2><p>为了结束我们的讨论，我们概述了几个其他的主题，这些主题是CNNs的核心，但在它们的共同使用中起着主要作用。特别是，你将看到Pooling操作、batch Normalization、network-in-network connection和residual connections的描述。</p>
<h3 id="Pooling-Operation"><a href="#Pooling-Operation" class="headerlink" title="Pooling Operation"></a>Pooling Operation</h3><p>Pooling是将高维特征映射总结为低维特征映射的操作。卷积的输出是一个特征映射。feature map中的值总结了输入的一些区域。由于卷积计算的重叠性，许多计算出的特征可能是冗余的。Pooling是一种将高维(可能是冗余的)特征映射总结为低维特征映射的方法。在形式上，池是一种像sum、mean或max这样的算术运算符，系统地应用于feature map中的局部区域，得到的池操作分别称为sum pooling、average pooling和max pooling。池还可以作为一种方法，将较大但较弱的feature map的统计强度改进为较小但较强的feature map。下图说明了Pooling。</p>
<p><img src="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596120.png"><br><font color='grey'><center> 这里所示的池操作在功能上与卷积相同：它应用于输入矩阵中的不同位置。然而，池操作不是将输入矩阵的值相乘和求和，而是应用一些函数G来汇集这些值。G可以是任何运算，但求和、求最大值和计算平均值是最常见的。</center></font></p>
<h3 id="Batch-Normalization-BatchNorm"><a href="#Batch-Normalization-BatchNorm" class="headerlink" title="Batch Normalization (BatchNorm)"></a>Batch Normalization (BatchNorm)</h3><p>批处理标准化是设计网络时经常使用的一种工具。BatchNorm对CNN的输出进行转换，方法是将激活量缩放为零均值和单位方差。它用于Z-transform的平均值和方差值每批更新一次，这样任何单个批中的波动都不会太大地移动或影响它。BatchNorm允许模型对参数的初始化不那么敏感，并且简化了学习速率的调整(Ioffe and Szegedy, 2015)。在PyTorch中，批处理规范是在nn模块中定义的。例4-22展示了如何用卷积和线性层实例化和使用批处理规范。</p>
<p>Example 4-22. Using s Conv1D layer with batch normalization.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">    self.conv1 = nn.Conv1d(in_channels=<span class="number">1</span>, out_channels=<span class="number">10</span>,</span><br><span class="line">                           kernel_size=<span class="number">5</span>,</span><br><span class="line">                           stride=<span class="number">1</span>)</span><br><span class="line">    self.conv1_bn = nn.BatchNorm1d(num_features=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">   <span class="comment"># ...</span></span><br><span class="line">   x = F.relu(self.conv1(x))</span><br><span class="line">   x = self.conv1_bn(x)</span><br></pre></td></tr></table></figure>
<h3 id="Network-in-Network-Connections-1x1-Convolutions"><a href="#Network-in-Network-Connections-1x1-Convolutions" class="headerlink" title="Network-in-Network Connections (1x1 Convolutions)"></a>Network-in-Network Connections (1x1 Convolutions)</h3><p>Network-in-Network (NiN)连接是具有<code>kernel_size=1</code>的卷积内核，具有一些有趣的特性。具体来说，1x1卷积就像通道之间的一个完全连通的线性层。这在从多通道feature map映射到更浅的feature map时非常有用。在下图中，我们展示了一个应用于输入矩阵的NiN连接。它将两个通道简化为一个通道。因此，NiN或1x1卷积提供了一种廉价的方法来合并参数较少的额外非线性(Lin et al.， 2013)。</p>
<p><img src="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596110.png"><br><font color='grey'><center> 一个1×1卷积运算的例子。观察1×1卷积是如何进行的操作将通道数从两个减少到一个。</center></font></p>
<h3 id="Residual-Connections-Residual-Block"><a href="#Residual-Connections-Residual-Block" class="headerlink" title="Residual Connections&#x2F;Residual Block"></a>Residual Connections&#x2F;Residual Block</h3><p>CNNs中最重要的趋势之一是Residual connection，它支持真正深层的网络(超过100层)。它也称为skip connection。如果将卷积函数表示为conv，则residual block的输出如下:</p>
<p>$$output &#x3D; conv ( input ) + input$$</p>
<p>然而，这个操作有一个隐含的技巧，如图下所示。对于要添加到卷积输出的输入，它们必须具有相同的形状。为此，标准做法是在卷积之前应用填充。在图4-15中，填充尺寸为1，卷积大小为3。</p>
<p><img src="https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596102.png"><br><font color='grey'><center> 残差连接是一种将原始矩阵加到卷积输出上的方法。当将卷积层应用于输入矩阵并将结果添加到输入矩阵时，以上直观地描述了这一点。创建与输入大小相同的输出的通用超参数设置是让kernel_size&#x3D;3和padding&#x3D;1。一般来说，任何带 adding&#x3D;(floor(kernel_size)&#x2F;2-1) 的奇数内核大小都将导致与输入大小相同的输出。卷积层产生的矩阵被加到输入端，最后的结果是剩余连接计算的输出端。</center></font></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">1.</span> <span class="nav-text">感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">1.2.</span> <span class="nav-text">单层感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">1.2.1.</span> <span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.2.</span> <span class="nav-text">单层感知机存在的问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">1.3.</span> <span class="nav-text">多层感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E7%BB%93%E6%9E%84"><span class="nav-number">1.3.1.</span> <span class="nav-text">多层感知机的结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82"><span class="nav-number">1.3.2.</span> <span class="nav-text">隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.3.3.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RELU%E5%87%BD%E6%95%B0"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">RELU函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sigmoid"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">sigmoid</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tanh"><span class="nav-number">1.3.3.3.</span> <span class="nav-text">tanh</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.3.4.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9D%87%E6%96%B9%E5%B7%AE%E6%8D%9F%E5%A4%B1%EF%BC%88Mean-Square-Error%EF%BC%8CMSE%EF%BC%89"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">均方差损失（Mean Square Error，MSE）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F"><span class="nav-number">1.3.4.1.1.</span> <span class="nav-text">计算方式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%EF%BC%88cross-entropy-loss-%EF%BC%89"><span class="nav-number">1.3.4.2.</span> <span class="nav-text">交叉熵损失（cross-entropy loss ）</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%A7%93%E6%B0%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C"><span class="nav-number">2.</span> <span class="nav-text">基于多层感知机的姓氏分类网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Surname-Dataset"><span class="nav-number">2.1.</span> <span class="nav-text">The Surname Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vocabulary-Vectorizer-and-DataLoader"><span class="nav-number">2.2.</span> <span class="nav-text">Vocabulary, Vectorizer, and DataLoader</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#THE-VOCABULARY-CLASS"><span class="nav-number">2.2.1.</span> <span class="nav-text">THE VOCABULARY CLASS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#THE-SURNAMEVECTORIZER"><span class="nav-number">2.2.2.</span> <span class="nav-text">THE SURNAMEVECTORIZER</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Surname-Classifier-Model"><span class="nav-number">2.3.</span> <span class="nav-text">The Surname Classifier Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Training-Routine"><span class="nav-number">2.4.</span> <span class="nav-text">The Training Routine</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#THE-TRAINING-LOOP"><span class="nav-number">2.4.1.</span> <span class="nav-text">THE TRAINING LOOP</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Evaluation-and-Prediction"><span class="nav-number">2.5.</span> <span class="nav-text">Model Evaluation and Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#EVALUATING-ON-THE-TEST-DATASET"><span class="nav-number">2.5.1.</span> <span class="nav-text">EVALUATING ON THE TEST DATASET</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CLASSIFYING-A-NEW-SURNAME"><span class="nav-number">2.5.2.</span> <span class="nav-text">CLASSIFYING A NEW SURNAME</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RETRIEVING-THE-TOP-K-PREDICTIONS-FOR-A-NEW-SURNAME"><span class="nav-number">2.5.3.</span> <span class="nav-text">RETRIEVING THE TOP-K PREDICTIONS FOR A NEW SURNAME</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regularizing-MLPs-Weight-Regularization-and-Structural-Regularization-or-Dropout"><span class="nav-number">2.6.</span> <span class="nav-text">Regularizing MLPs: Weight Regularization and Structural Regularization (or Dropout)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DROPOUT"><span class="nav-number">2.6.1.</span> <span class="nav-text">DROPOUT</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">3.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B-1"><span class="nav-number">3.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-1"><span class="nav-number">3.2.</span> <span class="nav-text">模型结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Convolutional-layer"><span class="nav-number">3.2.1.</span> <span class="nav-text">Convolutional layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pooling-Layer"><span class="nav-number">3.2.2.</span> <span class="nav-text">Pooling Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fully-connected-layer"><span class="nav-number">3.2.3.</span> <span class="nav-text">Fully connected layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DIMENSION-OF-THE-CONVOLUTION-OPERATION"><span class="nav-number">3.2.4.</span> <span class="nav-text">DIMENSION OF THE CONVOLUTION OPERATION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CHANNELS"><span class="nav-number">3.2.5.</span> <span class="nav-text">CHANNELS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KERNEL-SIZE"><span class="nav-number">3.2.6.</span> <span class="nav-text">KERNEL SIZE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#STRIDE"><span class="nav-number">3.2.7.</span> <span class="nav-text">STRIDE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PADDING"><span class="nav-number">3.2.8.</span> <span class="nav-text">PADDING</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DILATION"><span class="nav-number">3.2.9.</span> <span class="nav-text">DILATION</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%A7%93%E6%B0%8F%E5%88%86%E7%B1%BB"><span class="nav-number">4.</span> <span class="nav-text">基于卷积神经网络的姓氏分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Implementing-CNNs-in-PyTorch"><span class="nav-number">4.1.</span> <span class="nav-text">Implementing CNNs in PyTorch</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vocabulary-Vectorizer-and-DataLoader-1"><span class="nav-number">4.2.</span> <span class="nav-text">Vocabulary, Vectorizer, and DataLoader</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reimplementing-the-SurnameClassifier-with-Convolutional-Networks"><span class="nav-number">4.3.</span> <span class="nav-text">Reimplementing the SurnameClassifier with Convolutional Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Training-Routine-1"><span class="nav-number">4.4.</span> <span class="nav-text">The Training Routine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Evaluation-and-Prediction-1"><span class="nav-number">4.5.</span> <span class="nav-text">Model Evaluation and Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluating-on-the-Test-Dataset"><span class="nav-number">4.5.1.</span> <span class="nav-text">Evaluating on the Test Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Classifying-or-retrieving-top-predictions-for-a-new-surname"><span class="nav-number">4.5.2.</span> <span class="nav-text">Classifying or retrieving top predictions for a new surname</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Miscellaneous-Topics-in-CNNs"><span class="nav-number">4.6.</span> <span class="nav-text">Miscellaneous Topics in CNNs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pooling-Operation"><span class="nav-number">4.6.1.</span> <span class="nav-text">Pooling Operation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Normalization-BatchNorm"><span class="nav-number">4.6.2.</span> <span class="nav-text">Batch Normalization (BatchNorm)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Network-in-Network-Connections-1x1-Convolutions"><span class="nav-number">4.6.3.</span> <span class="nav-text">Network-in-Network Connections (1x1 Convolutions)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Residual-Connections-Residual-Block"><span class="nav-number">4.6.4.</span> <span class="nav-text">Residual Connections&#x2F;Residual Block</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="闲云"
      src="/images/Christina.jpg">
  <p class="site-author-name" itemprop="name">闲云</p>
  <div class="site-description" itemprop="description">雄关漫道真如铁 而今迈步从头越</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">1</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">闲云</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">33k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">30 分钟</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>
        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
