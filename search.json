[{"title":"2025高等数学辅导讲义严选题多元函数微分学48题","url":"/2024/06/12/HM24-6-11/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><p>求椭球面 $\\frac{x^2}{3}+\\frac{y^2}{2}+z^2&#x3D;1$ 被平面 $x+y+z&#x3D;0$ 截得的椭圆的长半轴与短半轴<br>分析: 将空间几何问题转化为函数极值问题，利用拉格朗日乘数法解出最大值和最小值，即可得到结果。</p>\n<p>解: 可知平面 $x+y+z&#x3D;0$ 过点 $(0,0,0)$ ，也就是过椭球面中心，设椭圆上任意一点为 $(x, y, z)$ ，则坐标原点到 $(x, y, z)$ 的距离为 $d&#x3D;\\sqrt{x^2+y^2+z^2}$, 设</p>\n<span id=\"more\"></span>\n\n<p>$$<br>\\begin{gathered}<br>F(x)&#x3D;x^2+y^2+z^2+\\lambda\\left(\\frac{x^2}{3}+\\frac{y^2}{2}+z^2-1\\right)+\\mu(x+y+z) \\\\<br>\\left(\\begin{gather}<br>F_x^{\\prime}&#x3D;2x+\\frac{2 \\lambda}{3}x+\\mu&#x3D;0 \\\\<br>F_y^{\\prime}&#x3D;2y+\\lambda y+\\mu&#x3D;0  \\\\<br>F_z^{\\prime}&#x3D;2z+2 \\lambda z+\\mu&#x3D;0  \\\\<br>F_\\lambda^{\\prime}&#x3D;\\frac{x^2}{3}+\\frac{y^2}{2}+z^2-1&#x3D;0  \\\\<br>F_\\mu^{\\prime}&#x3D;x+y+z&#x3D;0<br>\\end{gather}\\right.<br>\\end{gathered}<br>$$</p>\n<p>由题意知，目标函数是 $x^2+y^2+z^2$ ，所以将 $(1) \\times x+(2) \\times y+(3) \\times z$ ，得<br>$$<br>2\\left(x^2+y^2+z^2\\right)+\\lambda\\left(\\frac{2 x^2}{3}+ y^2+2 z^2\\right)+\\mu(x+y+z)&#x3D;0 \\tag 6<br>$$</p>\n<p>根据(4), (5)式进一步化简 $(6)$ 式得 $2\\left( x^2+y^2+z^2 \\right)+2 \\lambda&#x3D;0$,即$\\lambda&#x3D;-d^2$<br>$(7)$同理再根据(1), (2), (3)式有<br>$$<br>-\\mu&#x3D; x(2+\\frac{ 2\\lambda}{3})&#x3D; y(2+ \\lambda)&#x3D; z(2+2 \\lambda)<br>$$</p>\n<p>将 $x$，$y$, $z$ 用表示$\\lambda$ 和$\\mu$ 表示，有 $x&#x3D;\\frac{-3 \\mu}{6+2 \\lambda} ， y&#x3D;\\frac{- \\mu}{2+ \\lambda} ,z&#x3D;\\frac{- \\mu}{2+2 \\lambda}$ ，再带入(5)式，有 $-\\frac{3 \\mu}{6+2 \\lambda} -\\frac{ \\mu}{2+ \\lambda} -\\frac{ \\mu}{2+2 \\lambda}&#x3D;0$ ，不难验证$\\mu&#x3D;0$时$x,y,z$均为$0$,与$(4)$相矛盾，化简得 $3 \\lambda^2+11 \\lambda+9&#x3D;0$， ，解得 $\\lambda_{1,2}&#x3D;\\frac{-11 \\pm \\sqrt{13}}{6}$ ，所以</p>\n<p>$$<br>d_{\\min }^2&#x3D;- \\lambda_1&#x3D;\\frac{11-\\sqrt{13}}{6}, d_{\\max }^2&#x3D;- \\lambda_2&#x3D;\\frac{11+\\sqrt{13}}{6}<br>$$</p>\n<p>即椭圆长半轴为 $\\sqrt{\\frac{11+\\sqrt{13}}{6}}$ ，短半轴为 $\\sqrt{\\frac{11-\\sqrt{13}}{6}}$</p>\n","categories":["高等数学"]},{"title":"2025张宇1000题提高篇第一章35题","url":"/2024/06/13/HM24-6-12/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><p>已知<br>$$<br>\\lim _{x \\rightarrow+\\infty} \\frac{\\int_0^x t^2 \\mathrm{e}^{x^2-t^2} \\mathrm{~d} t+a \\mathrm{e}^{x^2}}{x^b}&#x3D;-\\frac{1}{2}<br>$$<br>求 $a, b$ 的值.</p>\n<p>解：提出$\\mathrm{e}^{x^2}$,</p>\n<p>$$\\lim _{x \\rightarrow+\\infty}\\frac{ \\mathrm{e}^{x^2}\\left(\\int_0^x t^2 \\mathrm{e}^{-t^2}\\mathrm{~d} t+a\\right)}{x^b}&#x3D;-\\frac{1}{2}$$<br>可得</p>\n<span id=\"more\"></span>\n<p>$$\\lim _{x \\rightarrow+\\infty}\\int_0^x t^2 \\mathrm{e}^{-t^2} \\mathrm{~d} t+a&#x3D;0$$<br>将$e^{-t^2}$凑微分分部积分得</p>\n<p>$$a&#x3D;-\\frac{\\int_0^\\infty e^{-x^2}\\mathrm{~d}x}{2}&#x3D;-\\frac{\\sqrt\\pi}{4}$$</p>\n<p>原式即为<br>$$<br>\\lim _{x \\rightarrow+\\infty}\\frac{\\int_0^x t^2 \\mathrm{e}^{-t^2}\\mathrm{~d} t-\\frac{\\sqrt\\pi}{4}}{x^b \\mathrm{e}^{-x^2}}$$<br>使用洛必达法则得<br>$$<br>\\begin{aligned}<br>&amp;\\lim _{x \\rightarrow+\\infty} \\frac{x^2{e}^{-x^2}}{bx^{b-1} \\mathrm{e}^{-x^2}-2x^{b+1}e^{-x^2}}\\\\<br>&amp;&#x3D;\\lim _{x \\rightarrow+\\infty}\\frac{x^2}{bx^{b-1}-2x^{b+1}}\\\\<br>&amp;&#x3D;-\\frac{1}{2}<br>\\end{aligned}<br>$$<br>即 $b&#x3D;1$</p>\n<p>综上所述。$a&#x3D;-\\frac{\\sqrt\\pi}{4}$, $b&#x3D;1$</p>\n","categories":["高等数学"]},{"title":"Frullani 积分","url":"/2024/06/15/HM24-6-15/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><p>设函数 $f(x)$ 在 $[0,+\\infty)$ 上连续，且 $\\lim_{x \\rightarrow+\\infty} f(x)&#x3D;c \\in \\mathbb{R}$. 则对 $a, b&gt;0$ 成立</p>\n<p>$$\\int_0^{+\\infty} \\frac{f(a x)-f(b x)}{x} \\mathrm{~d} x&#x3D;(f(0)-c) \\ln \\frac{b}{a}$$</p>\n<p>如果将条件改为积分 $\\int_a^{+\\infty} \\frac{f(x)}{x} \\mathrm{~d} x$ 对某个 $a&gt;0$ 收敛，则有</p>\n<p>$$\\int_0^{+\\infty} \\frac{f(a x)-f(b x)}{x} \\mathrm{~d} x&#x3D;f(0) \\ln \\frac{b}{a}$$</p>\n<p>上述条件要求 $f(x)$ 在 $x&#x3D;0$ 处连续，也可减弱至 $\\lim_{x \\rightarrow 0^{+}} f(x)$ 存在，并将等式右侧的相应 $f(0)$ 做修改即可。</p>\n<p>下面给出第一条结论的证明,其余类似<span id=\"more\"></span></p>\n<p>$$<br>\\begin{aligned}<br>F(t_1,t_2)&amp;&#x3D;\\int_{t_1}^{t_2} \\frac{f(a x)-f(b x)}{x} \\mathrm{d} x\\\\<br>&amp;&#x3D;\\int_{t_1}^{t_2} \\frac{f(a x)}{ax}\\mathrm{d} (ax)-\\int_{t_1}^{t_2} \\frac{f(b x)}{bx}\\mathrm{d} (bx)\\\\<br>&amp;&#x3D;\\int_{at_1}^{at_2} \\frac{f(x)}{x}\\mathrm{d} x-\\int_{bt_1}^{bt_2} \\frac{f(x)}{x}\\mathrm{d}x\\\\<br>&amp;&#x3D;\\int_{at_1}^{\\xi} \\frac{f(x)}{x}\\mathrm{d} x-\\int_{bt_1}^{\\xi} \\frac{f(x)}{x}\\mathrm{d}x+\\int_{\\xi}^{at_2} \\frac{f(x)}{x}\\mathrm{d} x-\\int_{\\xi}^{bt_2} \\frac{f(x)}{x}\\mathrm{d}x\\\\<br>&amp;&#x3D;\\int_{at_1}^{bt_1} \\frac{f(x)}{x}\\mathrm{d} x-\\int_{bt_2}^{at_2} \\frac{f(x)}{x}\\mathrm{d}x\\\\<br>\\end{aligned}<br>$$</p>\n<p>$$\\begin{aligned}<br>&amp;\\lim_{\\substack{t_1 \\rightarrow 0 \\\\t_2\\rightarrow+\\infty}}F(t_1,t_2)&#x3D;\\lim_{\\substack{t_1 \\rightarrow 0 }}   \\int_{at_1}^{bt_1} \\frac{f(x)}{x}\\mathrm{d} x-\\lim_{\\substack{t_2 \\rightarrow +\\infty }}\\int_{bt_2}^{at_2} \\frac{f(x)}{x}\\mathrm{d}x\\\\<br>&amp;\\text{使用积分中值定理得}\\\\<br>&amp;\\lim_{\\substack{t_1 \\rightarrow 0 }}f(\\eta_1) \\int_{a t_1}^{b t_1} \\frac{1}{x} d x+ \\lim_{\\substack{t_2 \\rightarrow +\\infty }}f(\\eta_2) \\int_{b t_2}^{a t_2} \\frac{1}{x} d x. \\\\<br>&amp;&#x3D;\\lim_{\\substack{t_1 \\rightarrow 0 \\\\t_2\\rightarrow+\\infty}}\\left(f(\\eta_1)-f(\\eta_2) \\right)\\ln \\frac{b}{a}\\\\<br>&amp;0\\leq\\eta_1\\leq bt_1,min(at_2,bt_2)\\leq \\eta_2\\leq max (at_2,bt_2)\\\\<br>&amp;又f(x)在[0,+\\infty) 上连续，且 \\lim_{x \\rightarrow+\\infty} f(x)&#x3D;c,得\\\\<br>\\end{aligned}$$</p>\n<p>$$\\int_0^{+\\infty} \\frac{f(a x)-f(b x)}{x} \\mathrm{~d} x&#x3D;\\lim_{\\substack{t_1 \\rightarrow 0 \\\\t_2\\rightarrow+\\infty}}F(t_1,t_2)&#x3D;(f(0)-c) \\ln \\frac{b}{a}$$</p>\n","categories":["高等数学"]},{"title":"图像去噪","url":"/2024/06/26/cv1/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><p>为了操作方便，我们将原始输入的彩色图片转为黑白</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">rgb2gray</span>(<span class=\"params\">adr</span>):</span><br><span class=\"line\">    img=cv2.imread(adr)</span><br><span class=\"line\">    width = img.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    height = img.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    grayimg = np.zeros([width,height],np.uint8) </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(width):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(height):</span><br><span class=\"line\">            grayimg[i,j] = <span class=\"number\">0.299</span> * img[i,j][<span class=\"number\">0</span>] + <span class=\"number\">0.587</span> * img[i,j][<span class=\"number\">1</span>] </span><br><span class=\"line\">            +  <span class=\"number\">0.114</span> * img[i,j][<span class=\"number\">2</span>]        </span><br><span class=\"line\">    cv2.imshow(<span class=\"string\">&#x27;grayimage&#x27;</span>, grayimg)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> grayimg</span><br></pre></td></tr></table></figure>\n<span id=\"more\"></span>\n<p>原始图像</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/maid.jpg?raw=true\" alt=\"alt text\"></p>\n<p>灰度图像</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/cv.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"添加椒盐噪声\"><a href=\"#添加椒盐噪声\" class=\"headerlink\" title=\"添加椒盐噪声\"></a>添加椒盐噪声</h1><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">pepper_and_salt</span>(<span class=\"params\">img,percentage</span>):</span><br><span class=\"line\">    num=<span class=\"built_in\">int</span>(percentage*img.shape[<span class=\"number\">0</span>]*img.shape[<span class=\"number\">1</span>])<span class=\"comment\">#  椒盐噪声点数量</span></span><br><span class=\"line\">    random.randint(<span class=\"number\">0</span>, img.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">    img2=img.copy()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num):</span><br><span class=\"line\">        X=random.randint(<span class=\"number\">0</span>,img2.shape[<span class=\"number\">0</span>]-<span class=\"number\">1</span>)<span class=\"comment\">#从0到图像长度之间的一个随机整数</span></span><br><span class=\"line\">        Y=random.randint(<span class=\"number\">0</span>,img2.shape[<span class=\"number\">1</span>]-<span class=\"number\">1</span>)<span class=\"comment\">#因为是闭区间所以-1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> random.randint(<span class=\"number\">0</span>,<span class=\"number\">1</span>) ==<span class=\"number\">0</span>: <span class=\"comment\">#黑白色概率55开</span></span><br><span class=\"line\">            img2[X,Y] = (<span class=\"number\">255</span>)<span class=\"comment\">#白色</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            img2[X,Y] =(<span class=\"number\">0</span>)<span class=\"comment\">#黑色</span></span><br><span class=\"line\">    <span class=\"comment\">#cv2.imshow(&quot;shabi_pepper_and_salt&quot;,img2)</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> img2</span><br></pre></td></tr></table></figure>\n\n\n<p>加噪后图像</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/cv-1.jpg?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"基于像素的均值、中值滤波的去噪\"><a href=\"#基于像素的均值、中值滤波的去噪\" class=\"headerlink\" title=\"基于像素的均值、中值滤波的去噪\"></a>基于像素的均值、中值滤波的去噪</h1><h2 id=\"均值滤波\"><a href=\"#均值滤波\" class=\"headerlink\" title=\"均值滤波\"></a>均值滤波</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">mean_fliter</span>(<span class=\"params\">img,size</span>):</span><br><span class=\"line\">        edge=<span class=\"built_in\">int</span>((size-<span class=\"number\">1</span>)/<span class=\"number\">2</span>)</span><br><span class=\"line\">        width = img.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">        height = img.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">        img_fliter = np.zeros([width,height,<span class=\"number\">1</span>],np.uint8) </span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(width-size):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(height-size):</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> i &lt;= edge - <span class=\"number\">1</span> <span class=\"keyword\">or</span> i &gt;= height - <span class=\"number\">1</span> - edge <span class=\"keyword\">or</span> </span><br><span class=\"line\">                    j &lt;= edge - <span class=\"number\">1</span> <span class=\"keyword\">or</span> j &gt;= height - edge - <span class=\"number\">1</span>:</span><br><span class=\"line\">                        img_fliter[i, j] = img[i, j]</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                        img_fliter[i, j] = np.mean(img[i - edge:i + edge + <span class=\"number\">1</span>,</span><br><span class=\"line\">                         j - edge:j + edge + <span class=\"number\">1</span>])</span><br><span class=\"line\">        cv2.imshow(<span class=\"string\">&quot;mean fliter&quot;</span>,img_fliter)</span><br></pre></td></tr></table></figure>\n<p>均值滤波图像</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/cv-2.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"中值滤波\"><a href=\"#中值滤波\" class=\"headerlink\" title=\"中值滤波\"></a>中值滤波</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">median_fliter</span>(<span class=\"params\">img,size</span>):</span><br><span class=\"line\">        edge=<span class=\"built_in\">int</span>((size-<span class=\"number\">1</span>)/<span class=\"number\">2</span>)</span><br><span class=\"line\">        width = img.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">        height = img.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">        img_fliter = np.zeros([width,height,<span class=\"number\">1</span>],np.uint8) </span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(width-size):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(height-size):</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> i &lt;= edge - <span class=\"number\">1</span> <span class=\"keyword\">or</span> i &gt;= height - <span class=\"number\">1</span> </span><br><span class=\"line\">                    - edge <span class=\"keyword\">or</span> j &lt;= edge - <span class=\"number\">1</span> <span class=\"keyword\">or</span> j &gt;= height - edge - <span class=\"number\">1</span>:</span><br><span class=\"line\">                        img_fliter[i, j] = img[i, j]</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                        img_fliter[i, j] = np.median(img[i - edge:i + edge + <span class=\"number\">1</span>, </span><br><span class=\"line\">                        j - edge:j + edge + <span class=\"number\">1</span>])</span><br><span class=\"line\">        cv2.imshow(<span class=\"string\">&quot;meadian fliter&quot;</span>,img_fliter)</span><br></pre></td></tr></table></figure>\n<p>中值滤波图像</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/cv-3.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"基于卷积的均值滤波\"><a href=\"#基于卷积的均值滤波\" class=\"headerlink\" title=\"基于卷积的均值滤波\"></a>基于卷积的均值滤波</h1><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">kernel = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">                   [<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">2</span>],</span><br><span class=\"line\">                   [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]])/<span class=\"number\">16</span> <span class=\"comment\"># 定义卷积核</span></span><br><span class=\"line\">output = cv2.filter2D(img2,-<span class=\"number\">1</span>,kernel)<span class=\"comment\"># 进行卷积操作</span></span><br></pre></td></tr></table></figure>\n<p>卷积图像<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/cv-5.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"非局部均值去噪\"><a href=\"#非局部均值去噪\" class=\"headerlink\" title=\"非局部均值去噪\"></a>非局部均值去噪</h1><h2 id=\"算法原理\"><a href=\"#算法原理\" class=\"headerlink\" title=\"算法原理\"></a>算法原理</h2><p>非局部均值去噪 (NL-means)<br>非局部均值 (NL-means) 是近年来提出的一项新型的去噪技术。该方法充分利用了图像中的几余信息，在去噪的同时能最大程度地保持图像的细节特征。基本思想是：当前像素的估计值由图像中与它具有相似令域结构的像素加权平均得到。</p>\n<p>理论上，该算法需要在整个图像范围内判断像素间的相似度，也就是说，每处理一个像素点时，都要计算它与图像中所有像素点间的相似度。但是考虑到效率问题，实现的时候，会设定两个固定大小的窗口: 搜索窗口 $(D \\times D, D&#x3D;2 * D s+1)$ 和邻域窗口 $(d \\times d, d&#x3D;2 * d s+1)$ 。邻域窗口在搜索窗口中滑动，根据邻域间的相似性确定像素的权值。</p>\n<p>下图是NL-means算法执行过程，大窗口是以目标像素 $x$ 为中心的搜索窗口，两个灰色小窗口分别是以 $x 、 y$ 为中心的邻域窗口。其中以 $y$ 为中心的邻域窗口在搜索窗口中滑动，通过计算两个邻域窗口间的相似程度为 $y$赋以权值$w(x, y)$ 。</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/cv-6.png?raw=true\" alt=\"alt text\"></p>\n<p>设含噪声图像为 $v$ ，去噪后的图像为 $\\tilde{u}$ 。 $\\tilde{u}$ 中像素点 $x$ 处的灰度值通过如下方式得到:<br>$$<br>\\tilde{u}(x)&#x3D;\\sum_{y \\in I} w(x, y) * v(y)<br>$$</p>\n<p>其中权值 $w(x, y)$ 表示像素点 $x$ 和 $Y$ 间的相似度，它的值由以 $x, y$ 为中心的矩形邻域 $V(x) 、 V(y)$ 间的距离 $|V(x)-V(y)|^2$ 决定:<br>$$<br>w(x, y)&#x3D;\\frac{1}{Z(x)} \\exp \\left(-\\frac{|V(x)-V(y)|^2}{h^2}\\right)<br>$$</p>\n<p>具中<br>$$<br>\\begin{aligned}<br>&amp; |V(x)-V(y)|^2&#x3D;\\frac{1}{d^2} \\sum_{|z|_{\\infty} \\leq d s}|v(x+z)-v(y+z)|^2 \\\\<br>&amp; Z(x)&#x3D;\\sum_y \\exp \\left(-\\frac{|V(x)-V(y)|^2}{h^2}\\right)<br>\\end{aligned}<br>$$<br>$Z(x)$ 为归一化系数， $h$ 为平滑参数，控制高斯函数的衰减程度。 $h$ 越大高斯函数变化越平缓，去噪水平越高，但同时也会导致图像越模糊。 $h$ 越小，边缘细节成分保持得越多，但会残留过多的噪声点。 $h$ 的具体取值应当以图像中的噪声水平为依据。</p>\n<h2 id=\"具体内容\"><a href=\"#具体内容\" class=\"headerlink\" title=\"具体内容\"></a>具体内容</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">NLM_fliter</span>(<span class=\"params\">img</span>):</span><br><span class=\"line\">    img2 = np.zeros([img.shape[<span class=\"number\">0</span>],img.shape[<span class=\"number\">1</span>]],np.uint8) </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>,img.shape[<span class=\"number\">0</span>]-<span class=\"number\">10</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>,img.shape[<span class=\"number\">1</span>]-<span class=\"number\">10</span>):</span><br><span class=\"line\">            search_window=img[i-<span class=\"number\">10</span>:i+<span class=\"number\">11</span>,j-<span class=\"number\">10</span>:j+<span class=\"number\">11</span>]</span><br><span class=\"line\">            patch=img[i-<span class=\"number\">3</span>:i+<span class=\"number\">4</span>,j-<span class=\"number\">3</span>:j+<span class=\"number\">4</span>]</span><br><span class=\"line\">            img2[i,j]=pixel_estimation(search_window,patch,<span class=\"number\">100</span>/<span class=\"number\">256</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> img2</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">pixel_estimation</span>(<span class=\"params\">search_window,patch,h</span>):</span><br><span class=\"line\">    window_height=search_window.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    window_width=search_window.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    patch_height=patch.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    patch_width=patch.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    similarity=np.zeros((window_height-patch_height//<span class=\"number\">2</span>*<span class=\"number\">2</span>,</span><br><span class=\"line\">    window_width-patch_width//<span class=\"number\">2</span>*<span class=\"number\">2</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(patch_height//<span class=\"number\">2</span>,window_height-patch_height//<span class=\"number\">2</span>):</span><br><span class=\"line\">         <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(patch_width//<span class=\"number\">2</span>,window_width-patch_width//<span class=\"number\">2</span>):</span><br><span class=\"line\">              temp=search_window[i-patch_height//<span class=\"number\">2</span>:i+patch_height//<span class=\"number\">2</span>+<span class=\"number\">1</span>,</span><br><span class=\"line\">              j-patch_width//<span class=\"number\">2</span>:j+patch_width//<span class=\"number\">2</span>+<span class=\"number\">1</span>]</span><br><span class=\"line\">              similarity[i-patch_height//<span class=\"number\">2</span>,j-patch_width//<span class=\"number\">2</span>]=weight_eucledian_distance(patch,temp,h)</span><br><span class=\"line\">    Z=np.<span class=\"built_in\">sum</span>(similarity)</span><br><span class=\"line\">    weights=np.zeros(search_window.shape)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(patch_height//<span class=\"number\">2</span>,window_height-patch_height//<span class=\"number\">2</span>):</span><br><span class=\"line\">         <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(patch_width//<span class=\"number\">2</span>,window_width-patch_width//<span class=\"number\">2</span>):</span><br><span class=\"line\">              weights[i,j]=<span class=\"number\">1</span>/Z*similarity[i-patch_height//<span class=\"number\">2</span>,j-patch_width//<span class=\"number\">2</span>]</span><br><span class=\"line\">    NLM_estimation=np.<span class=\"built_in\">sum</span>(weights*search_window)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> NLM_estimation.astype(np.uint8)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">weight_eucledian_distance</span>(<span class=\"params\">patch1,patch2,h</span>):</span><br><span class=\"line\">     <span class=\"keyword\">return</span> np.exp(-(patch1-patch2)**<span class=\"number\">2</span>/(h*h))</span><br></pre></td></tr></table></figure>\n\n<p>NLM图像<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/cv-4.png?raw=true\" alt=\"alt text\"></p>\n","categories":["Computer Vision"]},{"title":"无题𝅘𝅥𝅯","url":"/2024/06/14/HM24-6-13/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><p>$$<br>\\begin{array}{l}<br>f(x) \\text { 在 }(0,2 \\pi) 2 \\text { 阶可导 } ，f^{\\prime \\prime}(x) \\neq f(x) &amp; \\\\<br>\\exists \\xi \\in(0,2 \\pi) \\text { 使得 } \\tan \\xi&#x3D;\\frac{2 \\cdot f^{\\prime}(\\xi)}{f(\\xi)-f^{\\prime \\prime}(\\xi)}  \\text { 成立 }<br>\\end{array}<br>$$</p>\n<p>解：</p>\n<p>$$\\frac{sin(\\xi)}{cos(\\xi)}&#x3D;\\frac{2 \\cdot f^{\\prime}(\\xi)}{f(\\xi)-f^{\\prime \\prime}(\\xi)}$$<br>$由于f^{\\prime \\prime}(x)\\neq f(x),交叉相乘得$<span id=\"more\"></span><br>$$<br>\\begin{array}{l}<br>f(\\xi)sin(\\xi)-f^{\\prime \\prime}(\\xi)sin(\\xi)&#x3D;2f’(\\xi)cos(\\xi)\\\\<br>f(\\xi)sin(\\xi)-f’(\\xi)cos(\\xi)&#x3D;f^{\\prime \\prime}(\\xi)sin(\\xi)+f’(\\xi)cos(\\xi)\\\\<br>-\\left(f(\\xi)cos(\\xi)\\right)’&#x3D;(f’(\\xi)sin(\\xi))’\\\\<br>\\left(f(\\xi)cos(\\xi)+f’(\\xi)sin(\\xi)\\right)’&#x3D;0\\\\<br>\\left(f(\\xi)sin(\\xi)\\right)’’&#x3D;0<br>\\end{array}<br>$$<br>故令$F(x)&#x3D;f(x)sin(x),F(0)&#x3D;F(\\pi)&#x3D;F(2\\pi)&#x3D;0$</p>\n<p>$\\exists \\xi_1 \\in(0,\\pi),\\exists \\xi_2 \\in(\\pi,2\\pi)$,使得$F’(\\xi_1)&#x3D;F’(\\xi_2)&#x3D;0$</p>\n<p>$\\exists \\xi_3 \\in(0,2\\pi)$,使得$F’’(\\xi_3)&#x3D;0$</p>\n","categories":["高等数学"]},{"title":"2025武忠详高数辅导讲义严选题无穷级数第二题第三项的反例","url":"/2024/06/19/HM24-6-19/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><p>若 $\\lim _{n \\rightarrow \\infty} \\frac{u_n}{v_n}&#x3D;l \\neq 0$, 则 $\\sum_{n&#x3D;1}^{\\infty} u_n$ 与 $\\sum_{n&#x3D;1}^{\\infty} v_n$ 同敛散.</p>\n<p>反例<br>$$<br>\\begin{aligned}<br>&amp; u_n&#x3D;\\frac{(-1)^n}{\\sqrt{n}}+\\frac{1}{n} \\\\\\\\<br>&amp; v_n&#x3D;\\frac{(-1)^n}{\\sqrt{n}}<br>\\end{aligned}<br>$$<br>$$<br>\\lim _{n \\rightarrow \\infty}\\frac{\\frac{(-1)^n}{\\sqrt{n}}+\\frac{1}{n}}{\\frac{(-1)^n}{\\sqrt{n}}}&#x3D; \\frac{(-1)^n+\\frac{1}{\\sqrt{n}}}{(-1)^n}&#x3D;1<br>$$<br>显然 $\\sum_{n&#x3D;1}^{\\infty} v_n$收敛而 $\\sum_{n&#x3D;1}^{\\infty} u_n$发散</p>\n","categories":["高等数学"]},{"title":"无题♪","url":"/2024/07/12/HM24-7-2/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><p>设 $f(x)$ 在 $(-\\infty,+\\infty)$上二阶可导, 且 $f^{\\prime \\prime}(x) \\geqslant 0$.</p>\n<p>(1) 证明: 对于任意 $x_0, x \\in(-\\infty,+\\infty)$, 有 $f(x) \\geqslant f(x_0)+f^{\\prime}(x_0)(x-x_0)$;</p>\n<p>(2) 证明: 若存在常数 $M&gt;0$, 使得任意 $x \\in(-\\infty,+\\infty)$, 均有 $|f(x)| \\leqslant M$, 则 $f(x)$ 为常值函数</p>\n<p>泰勒展开得<br>$$f(x)&#x3D;f(x_0)+f^{\\prime}(x)(x-x_0)+\\frac{f^{\\prime \\prime}(\\xi)}{2}(x-x_0)^2$$<br>又$f^{\\prime \\prime}(x) \\geqslant 0$，得证 $f(x) \\geqslant f(x_0)+f^{\\prime}(x)(x-x_0)$</p>\n<p>设$\\exists x_0$ ,使得$f^{\\prime}(x_0) \\neq 0$, 有$f(x) \\geqslant f^{\\prime}(x_0) x+f(x_0)-x_0 f^{\\prime}(x_0)$  ,$f(x)$ 在一条直线上方，对于  $\\forall M$  ，均有 $|f(x)| &gt;M$  , 与$|f(x)| \\leqslant M$矛盾，故  $f’(x)$恒为 $0$ ，即$f(x)$为常函数</p>\n","categories":["高等数学"]},{"title":"边缘检测","url":"/2024/06/26/cv2/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"canny边缘检测算子\"><a href=\"#canny边缘检测算子\" class=\"headerlink\" title=\"canny边缘检测算子\"></a>canny边缘检测算子</h1><h2 id=\"理论原理\"><a href=\"#理论原理\" class=\"headerlink\" title=\"理论原理\"></a>理论原理</h2><p>Canny 算子是常用的边缘检测算法，其执行步骤如下:</p>\n<ol>\n<li>应用高斯滤波器平滑图像以去除噪声</li>\n<li>计算图像的强度梯度</li>\n<li>应用非最大抑制（Non-maximum suppression）消除边缘检测的虚假响应</li>\n<li>应用双阈值确定潜在边缘</li>\n<li>通过滞后 (hysteresis ) 方法跟踪边缘：通过抑制所有其他弱边缘和未连接到强边缘的边缘，完成边缘检测</li>\n</ol>\n<h3 id=\"高斯滤波\"><a href=\"#高斯滤波\" class=\"headerlink\" title=\"高斯滤波\"></a>高斯滤波</h3><span id=\"more\"></span>\n\n<p>通过高斯滤波去除图像噪声的影响，常用 $5 \\times 5$ 大小的高斯核，设置 $\\sigma&#x3D;1.4$<br>$$<br>\\mathbf{B}&#x3D;\\frac{1}{159}\\left[\\begin{array}{ccccc}<br>2 &amp; 4 &amp; 5 &amp; 4 &amp; 2 \\\\<br>4 &amp; 9 &amp; 12 &amp; 9 &amp; 4 \\\\<br>5 &amp; 12 &amp; 15 &amp; 12 &amp; 5 \\\\<br>4 &amp; 9 &amp; 12 &amp; 9 &amp; 4 \\\\<br>2 &amp; 4 &amp; 5 &amp; 4 &amp; 2<br>\\end{array}\\right] * \\mathbf{A}<br>$$</p>\n<p>高斯滤波核尺寸越大，越能够平滑噪声影响，但与此同时 Canny 算子的边缘检测的性能会降低</p>\n<h3 id=\"图像梯度计算\"><a href=\"#图像梯度计算\" class=\"headerlink\" title=\"图像梯度计算\"></a>图像梯度计算</h3><p>通过 Sobel 算子计算图像的水平和垂直反向导数，然后计算梯度大小和方向<br>$$<br>\\begin{aligned}<br>&amp; G&#x3D;\\sqrt{G_x^2+G_y^2} \\<br>&amp; \\theta&#x3D;\\arctan \\left(\\frac{G_y}{G_x}\\right)<br>\\end{aligned}<br>$$</p>\n<p>将梯度方向通过四舍五入方法归入到水平&#x2F;垂直&#x2F;对角 $\\left(0^{\\circ}, 45^{\\circ}, 90^{\\circ}\\right.$ 和 $\\left.135^{\\circ}\\right)$ ，比如 $\\left[0^{\\circ}, 22.5^{\\circ}\\right]$ 和 $\\left[157.5^{\\circ}, 180^{\\circ}\\right]$ 映射为 $0^{\\circ}$</p>\n<h3 id=\"非最大抑制\"><a href=\"#非最大抑制\" class=\"headerlink\" title=\"非最大抑制\"></a>非最大抑制</h3><p>非最大抑制是一种边缘细化技术。进行梯度计算后的图像边缘仍旧很模糊，边缘拥有多个候选位置，所以需要应用非最大抑制来寻找“最大”像素点，即具有最大强度值变化的位置，移除其他梯度值，保证边缘具有准确的响应。其原理如下</p>\n<ol>\n<li>将当前像素的边缘强度与像素在正梯度方向和负梯度方向上的边缘强度进行比较</li>\n<li>如果与相同方向的掩模中的其他像素相比，当前像素的边缘强度是最大的(例如，指向 $\\mathrm{y}$ 轴方向的像素将与垂直轴上方和下方的像素相比较) ，则该值将被保留。否则，该值将被抑制(去除梯度值为 0 )</li>\n</ol>\n<p>具体实现时，将连续梯度方向分类为一组小的离散方向，然后在上一步的输出(即边缘强度和梯度方向 )上移动 $3 \\times 3$ 滤波器。在每个像素处，如果中心像素的大小不大于渐变方向上两个相邻像素的大小，它将抑制中心像素的边缘强度(通过将其值设置为 0 )</p>\n<ul>\n<li>如果梯度角度为 $0^{\\circ}$ （即边缘在南北方向），如果其梯度大小大于东西方向像素处的大小，则该点将被视为在边缘上</li>\n<li>如果梯度角度为 $45^{\\circ}$ （即边缘位于西北-东南方向），如果其梯度大小大于东北和西南方向像素处的大小，则该点将被视为位于边缘上</li>\n<li>如果梯度角度为 $90^{\\circ}$ （即边缘在东西方向），如果其梯度大小大于南北方向像素处的大小，则该点将被视为在边缘上</li>\n<li>如果梯度角度为 $135^{\\circ}$ （即边缘位于东北-西南方向），如果其梯度大小大于西北和东南方向像素处的大小，则该点将被视为位于边缘上</li>\n</ul>\n<h3 id=\"双边阈值\"><a href=\"#双边阈值\" class=\"headerlink\" title=\"双边阈值\"></a>双边阈值</h3><p>通过非最大抑制，可以有效确定边缘的最大像素点，剩余的边缘像素提供了图像中真实边缘的更精确表示。但是，由于噪声和颜色变化，一些边缘像素仍然存在。为了去除这些虚假响应，必须滤除具有弱梯度值的边缘像素，并保留具有高梯度值的边缘像素。这是通过选择高阈值和低阈值来实现的。如果边缘像素的渐变值高于高阈值，则将其标记为强边缘像素。如果边缘像素的渐变值小于高阈值且大于低阈值，则将其标记为弱边缘像素。如果边缘像素的值小于低阈值，它将被抑制</p>\n<p>两个阈值是通过经验确定的，其定义将取决于给定输入图像的内容。通过其比率（ $upper: lower$ ）设置为 $2: 1$ 和 $3: 1$ 之间</p>\n<h3 id=\"通过滞后方法进行边缘追踪\"><a href=\"#通过滞后方法进行边缘追踪\" class=\"headerlink\" title=\"通过滞后方法进行边缘追踪\"></a>通过滞后方法进行边缘追踪</h3><p>经过上述步骤处理后，结果图像仅包含了强边缘像素和弱边缘像素。对于弱边缘像素而言，这些像素既可以从真实边缘提取，也可以从噪声&#x2F;颜色变化中提取</p>\n<p>通常，由真实边缘引起的弱边缘像素将连接到强边缘像素，而噪声响应则不连接。为了跟踪边缘连接，通过观察弱边缘像素及其 8 个邻域像素来进行blob分析。只要 blob 中包含一个强边缘像素，就可以将该弱边缘点识别为一个应该保留的点</p>\n<h2 id=\"具体内容\"><a href=\"#具体内容\" class=\"headerlink\" title=\"具体内容\"></a>具体内容</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> cv2</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.setrecursionlimit(<span class=\"number\">3000</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">trace</span>(<span class=\"params\">M_supp,edge,i,j,LT</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> edge[i,j]==<span class=\"number\">0</span>:</span><br><span class=\"line\">        edge[i,j]=<span class=\"number\">255</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> di,dj <span class=\"keyword\">in</span>[(-<span class=\"number\">1</span>,-<span class=\"number\">1</span>),(-<span class=\"number\">1</span>,<span class=\"number\">0</span>),(-<span class=\"number\">1</span>,<span class=\"number\">1</span>),(<span class=\"number\">0</span>,-<span class=\"number\">1</span>),(<span class=\"number\">0</span>,<span class=\"number\">1</span>),(<span class=\"number\">1</span>,-<span class=\"number\">1</span>),(<span class=\"number\">1</span>,<span class=\"number\">0</span>),(<span class=\"number\">1</span>,<span class=\"number\">1</span>)]:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> M_supp[i+di,j+dj]&gt;=LT:</span><br><span class=\"line\">                trace(M_supp,edge,i+di,j+dj,LT)</span><br><span class=\"line\">HT=<span class=\"number\">200</span></span><br><span class=\"line\">LT=<span class=\"number\">50</span></span><br><span class=\"line\">src=cv2.imread(<span class=\"string\">&#x27;E:\\shabi.jpg?raw=true&#x27;</span>)</span><br><span class=\"line\">dst=cv2.cvtColor(src,cv2.COLOR_BGRA2GRAY)</span><br><span class=\"line\">blur_dst=cv2.GaussianBlur(dst,(<span class=\"number\">5</span>,<span class=\"number\">5</span>),<span class=\"number\">0.5</span>)</span><br><span class=\"line\">dx=cv2.Sobel(blur_dst,cv2.CV_16S,<span class=\"number\">1</span>,<span class=\"number\">0</span>)</span><br><span class=\"line\">dy=cv2.Sobel(blur_dst,cv2.CV_16S,<span class=\"number\">0</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">M=<span class=\"built_in\">abs</span>(dx)+<span class=\"built_in\">abs</span>(dy)</span><br><span class=\"line\">theta=np.arctan2(dx,dy)</span><br><span class=\"line\">M_supp=np.zeros(M.shape)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,M.shape[<span class=\"number\">0</span>]-<span class=\"number\">1</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,M.shape[<span class=\"number\">1</span>]-<span class=\"number\">1</span>):</span><br><span class=\"line\">        x=dx[i,j]</span><br><span class=\"line\">        y=dy[i,j]</span><br><span class=\"line\">        angle=theta[i,j]/np.pi</span><br><span class=\"line\">        mag=M[i,j]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">abs</span>(angle)&lt;=<span class=\"number\">1</span>/<span class=\"number\">8.</span> <span class=\"keyword\">or</span> <span class=\"built_in\">abs</span>(angle)&gt;=<span class=\"number\">7</span>/<span class=\"number\">8.</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> mag&gt;=M[i-<span class=\"number\">1</span>,j] <span class=\"keyword\">and</span> mag&gt;=M[i+<span class=\"number\">1</span>,j]:</span><br><span class=\"line\">                M_supp[i,j]=mag</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> <span class=\"built_in\">abs</span>(angle-<span class=\"number\">1</span>/<span class=\"number\">2.</span>)&lt;=<span class=\"number\">1</span>/<span class=\"number\">8.</span> <span class=\"keyword\">or</span> <span class=\"built_in\">abs</span>(angle+<span class=\"number\">1</span>/<span class=\"number\">2.</span>)&lt;=<span class=\"number\">1</span>/<span class=\"number\">8.</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> mag&gt;=M[i,j-<span class=\"number\">1</span>] <span class=\"keyword\">and</span> mag&gt;=M[i,j+<span class=\"number\">1</span>]:</span><br><span class=\"line\">                M_supp[i,j]=mag</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> <span class=\"built_in\">abs</span>(angle-<span class=\"number\">3</span>/<span class=\"number\">4.</span>)&lt;=<span class=\"number\">1</span>/<span class=\"number\">8.</span> <span class=\"keyword\">or</span> <span class=\"built_in\">abs</span>(angle+<span class=\"number\">3</span>/<span class=\"number\">4.</span>)&lt;=<span class=\"number\">1</span>/<span class=\"number\">8.</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> mag&gt;=M[i+<span class=\"number\">1</span>,j-<span class=\"number\">1</span>] <span class=\"keyword\">and</span> mag&gt;=M[i-<span class=\"number\">1</span>,j+<span class=\"number\">1</span>]:</span><br><span class=\"line\">                M_supp[i,j]=mag</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> mag&gt;=M[i+<span class=\"number\">1</span>,j+<span class=\"number\">1</span>] <span class=\"keyword\">and</span> mag&gt;=M[i-<span class=\"number\">1</span>,j-<span class=\"number\">1</span>]:</span><br><span class=\"line\">                M_supp[i,j]=mag</span><br><span class=\"line\">M_supp=np.pad(M_supp,((<span class=\"number\">1</span>,<span class=\"number\">1</span>)),<span class=\"string\">&#x27;constant&#x27;</span>)</span><br><span class=\"line\">edge=np.zeros(M_supp.shape,dtype=np.uint8)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,M_supp.shape[<span class=\"number\">0</span>]-<span class=\"number\">1</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,M_supp.shape[<span class=\"number\">1</span>]-<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> M_supp[i,j]&gt;=HT:</span><br><span class=\"line\">            trace(M_supp,edge,i,j,LT)</span><br><span class=\"line\">edge=edge[<span class=\"number\">1</span>:-<span class=\"number\">1</span>,<span class=\"number\">1</span>:-<span class=\"number\">1</span>]</span><br><span class=\"line\">cv2.imshow(<span class=\"string\">&#x27;edge&#x27;</span>,edge)</span><br><span class=\"line\">cv2.imwrite(<span class=\"string\">&#x27;edge.jpg&#x27;</span>,edge)</span><br><span class=\"line\">cv2.waitKey(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<p>原始图像<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/cv1.jpg?raw=true\"></p>\n<p>边缘检测<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/edge.jpg?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"基于阈值的二值化分割\"><a href=\"#基于阈值的二值化分割\" class=\"headerlink\" title=\"基于阈值的二值化分割\"></a>基于阈值的二值化分割</h1><h2 id=\"人工选取阈值\"><a href=\"#人工选取阈值\" class=\"headerlink\" title=\"人工选取阈值\"></a>人工选取阈值</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">human</span>(<span class=\"params\">gray_img,threhold</span>):</span><br><span class=\"line\">    dst=gray_img.copy()</span><br><span class=\"line\">    dst[dst&lt;threhold]=<span class=\"number\">0</span></span><br><span class=\"line\">    dst[dst&gt;=threhold]=<span class=\"number\">255</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dst</span><br><span class=\"line\">dst2=human(src,<span class=\"number\">150</span>)</span><br></pre></td></tr></table></figure>\n<p>分割后图像<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/cv-7.png?raw=true\"></p>\n<h2 id=\"迭代法\"><a href=\"#迭代法\" class=\"headerlink\" title=\"迭代法\"></a>迭代法</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">iteration</span>(<span class=\"params\">gray_img</span>):</span><br><span class=\"line\">    dst=gray_img.copy()</span><br><span class=\"line\">    threhold=<span class=\"number\">128</span></span><br><span class=\"line\">    threhold1=<span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span>(<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(<span class=\"built_in\">abs</span>(threhold-threhold1)&lt;=<span class=\"number\">1</span>):<span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: threhold=threhold1</span><br><span class=\"line\">        n0=dst[np.where(dst&lt;threhold)]</span><br><span class=\"line\">        n1=dst[np.where(dst&gt;=threhold)]</span><br><span class=\"line\">        miu0=n0.mean() <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(n0)&gt;<span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\">        miu1=n1.mean() <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(n1)&gt;<span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\">        threhold1=(miu0+miu1)/<span class=\"number\">2</span></span><br><span class=\"line\">    dst[dst&lt;threhold]=<span class=\"number\">0</span></span><br><span class=\"line\">    dst[dst&gt;=threhold]=<span class=\"number\">255</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dst</span><br></pre></td></tr></table></figure>\n<p>分割结果<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/cv-8.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"OSTU算法\"><a href=\"#OSTU算法\" class=\"headerlink\" title=\"OSTU算法\"></a>OSTU算法</h2><h3 id=\"理论原理-1\"><a href=\"#理论原理-1\" class=\"headerlink\" title=\"理论原理\"></a>理论原理</h3><p>大津算法（OTSU算法）是一种常用的图像二值化方法，用于将灰度图像转化为二值图像。该算法由日本学者大津展之于1979年提出，因此得名。</p>\n<p>大津算法的核心思想是通过寻找一个阈值，将图像的像素分为两个类别：前景和背景。具体步骤如下：</p>\n<ol>\n<li>统计图像的灰度直方图，得到每个灰度级的像素数目。</li>\n<li>遍历所有可能的阈值（0到255），计算根据该阈值将图像分为前景和背景的类内方差。</li>\n<li>根据类内方差的最小值确定最佳阈值。<br>在大津算法中，类内方差是衡量前景和背景之间差异的度量。通过选择使类内方差最小的阈值，可以实现最佳的图像分割效果。</li>\n</ol>\n<p>大津算法的优点是简单易懂，计算效率高。它适用于灰度图像的二值化处理，特别是对于具有双峰直方图的图像效果更好。然而，该算法对于具有非双峰直方图的图像可能产生较差的分割结果。因此，在应用大津算法之前，需要对图像的直方图进行分析，确保适用性。</p>\n<p>大津算法在图像处理中被广泛应用，例如在文档图像处理、目标检测、图像分割等领域。</p>\n<p>下面推导类间方差函数：<br>设阈值为灰度 $\\mathrm{k}(k \\in[0, L-1], L&#x3D;256)$ 。这个阈值把图像像素分割成两类，C1类像素小于等于 $\\mathrm{k} ， \\mathrm{C} 2$ 类像素大于 $\\mathrm{k}$ 。设这两类像素各自的均值为 $m_1, m_2$ ，图像全局均值为 $m_G$ 。同时像素被分为 $\\mathrm{C} 1$ 和 $\\mathrm{C} 2$ 类的概率分别为 $p_1, p_2$ 。则有:<br>$$<br>\\begin{gathered}<br>p_1 m_1+p_2 m_2&#x3D;m_G \\\\<br>p_1+p_2&#x3D;1<br>\\end{gathered}<br>$$</p>\n<p>根据方差的概念，类间方差表达式为:<br>$$<br>\\sigma^2&#x3D;p_1\\left(m_1-m_G\\right)^2+p_2\\left(m_2-m_G\\right)^2<br>$$</p>\n<p>展开:<br>$$<br>\\sigma^2&#x3D;p_1 m_1^2+p_1 m_G^2-2 p_1 m_1 m_G+p_2 m_2^2+p_2 m_G^2-2 p_2 m_2 m_G<br>$$</p>\n<p>合并2，5及3，6项可得:<br>$$<br>\\sigma^2&#x3D;p_1 m_1^2+p_2 m_2^2+m_G^2-2 m_G^2&#x3D;p_1 m_1^2+p_2 m_2^2-m_G^2<br>$$</p>\n<p>我们再把 $m_G&#x3D;p_1 m_1+p_2 m_2$带回得<br>$$<br>\\sigma^2&#x3D;\\left(p_1-p_1^2\\right) m_1^2+\\left(p_2-p_2^2\\right) m_2^2-2 p_1 p_2 m_1 m_2<br>$$</p>\n<p>再注意到 $p_1+p_2&#x3D;1$ ，所以 $p_1-p_1^2&#x3D;p_1\\left(1-p_1\\right)&#x3D;p_1 p_2 ， p_2-p_2^2&#x3D;p_2\\left(1-p_2\\right)&#x3D;p_1 p_2$ ，从而得到:<br>$$<br>\\sigma^2&#x3D;p_1 p_2\\left(m_1-m_2\\right)^2<br>$$</p>\n<p>对于给定的阈值 $k$ ，我们可以统计出灰度级的分布列:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">灰度值</th>\n<th align=\"left\">0</th>\n<th align=\"left\">1</th>\n<th align=\"left\">$\\ldots$</th>\n<th align=\"left\">255</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">$p_i$</td>\n<td align=\"left\">$p_0$</td>\n<td align=\"left\">$p_1$</td>\n<td align=\"left\">$\\ldots$</td>\n<td align=\"left\">$p_{255}$</td>\n</tr>\n</tbody></table>\n<p>显然根据分布列性质有 $\\sum_{i&#x3D;0}^{L-1} p_i&#x3D;1$ (请注意这里的 $p_1, p_2$ 是分布列中的，不是上面的定义)那么有:<br>$$<br>p_1&#x3D;\\sum_{i&#x3D;0}^{k-1} p_i, p_2&#x3D;\\sum_{i&#x3D;k}^{L-1} p_i, m_1&#x3D;\\sum_{i&#x3D;0}^{k-1} i p_i, m_2&#x3D;\\sum_{i&#x3D;k}^{L-1} i p_i<br>$$</p>\n<p>将 $\\mathrm{k}$ 从 $[0, L-1]$ 遍历，找出使得 $\\sigma^2$ 最大的 $\\mathrm{k}$ 值，这个 $\\mathrm{k}$ 值就是阈值。<br>对于分割，这个分割就是二值化</p>\n<h3 id=\"具体内容-1\"><a href=\"#具体内容-1\" class=\"headerlink\" title=\"具体内容\"></a>具体内容</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">ostu</span>(<span class=\"params\">gray_img</span>):</span><br><span class=\"line\">    dst=gray_img.copy()</span><br><span class=\"line\">    h=dst.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    w=dst.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    threhold_t=<span class=\"number\">0</span></span><br><span class=\"line\">    max_g=<span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">255</span>):</span><br><span class=\"line\">        n0=dst[np.where(dst&lt;t)]</span><br><span class=\"line\">        n1=dst[np.where(dst&gt;=t)]</span><br><span class=\"line\">        w0=<span class=\"built_in\">len</span>(n0)/(h*w)</span><br><span class=\"line\">        w1=<span class=\"built_in\">len</span>(n1)/(h*w)</span><br><span class=\"line\">        u0=np.mean(n0) <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(n0)&gt;<span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">0.</span></span><br><span class=\"line\">        u1=np.mean(n1) <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(n1)&gt;<span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">0.</span></span><br><span class=\"line\">        g=w0*w1*(u0-u1)**<span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> g&gt;max_g:</span><br><span class=\"line\">            max_g=g</span><br><span class=\"line\">            threhold_t=t</span><br><span class=\"line\">    dst[dst&lt;threhold_t]=<span class=\"number\">0</span></span><br><span class=\"line\">    dst[dst&gt;=threhold_t]=<span class=\"number\">255</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dst</span><br></pre></td></tr></table></figure>\n<p>分割图像<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/cv-9.png?raw=true\" alt=\"alt text\"></p>\n","categories":["Computer Vision"]},{"title":"基于MATLAB的卷积演示系统","url":"/2023/08/28/dsplab1/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的和要求\"><a href=\"#实验目的和要求\" class=\"headerlink\" title=\"实验目的和要求\"></a>实验目的和要求</h1><h2 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h2><p>卷积作为一种基本的信号处理方法，拥有许 多不同的变体和应用。通过选择合适的卷积算 法，我们可以更好地处理各种类型的信号数据， 从而为我们解决实际问题提供更好的支持。 通过本次课程设计，使得学生能够充分了 解卷积的相关定义、计算和定理；使得学生能 够使用GUI&#x2F;app design进行图形化界面设计。</p>\n<h2 id=\"实验要求\"><a href=\"#实验要求\" class=\"headerlink\" title=\"实验要求\"></a>实验要求</h2><ul>\n<li>用matlab完成线性卷积的计算过程，并绘图;使用GUI&#x2F;app design设计一个线性卷积的基本演 示系统；要求两个卷积信号的参数可以自由给定；设计框图中直接包含“计算按钮” ，直接计算卷积结果</li>\n<li>分析卷积计算结果<span id=\"more\"></span></li>\n</ul>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理</h1><h2 id=\"线性卷积\"><a href=\"#线性卷积\" class=\"headerlink\" title=\"线性卷积\"></a>线性卷积</h2><p>设两序列 $x(n), h(n)$,其线性卷积定义为<br>$$<br>y(n)&#x3D;\\sum_{m&#x3D;-\\infty}^{\\infty} x(m) * h(n-m)<br>$$<br>(1)翻褶: $\\quad h(m) \\rightarrow h(-m)$</p>\n<p>(2) 移位: $h(-m) \\rightarrow h(n-m)$</p>\n<p>(3) 相乘: $x(m) h(n-m)$</p>\n<p>(4) 相加: $\\sum_{m&#x3D;-\\infty}^{\\infty} x(m) h(n-m)$</p>\n<h2 id=\"圆周卷积\"><a href=\"#圆周卷积\" class=\"headerlink\" title=\"圆周卷积\"></a>圆周卷积</h2><p>设 $x(n), h(n)$ 为 $N$ 点有限长序列, 其圆周卷积定义为<br>$$<br>y(n)&#x3D;\\sum_{m&#x3D;0}^{N-1} x_1(m) x_2((n-m))_N R_N(n)\\space\\space\\space\\space\\space\\space\\space\\space\\space N 点圆周卷积<br>$$<br>圆周卷积时域过程:</p>\n<ol>\n<li>补零: $x_1(n), x_2(n)$ 补零至 $N$ 点长</li>\n<li>周期延拓: $x_2(m)$ 周期延拓为 $x_2((m))_N$</li>\n<li>翻褶, 取主值序列: $x_2((-m))_N R_N(m)$</li>\n<li>圆周移位: $x_2((n-m))_N R_N(m)$</li>\n<li>相乘相加: $x_1(m) x_2((n-m))_N R_N(m)$</li>\n</ol>\n<p>圆周卷积频域过程<br>$$<br>\\begin{gathered}<br>D F T\\left[x_1(n)\\right]&#x3D;X_1(k) \\quad \\operatorname{DFT}\\left[x_2(n)\\right]&#x3D;X_2(k) \\\\<br>\\text { 若 } Y(k)&#x3D;X_1(k) \\cdot X_2(k) \\\\<br>y(n)&#x3D;I D F T[Y(k)]&#x3D;\\left[\\sum_{m&#x3D;0}^{N-1} x_1(m) x_2((n-m))_N\\right] R_N(n)<br>\\end{gathered}<br>$$</p>\n<h2 id=\"二者关系\"><a href=\"#二者关系\" class=\"headerlink\" title=\"二者关系\"></a>二者关系</h2><p>$$<br>x_1(n) Ⓝ  x_2(n)&#x3D;x_1(n) * x_2(n)\\left\\{\\begin{array}{c}<br>N \\geq N_1+N_2-1 \\\\<br>0 \\leq n \\leq N_1+N_2-2<br>\\end{array}\\right.<br>$$</p>\n<h1 id=\"实验方法与内容\"><a href=\"#实验方法与内容\" class=\"headerlink\" title=\"实验方法与内容\"></a>实验方法与内容</h1><h2 id=\"实验思路\"><a href=\"#实验思路\" class=\"headerlink\" title=\"实验思路\"></a>实验思路</h2><p>用matlab设计卷积演示界面，然后设计两个有限长序列，当演示时进行输入。要求能够体现翻褶，移位，相加，相乘。</p>\n<h2 id=\"需求分析\"><a href=\"#需求分析\" class=\"headerlink\" title=\"需求分析\"></a>需求分析</h2><h3 id=\"线性卷积需求分析\"><a href=\"#线性卷积需求分析\" class=\"headerlink\" title=\"线性卷积需求分析\"></a>线性卷积需求分析</h3><p>(1) 线性卷积的翻褶: $h(m) \\rightarrow h(-m)$, 可通过以 $\\mathrm{y}$ 轴为对称翻转。</p>\n<p>(2) 线性卷积的移位: 将 $h(-m)$ 移位 $n$, 得 $h(n-m)$ 。当 $n$ 为正, 右移, $n$ 为负, 左移。</p>\n<p>(3) 线性卷积的相乘：将对应点相乘。</p>\n<p>(4) 线性卷积的相加：将对应点相加。</p>\n<h3 id=\"圆周卷积需求分析\"><a href=\"#圆周卷积需求分析\" class=\"headerlink\" title=\"圆周卷积需求分析\"></a>圆周卷积需求分析</h3><p>圆周卷积可通过三个步骤完成</p>\n<p>(1) 初始化：确定 $\\mathrm{N}$ ，若序列长度小于 $\\mathrm{N}$ 则补 0</p>\n<p>(2) 右移: 将序列 $\\times(n)$ 循环右移, 移动 $N$ 次, 得到矩阵</p>\n<p>(3) 相乘：将得到的矩阵和第二个序列相乘, 得到结果</p>\n<p>然后进行时域演示和频域演示</p>\n<h1 id=\"实验结果及分析\"><a href=\"#实验结果及分析\" class=\"headerlink\" title=\"实验结果及分析\"></a>实验结果及分析</h1><h2 id=\"线性卷积演示\"><a href=\"#线性卷积演示\" class=\"headerlink\" title=\"线性卷积演示\"></a>线性卷积演示</h2><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp.gif?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"圆周卷积时域演示\"><a href=\"#圆周卷积时域演示\" class=\"headerlink\" title=\"圆周卷积时域演示\"></a>圆周卷积时域演示</h2><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-1.gif?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"圆周卷积频域演示\"><a href=\"#圆周卷积频域演示\" class=\"headerlink\" title=\"圆周卷积频域演示\"></a>圆周卷积频域演示</h2><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-2.png?raw=true\"></p>\n<h2 id=\"两者关系验证\"><a href=\"#两者关系验证\" class=\"headerlink\" title=\"两者关系验证\"></a>两者关系验证</h2><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-1.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-3.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-4.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-5.png?raw=true\" alt=\"alt text\"><br>当 $\\mathrm{L}&#x3D;\\mathrm{N} 1+\\mathrm{N} 2$ 时, 线性卷积和圆周卷积结果相等。</p>\n","categories":["Digital Signal Processing"]},{"title":"基于MATLAB的语音信号去噪","url":"/2023/11/15/dsplab2/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的和要求\"><a href=\"#实验目的和要求\" class=\"headerlink\" title=\"实验目的和要求\"></a>实验目的和要求</h1><p>综合应用数字信号处理的理论知识进行信号的频谱分析和滤波器设计，通过理论推导得到响应结论，利用MATLAB进行计算机仿真，加深对所学知识的理论界，融会贯通所学知识。</p>\n<p>通过本次课程设计，掌握用MATLAB对语音信号进行分析和处理的能力，并进一步掌握MATLAB 设计数字滤波器的方法。</p>\n<span id=\"more\"></span>\n<p>基础设计要求：</p>\n<p>(1)录制或采集一段语音信号，画出原始语音信号的时域波形及频谱。</p>\n<p>(2)应用Matlab平台给语音信号叠加高斯白噪声；画出加噪信号的时域波形及频谱。</p>\n<p>(3)确定设计性能指标，设计IIR数字滤波器进行滤波，画出滤波器的频谱。<br>(4)使用设计好的滤波器对加噪信号进行滤波，得到去噪信号。画出去噪信号的时域及频谱图。回放滤波前后语音，对比滤波效果。</p>\n<p>提高设计要求：</p>\n<p>(1)在原始语音上叠加高频噪音或低频噪音，完成滤波器设计过程。</p>\n<p>(2)设计两种及以上滤波器对语音信号进行滤波，给出不同滤波器滤波前后的信噪比（SNR），分析得出滤波效果最好的滤波器。</p>\n<p>(3)使用GUI&#x2F;app design设计语音去噪系统界面。</p>\n<p>(4)查阅课本或文献使用其他滤波算法完成滤波。(谱减法、卡尔曼滤波等)</p>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理</h1><h2 id=\"语言去噪步骤\"><a href=\"#语言去噪步骤\" class=\"headerlink\" title=\"语言去噪步骤\"></a>语言去噪步骤</h2><p>基于MATLAB的语音信号去噪设计主要分为以下五个步骤：</p>\n<p>（1）语音采集：可在MATLAB平台上录入一段语音信号，也可直接导入收集好的语音信号；</p>\n<p>（2）语音分析：绘制原始语音信号时域及频谱图，分析原始语音信号频谱特征；</p>\n<p>（3）语音加噪：对原始语音信号叠加噪声，并绘制加噪信号时域及频谱图。常见噪声包括高斯白噪声、高频噪声、低频噪声等；</p>\n<p>（4）滤波器设计：结合原始语音信号频谱，针对不同的叠加噪声，确定滤波器设计性能指标，设计相应滤波器，并绘制滤波器的频谱图，判断是否符合设计要求；</p>\n<p>（5）去噪信号分析：利用设计的滤波器对加噪信号进行滤波，绘制去噪信号时域及频谱图，并播放去噪语音信号，与原始语音信号进行对比分析。</p>\n<h2 id=\"谱减法的理论基础\"><a href=\"#谱减法的理论基础\" class=\"headerlink\" title=\"谱减法的理论基础\"></a>谱减法的理论基础</h2><p>假设带噪语音中的噪声是加性的，带噪语音谱减去估计出的噪声谱便可以得到干净语音谱。<br>根据加性假设，干净语音、噪声和带噪语音的关系可以写成如下所示：<br>$$<br>y(n)&#x3D;x(n)+d(n)<br>$$</p>\n<p>其中, $y(n)$ 为采集到的带噪语音, $x(n)$ 为干净语音, $d(n)$ 为噪声。传换至频域:<br>$$<br>\\begin{aligned}<br>&amp; Y(\\omega)&#x3D;X(\\omega)+D(\\omega) \\<br>&amp; Y(\\omega)&#x3D;|Y(\\omega)| e^{j \\Phi_y(\\omega)}<br>\\end{aligned}<br>$$</p>\n<p>其中 $|Y(\\omega)|$ 为幅度谱 $e^{j \\Phi_y(\\omega)}$ 为信号相位<br>则干净语音可以通过带噪语音减去噪声谱得到:<br>$$<br>\\hat{X}(\\omega)&#x3D;[|Y(\\omega)|-\\hat{D}(\\omega)] e^{j \\Phi_y(\\omega)}<br>$$</p>\n<p>但在实践中, $\\hat{X}(\\omega)$ 很有可能是负数, 所以常会做一个半波整流:</p>\n<p>由于对负值进行半波整流, 导致帧频谱的随机频率上出现小的、独立的峰值, 变换到时域上面, 这些峰值听起来就像帧与帧之间频率随机变化的多颤音, 也就是通常所说的 “音乐噪声” (Musical Noise)</p>\n<h1 id=\"实验方法与内容\"><a href=\"#实验方法与内容\" class=\"headerlink\" title=\"实验方法与内容\"></a>实验方法与内容</h1><h2 id=\"基础设计\"><a href=\"#基础设计\" class=\"headerlink\" title=\"基础设计\"></a>基础设计</h2><p>我们选取了一段CCTV的音频，音频的读取通过audioread函数实现<br>由于我们选取的音频为双通道的，为了后续处理方便将其改为单通道</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">y</span>=y(:,<span class=\"number\">1</span>)<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>通过plot函数即可得到它的时域图，通过fft即可得到它的频谱图，通过randn函数来生成白噪声</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">x</span>=randn(m,<span class=\"number\">1</span>)<span class=\"comment\">; %m为CCTV音频的长度</span></span><br></pre></td></tr></table></figure>\n\n<p>将其与原信号相加即可得到带噪信号，再对带噪信号用plot函数即可画出时域图，通过fft得到它的频谱图</p>\n<p>通过观察频谱图的信号和噪声分布，选择合适的性能指标，通过buttord函数得到相应的N和Wn</p>\n<figure class=\"highlight csharp\"><table><tr><td class=\"code\"><pre><span class=\"line\">[<span class=\"meta\">N,Wn</span>]=buttord(Wp,Ws,ap,<span class=\"keyword\">as</span>);</span><br></pre></td></tr></table></figure>\n<p>通过butter函数得到传递函数的系数B和A</p>\n<figure class=\"highlight angelscript\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">[B,A]</span>=butter(N,Wn);</span><br></pre></td></tr></table></figure>\n<p>用filter函数得到经降噪后的音频</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">y2</span>=filter(B,A,y1)<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>然后通过sound函数播放降噪前后的音频，比对得到滤波效果的好坏并调整性能指标</p>\n<h2 id=\"提高设计\"><a href=\"#提高设计\" class=\"headerlink\" title=\"提高设计\"></a>提高设计</h2><h3 id=\"非GUI设计\"><a href=\"#非GUI设计\" class=\"headerlink\" title=\"非GUI设计\"></a>非GUI设计</h3><p>高频或低频噪声的获取可通过对高斯白噪声通过一个高通或低通滤波器得到，由于我们选择的音频信息都集中在低频段，若添加低频噪声则用传统的滤波器很难消除，所以我们选择在原信号上添加高频噪声</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">[N,<span class=\"built_in\">wc</span>]=buttord(0.8,0.5,3,35);</span><br><span class=\"line\">[B,A]=butter(N,<span class=\"built_in\">wc</span>,<span class=\"string\">&#x27;high&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p>高斯白噪声通过这样一个高通滤波器即可变为高频信号，由于信号在低频，噪声在高频，故低通滤波器能够很好的满足我们的需求，而通过基础设计我们得到了一个巴特沃斯低通滤波器，则可以直接沿用该滤波器进行滤波</p>\n<p>我们还可以分别设计切比雪夫Ⅰ型滤波器，切比雪夫Ⅱ型滤波器，和椭圆滤波器来完成滤波</p>\n<p>切比雪夫Ⅰ型滤波器可通过cheb1ord函数和cheby1函数得到</p>\n<figure class=\"highlight csharp\"><table><tr><td class=\"code\"><pre><span class=\"line\">[<span class=\"meta\">N,Wn</span>]=cheb1ord(Wp,Ws,ap,<span class=\"keyword\">as</span>);</span><br><span class=\"line\">[<span class=\"meta\">B,A</span>]=cheby1(N,ap,Wn);</span><br><span class=\"line\">切比雪夫Ⅱ型滤波器可通过cheb2ord函数和cheby2函数得到</span><br><span class=\"line\">[<span class=\"meta\">N,Wn</span>]=cheb2ord(wp,ws,ap,<span class=\"keyword\">as</span>);</span><br><span class=\"line\">[<span class=\"meta\">B,A</span>]=cheby2(N,Rs,Wn);</span><br></pre></td></tr></table></figure>\n<p>椭圆滤波器可通过ellipord函数和ellip函数得到</p>\n<figure class=\"highlight angelscript\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">[N,Wn]</span>=ellipord(wp,ws,Rp,Rs);</span><br><span class=\"line\"><span class=\"string\">[B,A]</span>=ellip(N,Rp,Rs,Wn);</span><br></pre></td></tr></table></figure>\n<p>为了比较滤波器的滤波效果，我们需要计算滤波前后的信噪比，而我们可以选取的带噪信号相同，有相同的信噪比，只需计算滤波后的信噪比大小就能分析滤波器的性能好坏</p>\n<p>使用滤波器进行滤波后的信噪比计算方法为：</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"code\"><pre><span class=\"line\">SNR=<span class=\"number\">20</span>*<span class=\"built_in\">log10</span>(<span class=\"keyword\">norm</span>(y_flitered-x_flitered)/<span class=\"keyword\">norm</span>(x_flitered)) %y_flitered为滤波后的带噪信号,x_flitered为滤波后的噪声,<span class=\"keyword\">norm</span>为<span class=\"number\">2</span>范数</span><br></pre></td></tr></table></figure>\n<p>即滤波后的带噪信号减去滤波后的噪声即为经滤波后的纯净信号</p>\n<h3 id=\"GUI设计\"><a href=\"#GUI设计\" class=\"headerlink\" title=\"GUI设计\"></a>GUI设计</h3><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-6.png?raw=true\" alt=\"alt text\"></p>\n<p>我们在界面中放入三个坐标轴，分别用于观察原信号，加噪信号去噪信号的时域和频域图。</p>\n<p>放入两个列表框，用于选择添加信号的类型和滤波器的类型或者滤波方法。</p>\n<p>第一个列表框的选项如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-7.png?raw=true\" alt=\"alt text\"></p>\n<p>第二个列表框选项如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-8.png?raw=true\" alt=\"alt text\"></p>\n<p>列表框的选择选项功能用switch语句实现</p>\n<p>一个可编辑文本框用于设置加噪信号的信噪比，输入即为SNR</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">SNR</span>=<span class=\"number\">0</span><span class=\"comment\">;       %信噪比大小</span></span><br><span class=\"line\"><span class=\"attr\">x</span>=x/norm(x,<span class=\"number\">2</span>).*<span class=\"number\">10</span>^(-SNR/<span class=\"number\">20</span>)*norm(y)<span class=\"comment\">; %x为噪声,y为原信号</span></span><br></pre></td></tr></table></figure>\n<p>四个可编辑文本框用于输入滤波器的性能指标</p>\n<p>选用传统滤波器滤波后会计算出相应的信噪比</p>\n<p>若选用谱减法进行滤波，由于该方法滤波后的带噪信号减去滤波后的噪声不为纯净信号，则无法通过该方法计算信噪比</p>\n<p>经计算得到的信噪比输出在可编辑文本框中</p>\n<p>有三个播放按钮用于播放原信号，加噪信号，降噪信号，通过比对分析滤波效果的好坏</p>\n<h1 id=\"实验原始纪录\"><a href=\"#实验原始纪录\" class=\"headerlink\" title=\"实验原始纪录\"></a>实验原始纪录</h1><figure class=\"highlight routeros\"><table><tr><td class=\"code\"><pre><span class=\"line\">[y,Fs]=audioread(<span class=\"string\">&quot;C:\\Users\\kiana\\Desktop\\cctv.wav&quot;</span>);</span><br><span class=\"line\">% <span class=\"attribute\">y</span>=y(:,1);</span><br><span class=\"line\"><span class=\"attribute\">m</span>=length(y);</span><br><span class=\"line\"><span class=\"attribute\">x</span>=randn(m,1);</span><br><span class=\"line\"><span class=\"attribute\">SNR</span>=0;                 %信噪比大小</span><br><span class=\"line\"><span class=\"attribute\">x</span>=x/norm(x,2).*10^(-SNR/20)*norm(y);</span><br><span class=\"line\"><span class=\"attribute\">y1</span>=x+y;</span><br><span class=\"line\">% sound(y,Fs);</span><br><span class=\"line\">subplot(4,2,1);plot(y);</span><br><span class=\"line\">xlabel(<span class=\"string\">&#x27;时间&#x27;</span>);</span><br><span class=\"line\">ylabel(<span class=\"string\">&#x27;幅值&#x27;</span>);</span><br><span class=\"line\">title(<span class=\"string\">&#x27;原始语音信号&#x27;</span>);</span><br><span class=\"line\">subplot(4,2,3);plot(y1)</span><br><span class=\"line\">xlabel(<span class=\"string\">&#x27;时间&#x27;</span>);</span><br><span class=\"line\">ylabel(<span class=\"string\">&#x27;幅值&#x27;</span>);</span><br><span class=\"line\">title(<span class=\"string\">&#x27;加噪语音信号&#x27;</span>);</span><br><span class=\"line\">% sound(y1,Fs);</span><br><span class=\"line\">% audiowrite(<span class=\"string\">&#x27;C:\\Users\\kiana\\Desktop\\cctv_noise.wav&#x27;</span>,y1,Fs); </span><br><span class=\"line\">Y = fft(y);  % 傅里叶变换</span><br><span class=\"line\"><span class=\"attribute\">Y1</span>=fft(y1);</span><br><span class=\"line\">P = abs(Y);  % 双边频谱</span><br><span class=\"line\"><span class=\"attribute\">P1</span>=abs(Y1);</span><br><span class=\"line\"><span class=\"attribute\">n</span>=0:m-1;</span><br><span class=\"line\"><span class=\"attribute\">w</span>=2*n/m;</span><br><span class=\"line\">subplot(4,2,2);plot(w,P);title(<span class=\"string\">&#x27;原始语音频谱&#x27;</span>);</span><br><span class=\"line\">subplot(4,2,4);plot(w,P1);</span><br><span class=\"line\">xlabel(<span class=\"string\">&#x27;频率\\omega/\\pi&#x27;</span>);</span><br><span class=\"line\">ylabel(<span class=\"string\">&#x27;幅度&#x27;</span>);</span><br><span class=\"line\">title(<span class=\"string\">&#x27;加噪语音频谱&#x27;</span>);</span><br><span class=\"line\"><span class=\"attribute\">fp</span>=0.15;fs=0.24;ap=3;as=35;</span><br><span class=\"line\">[N,Wn]=buttord(fp,fs,ap,as);</span><br><span class=\"line\">[B,A]=butter(N,Wn);</span><br><span class=\"line\">% [N,Wn]=cheb1ord(fp,fs,ap,as); %使用其他滤波器即注释当前的滤波器和去相应滤波器的注释</span><br><span class=\"line\">% [B,A]=cheby1(N,ap,Wn);</span><br><span class=\"line\">% [N,Wn]=cheb2ord(fp,fs,ap,as);</span><br><span class=\"line\">% [B,A]=cheby2(N,ap,Wn);</span><br><span class=\"line\">[H,W]=freqz(B,A);</span><br><span class=\"line\">% plot(abs(H));</span><br><span class=\"line\">% [N,Wn]=ellipord(fp,fs,ap,as);</span><br><span class=\"line\">% [B,A]=ellip(N,ap,as,Wn);</span><br><span class=\"line\"><span class=\"attribute\">y2</span>=filter(B,A,y1);</span><br><span class=\"line\"><span class=\"attribute\">x2</span>=filter(B,A,x);</span><br><span class=\"line\"><span class=\"attribute\">Ps</span>=sum((y2-x2).^2);</span><br><span class=\"line\"><span class=\"attribute\">Pu</span>=sum(x2.^2);</span><br><span class=\"line\"><span class=\"attribute\">SNR1</span>=10*log10(Ps/Pu); </span><br><span class=\"line\">subplot( 4,2,5);plot(y2);xlabel(<span class=\"string\">&#x27;时间&#x27;</span>);</span><br><span class=\"line\">ylabel(<span class=\"string\">&#x27;幅值&#x27;</span>);title(<span class=\"string\">&quot;去噪语音信号&quot;</span>);</span><br><span class=\"line\"><span class=\"attribute\">Y2</span>=fft(y2);</span><br><span class=\"line\"><span class=\"attribute\">P2</span>=abs(Y2);</span><br><span class=\"line\">subplot(4,2,6);plot(w,P2);xlabel(<span class=\"string\">&#x27;频率\\omega/\\pi&#x27;</span>);</span><br><span class=\"line\">ylabel(<span class=\"string\">&#x27;幅度&#x27;</span>);title(<span class=\"string\">&#x27;去噪语音频谱&#x27;</span>);</span><br><span class=\"line\">subplot(4,2,7);plot(W/pi,20*log10(abs(H)));</span><br><span class=\"line\">% axis([0,1,-440,0]);title(<span class=\"string\">&quot;巴特沃斯低通滤波器&quot;</span>);</span><br><span class=\"line\">xlabel(<span class=\"string\">&quot;\\omega/pi&quot;</span>);ylabel(<span class=\"string\">&quot;增益(dB)&quot;</span>);</span><br><span class=\"line\">% sound(y2,Fs);</span><br><span class=\"line\">% audiowrite(<span class=\"string\">&#x27;C:\\Users\\kiana\\Desktop\\cctv_denosied.wav&#x27;</span>,y2,Fs); </span><br></pre></td></tr></table></figure>\n<h1 id=\"实验结果及分析\"><a href=\"#实验结果及分析\" class=\"headerlink\" title=\"实验结果及分析\"></a>实验结果及分析</h1><h2 id=\"基础部分\"><a href=\"#基础部分\" class=\"headerlink\" title=\"基础部分\"></a>基础部分</h2><p>我们选取的音频的时域和频域图如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-9.png?raw=true\" alt=\"alt text\"></p>\n<p>可以看到它的信号主要集中在低频部分</p>\n<p>添加高斯白噪声得到信噪比为0的带噪信号，它的时域和频域图如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-10.png?raw=true\" alt=\"alt text\"></p>\n<p>发现原来各频率上均已有值，噪声的幅度均匀分布在各频率段，通过观察我们发现，在$0.15$之前的频率就包含了原信号频谱的绝大部分，而在$0.24$之后几乎没有值，所以我们可以设计相应的滤波器以保留低频段信息，滤去其他信息</p>\n<p>得到的滤波器响应曲线</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-11.png?raw=true\" alt=\"alt text\"></p>\n<p>经过滤后的信号的时域和频域图如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-12.png?raw=true\" alt=\"alt text\"></p>\n<p>通过先前的信噪比计算方法得到滤波后的信号信噪比约为$7.87$这是一个合理的值，由于加噪信号信噪比为 $0$，即噪声功率$Pu$和纯净信号功率$Ps$相等，我们通过计算可得它的值应该大于$10\\times log10(Ps&#x2F;0.24Pu)&#x3D;6.19$,小于$10\\times log10（Ps&#x2F;0.15Pu）&#x3D;8.24$</p>\n<h2 id=\"提高部分\"><a href=\"#提高部分\" class=\"headerlink\" title=\"提高部分\"></a>提高部分</h2><p>我们向原信号中加入高频噪声得到信噪比为0的加噪信号，它的时域和频域图如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-13.png?raw=true\" alt=\"alt text\"></p>\n<p>可以看到噪声都集中在0.6到1之间</p>\n<p>我们将其通过沿用之前的性能指标得到的巴特沃斯滤波器</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-14.png?raw=true\" alt=\"alt text\"></p>\n<p>它的信噪比为$76.5$</p>\n<p>我们再设计切比雪夫Ⅰ型滤波器，它的响应曲线和去噪效果如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-15.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-16.png?raw=true\" alt=\"alt text\"></p>\n<p>信噪比为$75.3$<br>切比雪夫Ⅱ型滤波器的响应曲线和滤波效果如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-17.png?raw=true\" alt=\"alt text\"></p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-18.png?raw=true\" alt=\"alt text\"></p>\n<p>它的信噪比为$42.5$<br>椭圆滤波器的响应曲线和滤波效果如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-19.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-20.png?raw=true\" alt=\"alt text\"></p>\n<p>它的信噪比为$33.1$<br>通过比对我们可以发现巴特沃斯滤波器的效果最好，切比雪夫Ⅰ型与其相似，而切比雪夫Ⅱ型和椭圆滤波器的效果与前两个相差较大</p>\n<p>这是由他们的响应曲线决定的，切比雪夫Ⅰ型在低频处有波纹使得它在保留低频信息上劣于巴特沃斯，而切比雪夫Ⅱ型和椭圆滤波器在高频出的衰减不如前两个滤波器剧烈，也就是噪声的消除不如前两个滤波器，而椭圆滤波器的信噪比不如切比雪夫Ⅱ型则是由于它对低频信号的衰减明显强于切比雪夫Ⅱ型</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/dsp-21.png?raw=true\" alt=\"alt text\"></p>\n<p>播放谱减法处理后的音频我们可以发现在最后有明显的多颤音，这一点在时域图上也能看出，在$6$至$8$处的幅值对比原音频时域图有明显的衰减，这是谱减法的一个弊端，它会产生音乐噪声，而这一现象在另一音频中体现的尤为明显</p>\n","categories":["Digital Signal Processing"]},{"title":"机器翻译","url":"/2024/06/23/lab13/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"背景知识\"><a href=\"#背景知识\" class=\"headerlink\" title=\"背景知识\"></a>背景知识</h1><h2 id=\"编码器—解码器（seq2seq）\"><a href=\"#编码器—解码器（seq2seq）\" class=\"headerlink\" title=\"编码器—解码器（seq2seq）\"></a>编码器—解码器（seq2seq）</h2><p>在自然语言处理的很多应用中，输入和输出都可以是不定长序列。以机器翻译为例，输入可以是一段不定长的英语文本序列，输出可以是一段不定长的法语文本序列，例如</p>\n<blockquote>\n<p>英语输入：“They”、“are”、“watching”、“.”</p>\n</blockquote>\n<blockquote>\n<p>法语输出：“Ils”、“regardent”、“.”</p>\n</blockquote>\n<p>当输入和输出都是不定长序列时，我们可以使用编码器—解码器（encoder-decoder） 或者seq2seq模型。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器用来分析输入序列，解码器用来生成输出序列。<span id=\"more\"></span></p>\n<p>图10.8描述了使用编码器—解码器将上述英语句子翻译成法语句子的一种方法。在训练数据集中，我们可以在每个句子后附上特殊符号“&lt;eos&gt;”（end of sequence）以表示序列的终止。编码器每个时间步的输入依次为英语句子中的单词、标点和特殊符号“&lt;eos&gt;”。图10.8中使用了编码器在最终时间步的隐藏状态作为输入句子的表征或编码信息。解码器在各个时间步中使用输入句子的编码信息和上个时间步的输出以及隐藏状态作为输入。我们希望解码器在各个时间步能正确依次输出翻译后的法语单词、标点和特殊符号”&lt;eos&gt;”。需要注意的是，解码器在最初时间步的输入用到了一个表示序列开始的特殊符号”&lt;bos&gt;”（beginning of sequence）。</p>\n<div align=\"center\">\n<img width=\"500\" src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/10.9_seq2seq.svg?raw=true\">\n</div>\n<div align=\"center\">图10.8 使用编码器—解码器将句子由英语翻译成法语。编码器和解码器分别为循环神经网络</div>\n\n\n<p>接下来，我们分别介绍编码器和解码器的定义。</p>\n<h3 id=\"编码器\"><a href=\"#编码器\" class=\"headerlink\" title=\"编码器\"></a>编码器</h3><p>编码器的作用是把一个不定长的输入序列变换成一个定长的背景变量$\\boldsymbol{c}$，并在该背景变量中编码输入序列信息。常用的编码器是循环神经网络。</p>\n<p>让我们考虑批量大小为1的时序数据样本。假设输入序列是$x_1,\\ldots,x_T$，例如$x_i$是输入句子中的第$i$个词。在时间步$t$，循环神经网络将输入$x_t$的特征向量$\\boldsymbol{x_t}$和上个时间步的隐藏状态$\\boldsymbol{h}_{t-1}$变换为当前时间步的隐藏状态$\\boldsymbol{h}_t$。我们可以用函数$f$表达循环神经网络隐藏层的变换：</p>\n<p>$$\\boldsymbol{h_t} &#x3D; f(\\boldsymbol{x_t}, \\boldsymbol{h}_{t-1})$$</p>\n<p>接下来，编码器通过自定义函数$q$将各个时间步的隐藏状态变换为背景变量</p>\n<p>$$<br>\\boldsymbol{c} &#x3D;  q(\\boldsymbol{h}_1, \\ldots, \\boldsymbol{h}_T).<br>$$</p>\n<p>例如，当选择$q(\\boldsymbol{h}_1, \\ldots, \\boldsymbol{h}_T) &#x3D; \\boldsymbol{h}_T$时，背景变量是输入序列最终时间步的隐藏状态$\\boldsymbol{h}_T$。</p>\n<p>以上描述的编码器是一个单向的循环神经网络，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用双向循环神经网络构造编码器。在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。</p>\n<h3 id=\"解码器\"><a href=\"#解码器\" class=\"headerlink\" title=\"解码器\"></a>解码器</h3><p>刚刚已经介绍，编码器输出的背景变量$\\boldsymbol{c}$编码了整个输入序列$x_1, \\ldots, x_T$的信息。给定训练样本中的输出序列$y_1, y_2, \\ldots, y_{T’}$，对每个时间步$t’$（符号与输入序列或编码器的时间步$t$有区别），解码器输出$y_{t’}$的条件概率将基于之前的输出序列$y_1,\\ldots,y_{t’-1}$和背景变量$\\boldsymbol{c}$，即$P(y_{t’} \\mid y_1, \\ldots, y_{t’-1}, \\boldsymbol{c})$。</p>\n<p>为此，我们可以使用另一个循环神经网络作为解码器。在输出序列的时间步$t^\\prime$，解码器将上一时间步的输出$y_{t^\\prime-1}$以及背景变量$\\boldsymbol{c}$作为输入，并将它们与上一时间步的隐藏状态$\\boldsymbol{s_{t^\\prime-1}}$变换为当前时间步的隐藏状态$\\boldsymbol{s}_{t^\\prime}$。因此，我们可以用函数$g$表达解码器隐藏层的变换：</p>\n<p>$$<br>\\boldsymbol{s_{t^\\prime}} &#x3D; g(y_{t^\\prime-1}, \\boldsymbol{c}, \\boldsymbol{s}_{t^\\prime-1}).<br>$$</p>\n<p>有了解码器的隐藏状态后，我们可以使用自定义的输出层和softmax运算来计算$P(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\boldsymbol{c})$，例如，基于当前时间步的解码器隐藏状态 $\\boldsymbol{s}_ {t^\\prime}$、上一时间步的输出$y_{t^\\prime-1}$以及背景变量$\\boldsymbol{c}$来计算当前时间步输出$y_{t^\\prime}$的概率分布。</p>\n<h3 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h3><p>根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率</p>\n<p>$$<br>\\begin{aligned}<br>P(y_1, \\ldots, y_{T’} \\mid x_1, \\ldots, x_T)<br>&amp;&#x3D; \\prod_{t’&#x3D;1}^{T’} P(y_{t’} \\mid y_1, \\ldots, y_{t’-1}, x_1, \\ldots, x_T)\\\\<br>&amp;&#x3D; \\prod_{t’&#x3D;1}^{T’} P(y_{t’} \\mid y_1, \\ldots, y_{t’-1}, \\boldsymbol{c}),<br>\\end{aligned}<br>$$</p>\n<p>并得到该输出序列的损失</p>\n<p>$$<br>-\\log P(y_1, \\ldots, y_{T’} \\mid x_1, \\ldots, x_T) &#x3D; -\\sum_{t’&#x3D;1}^{T’} \\log P(y_{t’} \\mid y_1, \\ldots, y_{t’-1}, \\boldsymbol{c}),<br>$$</p>\n<p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在图10.8所描述的模型预测中，我们需要将解码器在上一个时间步的输出作为当前时间步的输入。与此不同，在训练中我们也可以将标签序列（训练集的真实输出序列）在上一个时间步的标签作为解码器在当前时间步的输入。这叫作强制教学（teacher forcing）。</p>\n<h2 id=\"束搜索\"><a href=\"#束搜索\" class=\"headerlink\" title=\"束搜索\"></a>束搜索</h2><p>上一节介绍了如何训练输入和输出均为不定长序列的编码器—解码器。本节我们介绍如何使用编码器—解码器来预测不定长的序列。</p>\n<p>上一节里已经提到，在准备训练数据集时，我们通常会在样本的输入序列和输出序列后面分别附上一个特殊符号”&lt;eos&gt;”表示序列的终止。我们在接下来的讨论中也将沿用上一节的全部数学符号。为了便于讨论，假设解码器的输出是一段文本序列。设输出文本词典$\\mathcal{Y}$（包含特殊符号”&lt;eos&gt;”）的大小为$\\left|\\mathcal{Y}\\right|$，输出序列的最大长度为$T’$。所有可能的输出序列一共有$\\mathcal{O}(\\left|\\mathcal{Y}\\right|^{T’})$种。这些输出序列中所有特殊符号”&lt;eos&gt;”后面的子序列将被舍弃。</p>\n<h3 id=\"贪婪搜索\"><a href=\"#贪婪搜索\" class=\"headerlink\" title=\"贪婪搜索\"></a>贪婪搜索</h3><p>让我们先来看一个简单的解决方案：贪婪搜索（greedy search）。对于输出序列任一时间步$t’$，我们从$|\\mathcal{Y}|$个词中搜索出条件概率最大的词</p>\n<p>$$<br>y _ { t ^ { \\prime } } &#x3D; \\underset { y \\in \\mathcal { Y } } { \\operatorname { argmax } } P \\left( y | y _ { 1 } , \\ldots , y _ { t ^ { \\prime } - 1 } , c \\right)<br>$$</p>\n<p>作为输出。一旦搜索出”&lt;eos&gt;”符号，或者输出序列长度已经达到了最大长度$T’$，便完成输出。</p>\n<p>我们在描述解码器时提到，基于输入序列生成输出序列的条件概率是$\\prod_{t’&#x3D;1}^{T’} P(y_{t’} \\mid y_1, \\ldots, y_{t’-1}, \\boldsymbol{c})$。我们将该条件概率最大的输出序列称为最优输出序列。而贪婪搜索的主要问题是不能保证得到最优输出序列。</p>\n<p>下面来看一个例子。假设输出词典里面有“A”“B”“C”和“&lt;eos&gt;”这4个词。图10.9中每个时间步下的4个数字分别代表了该时间步生成“A”“B”“C”和“&lt;eos&gt;”这4个词的条件概率。在每个时间步，贪婪搜索选取条件概率最大的词。因此，图10.9中将生成输出序列“A”“B”“C”“&lt;eos&gt;”。该输出序列的条件概率是$0.5\\times0.4\\times0.4\\times0.6 &#x3D; 0.048$。</p>\n<div align=\"center\">\n<img width=\"200\" src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/10.10_s2s_prob1.svg?raw=true\">\n</div>\n<div align=\"center\">图10.9 在每个时间步，贪婪搜索选取条件概率最大的词</div>\n\n\n<p>接下来，观察图10.10演示的例子。与图10.9中不同，图10.10在时间步2中选取了条件概率第二大的词“C”。由于时间步3所基于的时间步1和2的输出子序列由图10.9中的“A”“B”变为了图10.10中的“A”“C”，图10.10中时间步3生成各个词的条件概率发生了变化。我们选取条件概率最大的词“B”。此时时间步4所基于的前3个时间步的输出子序列为“A”“C”“B”，与图10.9中的“A”“B”“C”不同。因此，图10.10中时间步4生成各个词的条件概率也与图10.9中的不同。我们发现，此时的输出序列“A”“C”“B”“&lt;eos&gt;”的条件概率是$0.5\\times0.3\\times0.6\\times0.6&#x3D;0.054$，大于贪婪搜索得到的输出序列的条件概率。因此，贪婪搜索得到的输出序列“A”“B”“C”“&lt;eos&gt;”并非最优输出序列。</p>\n<div align=\"center\">\n<img width=\"200\" src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/10.10_s2s_prob2.svg?raw=true\">\n</div>\n<div align=\"center\">图10.10 在时间步2选取条件概率第二大的词“C”</div>\n\n\n<h3 id=\"穷举搜索\"><a href=\"#穷举搜索\" class=\"headerlink\" title=\"穷举搜索\"></a>穷举搜索</h3><p>如果目标是得到最优输出序列，我们可以考虑穷举搜索（exhaustive search）：穷举所有可能的输出序列，输出条件概率最大的序列。</p>\n<p>虽然穷举搜索可以得到最优输出序列，但它的计算开销$\\mathcal{O}(\\left|\\mathcal{Y}\\right|^{T’})$很容易过大。例如，当$|\\mathcal{Y}|&#x3D;10000$且$T’&#x3D;10$时，我们将评估$10000^{10} &#x3D; 10^{40}$个序列：这几乎不可能完成。而贪婪搜索的计算开销是$\\mathcal{O}(\\left|\\mathcal{Y}\\right|T’)$，通常显著小于穷举搜索的计算开销。例如，当$|\\mathcal{Y}|&#x3D;10000$且$T’&#x3D;10$时，我们只需评估$10000\\times10&#x3D;10^5$个序列。</p>\n<h3 id=\"束搜索-1\"><a href=\"#束搜索-1\" class=\"headerlink\" title=\"束搜索\"></a>束搜索</h3><p>束搜索（beam search）是对贪婪搜索的一个改进算法。它有一个束宽（beam size）超参数。我们将它设为$k$。在时间步1时，选取当前时间步条件概率最大的$k$个词，分别组成$k$个候选输出序列的首词。在之后的每个时间步，基于上个时间步的$k$个候选输出序列，从$k\\left|\\mathcal{Y}\\right|$个可能的输出序列中选取条件概率最大的$k$个，作为该时间步的候选输出序列。最终，我们从各个时间步的候选输出序列中筛选出包含特殊符号“&lt;eos&gt;”的序列，并将它们中所有特殊符号“&lt;eos&gt;”后面的子序列舍弃，得到最终候选输出序列的集合。</p>\n<div align=\"center\">\n<img width=\"500\" src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/10.10_beam_search.svg?raw=true\">\n</div>\n<div align=\"center\">图10.11 束搜索的过程。束宽为2，输出序列最大长度为3。候选输出序列有A、C、AB、CE、ABD和CED</div>\n\n<p>图10.11通过一个例子演示了束搜索的过程。假设输出序列的词典中只包含5个元素，即$\\mathcal{Y} &#x3D; {A, B, C, D, E}$，且其中一个为特殊符号“&lt;eos&gt;”。设束搜索的束宽等于2，输出序列最大长度为3。在输出序列的时间步1时，假设条件概率$P(y_1 \\mid \\boldsymbol{c})$最大的2个词为$A$和$C$。我们在时间步2时将对所有的$y_2 \\in \\mathcal{Y}$都分别计算$P(y_2 \\mid A, \\boldsymbol{c})$和$P(y_2 \\mid C, \\boldsymbol{c})$，并从计算出的10个条件概率中取最大的2个，假设为$P(B \\mid A, \\boldsymbol{c})$和$P(E \\mid C, \\boldsymbol{c})$。那么，我们在时间步3时将对所有的$y_3 \\in \\mathcal{Y}$都分别计算$P(y_3 \\mid A, B, \\boldsymbol{c})$和$P(y_3 \\mid C, E, \\boldsymbol{c})$，并从计算出的10个条件概率中取最大的2个，假设为$P(D \\mid A, B, \\boldsymbol{c})$和$P(D \\mid C, E, \\boldsymbol{c})$。如此一来，我们得到6个候选输出序列：（1）$A$；（2）$C$；（3）$A$、$B$；（4）$C$、$E$；（5）$A$、$B$、$D$和（6）$C$、$E$、$D$。接下来，我们将根据这6个序列得出最终候选输出序列的集合。</p>\n<p>在最终候选输出序列的集合中，我们取以下分数最高的序列作为输出序列：</p>\n<p>$$ \\frac{1}{L^\\alpha} \\log P(y_1, \\ldots, y_{L}) &#x3D; \\frac{1}{L^\\alpha} \\sum_{t’&#x3D;1}^L \\log P(y_{t’} \\mid y_1, \\ldots, y_{t’-1}, \\boldsymbol{c}),$$</p>\n<p>其中$L$为最终候选序列长度，$\\alpha$一般可选为0.75。分母上的$L^\\alpha$是为了惩罚较长序列在以上分数中较多的对数相加项。分析可知，束搜索的计算开销为$\\mathcal{O}(k\\left|\\mathcal{Y}\\right|T’)$。这介于贪婪搜索和穷举搜索的计算开销之间。此外，贪婪搜索可看作是束宽为1的束搜索。束搜索通过灵活的束宽$k$来权衡计算开销和搜索质量。</p>\n<h2 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h2><p>在编码器—解码器（seq2seq）里，解码器在各个时间步依赖相同的背景变量来获取输入序列信息。当编码器为循环神经网络时，背景变量来自它最终时间步的隐藏状态。</p>\n<p>现在，让我们再次思考那一节提到的翻译例子：输入为英语序列“They”“are”“watching”“.”，输出为法语序列“Ils”“regardent”“.”。不难想到，解码器在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步1，解码器可以主要依赖“They”“are”的信息来生成“Ils”，在时间步2则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步3则直接映射句号“.”。这看上去就像是在解码器的每一时间步对输入序列中不同时间步的表征或编码信息分配不同的注意力一样。这也是注意力机制的由来。</p>\n<p>仍然以循环神经网络为例，注意力机制通过对编码器所有时间步的隐藏状态做加权平均来得到背景变量。解码器在每一时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量。本节我们将讨论注意力机制是怎么工作的。</p>\n<p>$$\\boldsymbol{s}_{t’} &#x3D; g(\\boldsymbol{y}_{t’-1}, \\boldsymbol{c}_{t’}, \\boldsymbol{s}_{t’-1}).$$</p>\n<p>这里的关键是如何计算背景变量$\\boldsymbol{c}_{t’}$和如何利用它来更新隐藏状态$\\boldsymbol{s}_{t’}$。下面将分别描述这两个关键点。</p>\n<h3 id=\"计算背景变量\"><a href=\"#计算背景变量\" class=\"headerlink\" title=\"计算背景变量\"></a>计算背景变量</h3><p>我们先描述第一个关键点，即计算背景变量。图10.12描绘了注意力机制如何为解码器在时间步2计算背景变量。首先，函数$a$根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输入。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量。</p>\n<div align=\"center\">\n<img width=\"500\" src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/10.11_attention.svg?raw=true\">\n</div>\n<div align=\"center\">图10.12 编码器—解码器上的注意力机制</div>\n\n\n<p>具体来说，令编码器在时间步$t$的隐藏状态为$\\boldsymbol{h}_t$，且总时间步数为$T$。那么解码器在时间步$t’$的背景变量为所有编码器隐藏状态的加权平均：</p>\n<p>$$<br>\\boldsymbol{c}_{t’} &#x3D; \\sum_{t&#x3D;1}^T \\alpha_{t’t} \\boldsymbol{h}_t,<br>$$</p>\n<p>其中给定$t’$时，权重$\\alpha_{t’ t}$在$t&#x3D;1,\\ldots,T$的值是一个概率分布。为了得到概率分布，我们可以使用softmax运算:</p>\n<p>$$<br>\\alpha_{t’ t} &#x3D; \\frac{\\exp(e_{t’ t})}{ \\sum_{k&#x3D;1}^T \\exp(e_{t’ k}) },\\quad t&#x3D;1,\\ldots,T.<br>$$</p>\n<p>现在，我们需要定义如何计算上式中softmax运算的输入$e_{t’ t}$。由于$e_{t’ t}$同时取决于解码器的时间步$t’$和编码器的时间步$t$，我们不妨以解码器在时间步$t’-1$的隐藏状态$\\boldsymbol{s}_{t’ - 1}$与编码器在时间步$t$的隐藏状态$\\boldsymbol{h}_t$为输入，并通过函数$a$计算$e_{t’ t}$：</p>\n<p>$$<br>e_{t’ t} &#x3D; a(\\boldsymbol{s}_{t’ - 1}, \\boldsymbol{h}_t).<br>$$</p>\n<p>这里函数$a$有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积$a(\\boldsymbol{s}, \\boldsymbol{h})&#x3D;\\boldsymbol{s}^\\top \\boldsymbol{h}$。而最早提出注意力机制的论文则将输入连结后通过含单隐藏层的多层感知机变换 ：</p>\n<p>$$<br>a(\\boldsymbol{s}, \\boldsymbol{h}) &#x3D; \\boldsymbol{v}^\\top \\tanh(\\boldsymbol{W}_s \\boldsymbol{s} + \\boldsymbol{W}_h \\boldsymbol{h}),<br>$$</p>\n<p>其中$\\boldsymbol{v}$、$\\boldsymbol{W}_s$、$\\boldsymbol{W}_h$都是可以学习的模型参数。</p>\n<h4 id=\"矢量化计算\"><a href=\"#矢量化计算\" class=\"headerlink\" title=\"矢量化计算\"></a>矢量化计算</h4><p>我们还可以对注意力机制采用更高效的矢量化计算。广义上，注意力机制的输入包括查询项以及一一对应的键项和值项，其中值项是需要加权平均的一组项。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。</p>\n<p>在上面的例子中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。<br>让我们考虑一个常见的简单情形，即编码器和解码器的隐藏单元个数均为$h$，且函数$a(\\boldsymbol{s}, \\boldsymbol{h})&#x3D;\\boldsymbol{s}^\\top \\boldsymbol{h}$。假设我们希望根据解码器单个隐藏状态$\\boldsymbol{s}_{t’ - 1} \\in \\mathbb{R}^{h}$和编码器所有隐藏状态$\\boldsymbol{h}_t \\in \\mathbb{R}^{h}, t &#x3D; 1,\\ldots,T$来计算背景向量$\\boldsymbol{c}_{t’}\\in \\mathbb{R}^{h}$。<br>我们可以将查询项矩阵$\\boldsymbol{Q} \\in \\mathbb{R}^{1 \\times h}$设为$\\boldsymbol{s}_{t’ - 1}^\\top$，并令键项矩阵$\\boldsymbol{K} \\in \\mathbb{R}^{T \\times h}$和值项矩阵$\\boldsymbol{V} \\in \\mathbb{R}^{T \\times h}$相同且第$t$行均为$\\boldsymbol{h}_t^\\top$。此时，我们只需要通过矢量化计算</p>\n<p>$$\\text{softmax}(\\boldsymbol{Q}\\boldsymbol{K}^\\top)\\boldsymbol{V}$$</p>\n<p>即可算出转置后的背景向量$\\boldsymbol{c}_{t’}^\\top$。当查询项矩阵$\\boldsymbol{Q}$的行数为$n$时，上式将得到$n$行的输出矩阵。输出矩阵与查询项矩阵在相同行上一一对应。</p>\n<h3 id=\"更新隐藏状态\"><a href=\"#更新隐藏状态\" class=\"headerlink\" title=\"更新隐藏状态\"></a>更新隐藏状态</h3><p>现在我们描述第二个关键点，即更新隐藏状态。以门控循环单元为例，在解码器中我们可以对6.7节（门控循环单元（GRU））中门控循环单元的设计稍作修改，从而变换上一时间步$t’-1$的输出$\\boldsymbol{y}_{t’-1}$、隐藏状态$\\boldsymbol{s}_{t’ - 1}$和当前时间步$t’$的含注意力机制的背景变量$\\boldsymbol{c}_{t’}$ 。解码器在时间步$t’$的隐藏状态为</p>\n<p>$$\\boldsymbol{s}_{t’} &#x3D; \\boldsymbol{z}_{t’} \\odot \\boldsymbol{s}_{t’-1}  + (1 - \\boldsymbol{z}_{t’}) \\odot \\tilde{\\boldsymbol{s}}_{t’},$$</p>\n<p>其中的重置门、更新门和候选隐藏状态分别为</p>\n<p>$$<br>\\begin{aligned}<br>\\boldsymbol{r}_{t’} &amp;&#x3D; \\sigma(\\boldsymbol{W}_{yr} \\boldsymbol{y}_{t’-1} + \\boldsymbol{W}_{sr} \\boldsymbol{s}_{t’ - 1} + \\boldsymbol{W}_{cr} \\boldsymbol{c}_{t’} + \\boldsymbol{b}_r),\\\\<br>\\boldsymbol{z}_{t’} &amp;&#x3D; \\sigma(\\boldsymbol{W}_{yz} \\boldsymbol{y}_{t’-1} + \\boldsymbol{W}_{sz} \\boldsymbol{s}_{t’ - 1} + \\boldsymbol{W}_{cz} \\boldsymbol{c}_{t’} + \\boldsymbol{b}_z),\\\\<br>\\tilde{\\boldsymbol{s}}_{t’} &amp;&#x3D; \\text{tanh}(\\boldsymbol{W}_{ys} \\boldsymbol{y}_{t’-1} + \\boldsymbol{W}_{ss} (\\boldsymbol{s}_{t’ - 1} \\odot \\boldsymbol{r}_{t’}) + \\boldsymbol{W}_{cs} \\boldsymbol{c}_{t’} + \\boldsymbol{b}_s),<br>\\end{aligned}<br>$$</p>\n<p>其中含下标的$\\boldsymbol{W}$和$\\boldsymbol{b}$分别为门控循环单元的权重参数和偏差参数。</p>\n<h3 id=\"发展\"><a href=\"#发展\" class=\"headerlink\" title=\"发展\"></a>发展</h3><p>本质上，注意力机制能够为表征中较有价值的部分分配较多的计算资源。这个有趣的想法自提出后得到了快速发展，特别是启发了依靠注意力机制来编码输入序列并解码出输出序列的变换器（Transformer）模型的设计 。变换器抛弃了卷积神经网络和循环神经网络的架构。它在计算效率上比基于循环神经网络的编码器—解码器模型通常更具明显优势。含注意力机制的变换器的编码结构在后来的BERT预训练模型中得以应用并令后者大放异彩：微调后的模型在多达11项自然语言处理任务中取得了当时最先进的结果 。不久后，同样是基于变换器设计的GPT-2模型于新收集的语料数据集预训练后，在7个未参与训练的语言模型数据集上均取得了当时最先进的结果 。除了自然语言处理领域，注意力机制还被广泛用于图像分类、自动图像描述、唇语解读以及语音识别。</p>\n<h1 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h1><h2 id=\"读取和预处理数据\"><a href=\"#读取和预处理数据\" class=\"headerlink\" title=\"读取和预处理数据\"></a>读取和预处理数据</h2><p>我们先定义一些特殊符号。其中“&lt;pad&gt;”（padding）符号用来添加在较短序列后，直到每个序列等长，而“&lt;bos&gt;”和“&lt;eos&gt;”符号分别表示序列的开始和结束。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> collections</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> io</span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchtext.vocab <span class=\"keyword\">as</span> Vocab</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.utils.data <span class=\"keyword\">as</span> Data</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"><span class=\"comment\"># sys.path.append(&quot;..&quot;) </span></span><br><span class=\"line\"><span class=\"keyword\">import</span> d2lzh_pytorch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">PAD, BOS, EOS = <span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;bos&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;eos&gt;&#x27;</span></span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class=\"string\">&quot;0&quot;</span></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.__version__, device)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab13-2.png?raw=true\" alt=\"alt text\"></p>\n<p>接着定义两个辅助函数对后面读取的数据进行预处理。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列</span></span><br><span class=\"line\"><span class=\"comment\"># 长度变为max_seq_len，然后将序列保存在all_seqs中</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">process_one_seq</span>(<span class=\"params\">seq_tokens, all_tokens, all_seqs, max_seq_len</span>):</span><br><span class=\"line\">    all_tokens.extend(seq_tokens)</span><br><span class=\"line\">    seq_tokens += [EOS] + [PAD] * (max_seq_len - <span class=\"built_in\">len</span>(seq_tokens) - <span class=\"number\">1</span>)</span><br><span class=\"line\">    all_seqs.append(seq_tokens)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用所有的词来构造词典。并将所有序列中的词变换为词索引后构造Tensor</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">build_data</span>(<span class=\"params\">all_tokens, all_seqs</span>):</span><br><span class=\"line\">    vocab = Vocab.Vocab(collections.Counter(all_tokens),</span><br><span class=\"line\">                        specials=[PAD, BOS, EOS])</span><br><span class=\"line\">    indices = [[vocab.stoi[w] <span class=\"keyword\">for</span> w <span class=\"keyword\">in</span> seq] <span class=\"keyword\">for</span> seq <span class=\"keyword\">in</span> all_seqs]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> vocab, torch.tensor(indices)</span><br></pre></td></tr></table></figure>\n<p>为了演示方便，我们在这里使用一个很小的法语—英语数据集。在这个数据集里，每一行是一对法语句子和它对应的英语句子，中间使用<code>&#39;\\t&#39;</code>隔开。在读取数据时，我们在句末附上“&lt;eos&gt;”符号，并可能通过添加“&lt;pad&gt;”符号使每个序列的长度均为<code>max_seq_len</code>。我们为法语词和英语词分别创建词典。法语词的索引和英语词的索引相互独立。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">read_data</span>(<span class=\"params\">max_seq_len</span>):</span><br><span class=\"line\">    <span class=\"comment\"># in和out分别是input和output的缩写</span></span><br><span class=\"line\">    in_tokens, out_tokens, in_seqs, out_seqs = [], [], [], []</span><br><span class=\"line\">    <span class=\"keyword\">with</span> io.<span class=\"built_in\">open</span>(<span class=\"string\">&#x27;fr-en-small.txt&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        lines = f.readlines()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</span><br><span class=\"line\">        in_seq, out_seq = line.rstrip().split(<span class=\"string\">&#x27;\\t&#x27;</span>)</span><br><span class=\"line\">        in_seq_tokens, out_seq_tokens = in_seq.split(<span class=\"string\">&#x27; &#x27;</span>), out_seq.split(<span class=\"string\">&#x27; &#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">max</span>(<span class=\"built_in\">len</span>(in_seq_tokens), <span class=\"built_in\">len</span>(out_seq_tokens)) &gt; max_seq_len - <span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span>  <span class=\"comment\"># 如果加上EOS后长于max_seq_len，则忽略掉此样本</span></span><br><span class=\"line\">        process_one_seq(in_seq_tokens, in_tokens, in_seqs, max_seq_len)</span><br><span class=\"line\">        process_one_seq(out_seq_tokens, out_tokens, out_seqs, max_seq_len)</span><br><span class=\"line\">    in_vocab, in_data = build_data(in_tokens, in_seqs)</span><br><span class=\"line\">    out_vocab, out_data = build_data(out_tokens, out_seqs)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> in_vocab, out_vocab, Data.TensorDataset(in_data, out_data)</span><br></pre></td></tr></table></figure>\n<p>将序列的最大长度设成7，然后查看读取到的第一个样本。该样本分别包含法语词索引序列和英语词索引序列。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">max_seq_len = <span class=\"number\">7</span></span><br><span class=\"line\">in_vocab, out_vocab, dataset = read_data(max_seq_len)</span><br><span class=\"line\">dataset[<span class=\"number\">0</span>]</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab13-3.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"含注意力机制的编码器—解码器\"><a href=\"#含注意力机制的编码器—解码器\" class=\"headerlink\" title=\"含注意力机制的编码器—解码器\"></a>含注意力机制的编码器—解码器</h2><p>我们将使用含注意力机制的编码器—解码器来将一段简短的法语翻译成英语。下面我们来介绍模型的实现。</p>\n<h3 id=\"编码器-1\"><a href=\"#编码器-1\" class=\"headerlink\" title=\"编码器\"></a>编码器</h3><p>在编码器中，我们将输入语言的词索引通过词嵌入层得到词的表征，然后输入到一个多层门控循环单元中。PyTorch的<code>nn.GRU</code>实例在前向计算后也会分别返回输出和最终时间步的多层隐藏状态。其中的输出指的是最后一层的隐藏层在各个时间步的隐藏状态，并不涉及输出层计算。注意力机制将这些输出作为键项和值项。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Encoder</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class=\"line\"><span class=\"params\">                 drop_prob=<span class=\"number\">0</span>, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Encoder, self).__init__(**kwargs)</span><br><span class=\"line\">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class=\"line\">        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=drop_prob)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, inputs, state</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 输入形状是(批量大小, 时间步数)。将输出互换样本维和时间步维</span></span><br><span class=\"line\">        embedding = self.embedding(inputs.long()).permute(<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>) <span class=\"comment\"># (seq_len, batch, input_size)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.rnn(embedding, state)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">begin_state</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br></pre></td></tr></table></figure>\n<p>下面我们来创建一个批量大小为4、时间步数为7的小批量序列输入。设门控循环单元的隐藏层个数为2，隐藏单元个数为16。编码器对该输入执行前向计算后返回的输出形状为(时间步数, 批量大小, 隐藏单元个数)。门控循环单元在最终时间步的多层隐藏状态的形状为(隐藏层个数, 批量大小, 隐藏单元个数)。对于门控循环单元来说，<code>state</code>就是一个元素，即隐藏状态；如果使用长短期记忆，<code>state</code>是一个元组，包含两个元素即隐藏状态和记忆细胞。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">encoder = Encoder(vocab_size=<span class=\"number\">10</span>, embed_size=<span class=\"number\">8</span>, num_hiddens=<span class=\"number\">16</span>, num_layers=<span class=\"number\">2</span>)</span><br><span class=\"line\">output, state = encoder(torch.zeros((<span class=\"number\">4</span>, <span class=\"number\">7</span>)), encoder.begin_state())</span><br><span class=\"line\">output.shape, state.shape <span class=\"comment\"># GRU的state是h, 而LSTM的是一个元组(h, c)</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab13-4.png?raw=true\" alt=\"alt text\"></p>\n<h3 id=\"注意力机制-1\"><a href=\"#注意力机制-1\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h3><p>我们将实现注意力机制中定义的函数$a$：将输入连结后通过含单隐藏层的多层感知机变换。其中隐藏层的输入是解码器的隐藏状态与编码器在所有时间步上隐藏状态的一一连结，且使用tanh函数作为激活函数。输出层的输出个数为1。两个<code>Linear</code>实例均不使用偏差。其中函数$a$定义里向量$\\boldsymbol{v}$的长度是一个超参数，即<code>attention_size</code>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention_model</span>(<span class=\"params\">input_size, attention_size</span>):</span><br><span class=\"line\">    model = nn.Sequential(nn.Linear(input_size, attention_size, bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">                          nn.Tanh(),</span><br><span class=\"line\">                          nn.Linear(attention_size, <span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure>\n<p>注意力机制的输入包括查询项、键项和值项。设编码器和解码器的隐藏单元个数相同。这里的查询项为解码器在上一时间步的隐藏状态，形状为(批量大小, 隐藏单元个数)；键项和值项均为编码器在所有时间步的隐藏状态，形状为(时间步数, 批量大小, 隐藏单元个数)。注意力机制返回当前时间步的背景变量，形状为(批量大小, 隐藏单元个数)。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">attention_forward</span>(<span class=\"params\">model, enc_states, dec_state</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    enc_states: (时间步数, 批量大小, 隐藏单元个数)</span></span><br><span class=\"line\"><span class=\"string\">    dec_state: (批量大小, 隐藏单元个数)</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 将解码器隐藏状态广播到和编码器隐藏状态形状相同后进行连结</span></span><br><span class=\"line\">    dec_states = dec_state.unsqueeze(dim=<span class=\"number\">0</span>).expand_as(enc_states)</span><br><span class=\"line\">    enc_and_dec_states = torch.cat((enc_states, dec_states), dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">    e = model(enc_and_dec_states)  <span class=\"comment\"># 形状为(时间步数, 批量大小, 1)</span></span><br><span class=\"line\">    alpha = F.softmax(e, dim=<span class=\"number\">0</span>)  <span class=\"comment\"># 在时间步维度做softmax运算</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (alpha * enc_states).<span class=\"built_in\">sum</span>(dim=<span class=\"number\">0</span>)  <span class=\"comment\"># 返回背景变量</span></span><br></pre></td></tr></table></figure>\n<p>在下面的例子中，编码器的时间步数为10，批量大小为4，编码器和解码器的隐藏单元个数均为8。注意力机制返回一个小批量的背景向量，每个背景向量的长度等于编码器的隐藏单元个数。因此输出的形状为(4, 8)。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">seq_len, batch_size, num_hiddens = <span class=\"number\">10</span>, <span class=\"number\">4</span>, <span class=\"number\">8</span></span><br><span class=\"line\">model = attention_model(<span class=\"number\">2</span>*num_hiddens, <span class=\"number\">10</span>) </span><br><span class=\"line\">enc_states = torch.zeros((seq_len, batch_size, num_hiddens))</span><br><span class=\"line\">dec_state = torch.zeros((batch_size, num_hiddens))</span><br><span class=\"line\">attention_forward(model, enc_states, dec_state).shape</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab13-5.png?raw=true\" alt=\"alt text\"></p>\n<h3 id=\"含注意力机制的解码器\"><a href=\"#含注意力机制的解码器\" class=\"headerlink\" title=\"含注意力机制的解码器\"></a>含注意力机制的解码器</h3><p>我们直接将编码器在最终时间步的隐藏状态作为解码器的初始隐藏状态。这要求编码器和解码器的循环神经网络使用相同的隐藏层个数和隐藏单元个数。</p>\n<p>在解码器的前向计算中，我们先通过刚刚介绍的注意力机制计算得到当前时间步的背景向量。由于解码器的输入来自输出语言的词索引，我们将输入通过词嵌入层得到表征，然后和背景向量在特征维连结。我们将连结后的结果与上一时间步的隐藏状态通过门控循环单元计算出当前时间步的输出与隐藏状态。最后，我们将输出通过全连接层变换为有关各个输出词的预测，形状为(批量大小, 输出词典大小)。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Decoder</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class=\"line\"><span class=\"params\">                 attention_size, drop_prob=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Decoder, self).__init__()</span><br><span class=\"line\">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class=\"line\">        self.attention = attention_model(<span class=\"number\">2</span>*num_hiddens, attention_size)</span><br><span class=\"line\">        <span class=\"comment\"># GRU的输入包含attention输出的c和实际输入, 所以尺寸是 num_hiddens+embed_size</span></span><br><span class=\"line\">        self.rnn = nn.GRU(num_hiddens + embed_size, num_hiddens, </span><br><span class=\"line\">                          num_layers, dropout=drop_prob)</span><br><span class=\"line\">        self.out = nn.Linear(num_hiddens, vocab_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, cur_input, state, enc_states</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        cur_input shape: (batch, )</span></span><br><span class=\"line\"><span class=\"string\">        state shape: (num_layers, batch, num_hiddens)</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># 使用注意力机制计算背景向量</span></span><br><span class=\"line\">        c = attention_forward(self.attention, enc_states, state[-<span class=\"number\">1</span>])</span><br><span class=\"line\">        <span class=\"comment\"># 将嵌入后的输入和背景向量在特征维连结, (批量大小, num_hiddens+embed_size)</span></span><br><span class=\"line\">        input_and_c = torch.cat((self.embedding(cur_input), c), dim=<span class=\"number\">1</span>) </span><br><span class=\"line\">        <span class=\"comment\"># 为输入和背景向量的连结增加时间步维，时间步个数为1</span></span><br><span class=\"line\">        output, state = self.rnn(input_and_c.unsqueeze(<span class=\"number\">0</span>), state)</span><br><span class=\"line\">        <span class=\"comment\"># 移除时间步维，输出形状为(批量大小, 输出词典大小)</span></span><br><span class=\"line\">        output = self.out(output).squeeze(dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, state</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">begin_state</span>(<span class=\"params\">self, enc_state</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 直接将编码器最终时间步的隐藏状态作为解码器的初始隐藏状态</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> enc_state</span><br></pre></td></tr></table></figure>\n<h2 id=\"训练模型-1\"><a href=\"#训练模型-1\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h2><p>我们先实现<code>batch_loss</code>函数计算一个小批量的损失。解码器在最初时间步的输入是特殊字符<code>BOS</code>。之后，解码器在某时间步的输入为样本输出序列在上一时间步的词，即强制教学。我们在这里用掩码变量避免填充项对损失函数计算的影响。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">batch_loss</span>(<span class=\"params\">encoder, decoder, X, Y, loss</span>):</span><br><span class=\"line\">    batch_size = X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    enc_state = encoder.begin_state()</span><br><span class=\"line\">    enc_outputs, enc_state = encoder(X, enc_state)</span><br><span class=\"line\">    <span class=\"comment\"># 初始化解码器的隐藏状态</span></span><br><span class=\"line\">    dec_state = decoder.begin_state(enc_state)</span><br><span class=\"line\">    <span class=\"comment\"># 解码器在最初时间步的输入是BOS</span></span><br><span class=\"line\">    dec_input = torch.tensor([out_vocab.stoi[BOS]] * batch_size)</span><br><span class=\"line\">    <span class=\"comment\"># 我们将使用掩码变量mask来忽略掉标签为填充项PAD的损失, 初始全1</span></span><br><span class=\"line\">    mask, num_not_pad_tokens = torch.ones(batch_size,), <span class=\"number\">0</span></span><br><span class=\"line\">    l = torch.tensor([<span class=\"number\">0.0</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> Y.permute(<span class=\"number\">1</span>,<span class=\"number\">0</span>): <span class=\"comment\"># Y shape: (batch, seq_len)</span></span><br><span class=\"line\">        dec_output, dec_state = decoder(dec_input, dec_state, enc_outputs)</span><br><span class=\"line\">        l = l + (mask * loss(dec_output, y)).<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">        dec_input = y  <span class=\"comment\"># 使用强制教学</span></span><br><span class=\"line\">        num_not_pad_tokens += mask.<span class=\"built_in\">sum</span>().item()</span><br><span class=\"line\">        <span class=\"comment\"># EOS后面全是PAD. 下面一行保证一旦遇到EOS接下来的循环中mask就一直是0</span></span><br><span class=\"line\">        mask = mask * (y != out_vocab.stoi[EOS]).<span class=\"built_in\">float</span>()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> l / num_not_pad_tokens</span><br></pre></td></tr></table></figure>\n<p>在训练函数中，我们需要同时迭代编码器和解码器的模型参数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">encoder, decoder, dataset, lr, batch_size, num_epochs</span>):</span><br><span class=\"line\">    enc_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)</span><br><span class=\"line\">    dec_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)</span><br><span class=\"line\"></span><br><span class=\"line\">    loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">    data_iter = Data.DataLoader(dataset, batch_size, shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        l_sum = <span class=\"number\">0.0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, Y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">            enc_optimizer.zero_grad()</span><br><span class=\"line\">            dec_optimizer.zero_grad()</span><br><span class=\"line\">            l = batch_loss(encoder, decoder, X, Y, loss)</span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            enc_optimizer.step()</span><br><span class=\"line\">            dec_optimizer.step()</span><br><span class=\"line\">            l_sum += l.item()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % <span class=\"number\">10</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&quot;epoch %d, loss %.3f&quot;</span> % (epoch + <span class=\"number\">1</span>, l_sum / <span class=\"built_in\">len</span>(data_iter)))</span><br></pre></td></tr></table></figure>\n<p>接下来，创建模型实例并设置超参数。然后，我们就可以训练模型了。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">embed_size, num_hiddens, num_layers = <span class=\"number\">64</span>, <span class=\"number\">64</span>, <span class=\"number\">2</span></span><br><span class=\"line\">attention_size, drop_prob, lr, batch_size, num_epochs = <span class=\"number\">10</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.01</span>, <span class=\"number\">2</span>, <span class=\"number\">50</span></span><br><span class=\"line\">encoder = Encoder(<span class=\"built_in\">len</span>(in_vocab), embed_size, num_hiddens, num_layers,</span><br><span class=\"line\">                  drop_prob)</span><br><span class=\"line\">decoder = Decoder(<span class=\"built_in\">len</span>(out_vocab), embed_size, num_hiddens, num_layers,</span><br><span class=\"line\">                  attention_size, drop_prob)</span><br><span class=\"line\">train(encoder, decoder, dataset, lr, batch_size, num_epochs)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab13-6.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"预测不定长的序列\"><a href=\"#预测不定长的序列\" class=\"headerlink\" title=\"预测不定长的序列\"></a>预测不定长的序列</h2><p>在束搜索中我们介绍了3种方法来生成解码器在每个时间步的输出。这里我们实现最简单的贪婪搜索。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">translate</span>(<span class=\"params\">encoder, decoder, input_seq, max_seq_len</span>):</span><br><span class=\"line\">    in_tokens = input_seq.split(<span class=\"string\">&#x27; &#x27;</span>)</span><br><span class=\"line\">    in_tokens += [EOS] + [PAD] * (max_seq_len - <span class=\"built_in\">len</span>(in_tokens) - <span class=\"number\">1</span>)</span><br><span class=\"line\">    enc_input = torch.tensor([[in_vocab.stoi[tk] <span class=\"keyword\">for</span> tk <span class=\"keyword\">in</span> in_tokens]]) <span class=\"comment\"># batch=1</span></span><br><span class=\"line\">    enc_state = encoder.begin_state()</span><br><span class=\"line\">    enc_output, enc_state = encoder(enc_input, enc_state)</span><br><span class=\"line\">    dec_input = torch.tensor([out_vocab.stoi[BOS]])</span><br><span class=\"line\">    dec_state = decoder.begin_state(enc_state)</span><br><span class=\"line\">    output_tokens = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_seq_len):</span><br><span class=\"line\">        dec_output, dec_state = decoder(dec_input, dec_state, enc_output)</span><br><span class=\"line\">        pred = dec_output.argmax(dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        pred_token = out_vocab.itos[<span class=\"built_in\">int</span>(pred.item())]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> pred_token == EOS:  <span class=\"comment\"># 当任一时间步搜索出EOS时，输出序列即完成</span></span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            output_tokens.append(pred_token)</span><br><span class=\"line\">            dec_input = pred</span><br><span class=\"line\">    <span class=\"keyword\">return</span> output_tokens</span><br></pre></td></tr></table></figure>\n<p>简单测试一下模型。输入法语句子“ils regardent.”，翻译后的英语句子应该是“they are watching.”。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">input_seq = <span class=\"string\">&#x27;ils regardent .&#x27;</span></span><br><span class=\"line\">translate(encoder, decoder, input_seq, max_seq_len)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab13-7.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"评价翻译结果\"><a href=\"#评价翻译结果\" class=\"headerlink\" title=\"评价翻译结果\"></a>评价翻译结果</h2><p>评价机器翻译结果通常使用BLEU（Bilingual Evaluation Understudy）[1]。对于模型预测序列中任意的子序列，BLEU考察这个子序列是否出现在标签序列中。</p>\n<p>具体来说，设词数为$n$的子序列的精度为$p_n$。它是预测序列与标签序列匹配词数为$n$的子序列的数量与预测序列中词数为$n$的子序列的数量之比。举个例子，假设标签序列为$A$、$B$、$C$、$D$、$E$、$F$，预测序列为$A$、$B$、$B$、$C$、$D$，那么$p_1 &#x3D; 4&#x2F;5, p_2 &#x3D; 3&#x2F;4, p_3 &#x3D; 1&#x2F;3, p_4 &#x3D; 0$。设$len_{\\text{label}}$和$len_{\\text{pred}}$分别为标签序列和预测序列的词数，那么，BLEU的定义为</p>\n<p>$$ \\exp\\left(\\min\\left(0, 1 - \\frac{len_{\\text{label}}}{len_{\\text{pred}}}\\right)\\right) \\prod_{n&#x3D;1}^k p_n^{1&#x2F;2^n},$$</p>\n<p>其中$k$是我们希望匹配的子序列的最大词数。可以看到当预测序列和标签序列完全一致时，BLEU为1。</p>\n<p>因为匹配较长子序列比匹配较短子序列更难，BLEU对匹配较长子序列的精度赋予了更大权重。例如，当$p_n$固定在0.5时，随着$n$的增大，$0.5^{1&#x2F;2} \\approx 0.7, 0.5^{1&#x2F;4} \\approx 0.84, 0.5^{1&#x2F;8} \\approx 0.92, 0.5^{1&#x2F;16} \\approx 0.96$。另外，模型预测较短序列往往会得到较高$p_n$值。因此，上式中连乘项前面的系数是为了惩罚较短的输出而设的。举个例子，当$k&#x3D;2$时，假设标签序列为$A$、$B$、$C$、$D$、$E$、$F$，而预测序列为$A$、$B$。虽然$p_1 &#x3D; p_2 &#x3D; 1$，但惩罚系数$\\exp(1-6&#x2F;2) \\approx 0.14$，因此BLEU也接近0.14。</p>\n<p>下面来实现BLEU的计算。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">bleu</span>(<span class=\"params\">pred_tokens, label_tokens, k</span>):</span><br><span class=\"line\">    len_pred, len_label = <span class=\"built_in\">len</span>(pred_tokens), <span class=\"built_in\">len</span>(label_tokens)</span><br><span class=\"line\">    score = math.exp(<span class=\"built_in\">min</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span> - len_label / len_pred))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> n <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, k + <span class=\"number\">1</span>):</span><br><span class=\"line\">        num_matches, label_subs = <span class=\"number\">0</span>, collections.defaultdict(<span class=\"built_in\">int</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(len_label - n + <span class=\"number\">1</span>):</span><br><span class=\"line\">            label_subs[<span class=\"string\">&#x27;&#x27;</span>.join(label_tokens[i: i + n])] += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(len_pred - n + <span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> label_subs[<span class=\"string\">&#x27;&#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                num_matches += <span class=\"number\">1</span></span><br><span class=\"line\">                label_subs[<span class=\"string\">&#x27;&#x27;</span>.join(pred_tokens[i: i + n])] -= <span class=\"number\">1</span></span><br><span class=\"line\">        score *= math.<span class=\"built_in\">pow</span>(num_matches / (len_pred - n + <span class=\"number\">1</span>), math.<span class=\"built_in\">pow</span>(<span class=\"number\">0.5</span>, n))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> score</span><br></pre></td></tr></table></figure>\n<p>接下来，定义一个辅助打印函数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">score</span>(<span class=\"params\">input_seq, label_seq, k</span>):</span><br><span class=\"line\">    pred_tokens = translate(encoder, decoder, input_seq, max_seq_len)</span><br><span class=\"line\">    label_tokens = label_seq.split(<span class=\"string\">&#x27; &#x27;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;bleu %.3f, predict: %s&#x27;</span> % (bleu(pred_tokens, label_tokens, k),</span><br><span class=\"line\">                                      <span class=\"string\">&#x27; &#x27;</span>.join(pred_tokens)))</span><br></pre></td></tr></table></figure>\n<p>预测正确则分数为1。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">score(<span class=\"string\">&#x27;ils regardent .&#x27;</span>, <span class=\"string\">&#x27;they are watching .&#x27;</span>, k=<span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab13-8.png?raw=true\" alt=\"alt text\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">score(<span class=\"string\">&#x27;ils sont canadienne .&#x27;</span>, <span class=\"string\">&#x27;they are canadian .&#x27;</span>, k=<span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab13-9.png?raw=true\" alt=\"alt text\"></p>\n","categories":["Deep Learning"]},{"title":"基于transformer和pytorch的中日机器翻译模型","url":"/2024/06/24/lab14/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"背景知识\"><a href=\"#背景知识\" class=\"headerlink\" title=\"背景知识\"></a>背景知识</h1><h2 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h2><p>本篇章将从attention开始，逐步对Transformer结构所涉及的知识进行深入讲解，希望能给读者以形象生动的描述。</p>\n<p>问题：Attention出现的原因是什么？<br>潜在的答案：基于循环神经网络（RNN）一类的seq2seq模型，在处理长文本时遇到了挑战，而对长文本中不同位置的信息进行attention有助于提升RNN的模型效果。</p>\n<p>于是学习的问题就拆解为：1. 什么是seq2seq模型？2. 基于RNN的seq2seq模型如何处理文本&#x2F;长文本序列？3. seq2seq模型处理长文本序列时遇到了什么问题？4.基于RNN的seq2seq模型如何结合attention来改善模型效果？</p>\n<span id=\"more\"></span>\n<h3 id=\"seq2seq框架\"><a href=\"#seq2seq框架\" class=\"headerlink\" title=\"seq2seq框架\"></a>seq2seq框架</h3><p>seq2seq是一种常见的NLP模型结构，全称是：sequence to sequence，翻译为“序列到序列”。顾名思义：从一个文本序列得到一个新的文本序列。典型的任务有：机器翻译任务，文本摘要任务。谷歌翻译在2016年末开始使用seq2seq模型，并发表了2篇开创性的论文：<a href=\"https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\">Sutskever等2014年发表的Sequence to Sequence Learning with Neural Networks</a>和<a href=\"http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf\">Cho等2014年发表的Learning Phrase Representations using RNN Encoder–Decoder<br>for Statistical Machine Translation</a>，感兴趣的读者可以阅读原文进行学习。</p>\n<p>无论读者是否读过上述两篇谷歌的文章，NLP初学者想要充分理解并实现seq2seq模型很不容易。因为，我们需要拆解一系列相关的NLP概念，而这些NLP概念又是是层层递进的，所以想要清晰的对seq2seq模型有一个清晰的认识并不容易。但是，如果能够把这些复杂生涩的NLP概念可视化，理解起来其实就更简单了。因此，本文希望通过一系列图片、动态图帮助NLP初学者学习seq2seq以及attention相关的概念和知识。</p>\n<p>首先看seq2seq干了什么事情？seq2seq模型的输入可以是一个（单词、字母或者图像特征）序列，输出是另外一个（单词、字母或者图像特征）序列。一个训练好的seq2seq模型如下图所示（注释：将鼠标放在图上，图就会动起来）：</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-seq2seq.gif?raw=true\" alt=\"seq2seq\">动态图：seq2seq</p>\n<p>如下图所示，以NLP中的机器翻译任务为例，序列指的是一连串的单词，输出也是一连串单词。<br><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-2-translation.gif?raw=true\" alt=\"translation\">动态图：translation</p>\n<h3 id=\"seq2seq细节\"><a href=\"#seq2seq细节\" class=\"headerlink\" title=\"seq2seq细节\"></a>seq2seq细节</h3><p>将上图中蓝色的seq2seq模型进行拆解，如下图所示：seq2seq模型由编码器（Encoder）和解码器（Decoder）组成。绿色的编码器会处理输入序列中的每个元素并获得输入信息，这些信息会被转换成为一个黄色的向量（称为context向量）。当我们处理完整个输入序列后，编码器把 context向量 发送给紫色的解码器，解码器通过context向量中的信息，逐个元素输出新的序列。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-3-encoder-decoder.gif?raw=true\" alt=\"encoder-decode\">动态图：seq2seq中的encoder-decoder</p>\n<p>由于seq2seq模型可以用来解决机器翻译任务，因此机器翻译被任务seq2seq模型解决过程如下图所示，当作seq2seq模型的一个具体例子来学习。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-3-mt.gif?raw=true\" alt=\"encoder-decoder\">动态图：seq2seq中的encoder-decoder，机器翻译的例子</p>\n<p>深入学习机器翻译任务中的seq2seq模型，如下图所示。seq2seq模型中的编码器和解码器一般采用的是循环神经网络RNN（Transformer模型还没出现的过去时代）。编码器将输入的法语单词序列编码成context向量（在绿色encoder和紫色decoder中间出现），然后解码器根据context向量解码出英语单词序列。*</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-4-context-example.png?raw=true\" alt=\"context向量对应图里中间一个浮点数向量。在下文中，我们会可视化这些向量，使用更明亮的色彩来表示更高的值，如上图右边所示\"></p>\n<p>图：context向量对应上图中间浮点数向量。在下文中，我们会可视化这些数字向量，使用更明亮的色彩来表示更高的值，如上图右边所示</p>\n<p>如上图所示，我们来看一下黄色的context向量是什么？本质上是一组浮点数。而这个context的数组长度是基于编码器RNN的隐藏层神经元数量的。上图展示了长度为4的context向量，但在实际应用中，context向量的长度是自定义的，比如可能是256，512或者1024。</p>\n<p>那么RNN是如何具体地处理输入序列的呢？</p>\n<ol>\n<li><p>假设序列输入是一个句子，这个句子可以由$n$个词表示：$sentence &#x3D; {w_1, w_2,…,w_n}$。</p>\n</li>\n<li><p>RNN首先将句子中的每一个词映射成为一个向量得到一个向量序列：$X &#x3D; {x_1, x_2,…,x_n}$，每个单词映射得到的向量通常又叫做：word embedding。</p>\n</li>\n<li><p>然后在处理第$t \\in [1,n]$个时间步的序列输入$x_t$时，RNN网络的输入和输出可以表示为：$h_{t} &#x3D; RNN(x_t, h_{t-1})$</p>\n<ul>\n<li>输入：RNN在时间步$t$的输入之一为单词$w_t$经过映射得到的向量$x_t$。</li>\n<li>输入：RNN另一个输入为上一个时间步$t-1$得到的hidden state向量$h_{t-1}$，同样是一个向量。</li>\n<li>输出：RNN在时间步$t$的输出为$h_t$ hidden state向量。</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-5-word-vector.png?raw=true\" alt=\"我们在处理单词之前，需要把他们转换为向量。这个转换是使用 word embedding 算法来完成的。我们可以使用预训练好的 embeddings，或者在我们的数据集上训练自己的 embedding。通常 embedding 向量大小是 200 或者 300，为了简单起见，我们这里展示的向量长度是4\"> 图：word embedding例子。我们在处理单词之前，需要将单词映射成为向量，通常使用 word embedding 算法来完成。一般来说，我们可以使用提前训练好的 word embeddings，或者在自有的数据集上训练word embedding。为了简单起见，上图展示的word embedding维度是4。上图左边每个单词经过word embedding算法之后得到中间一个对应的4维的向量。</p>\n<p>让我们来进一步可视化一下基于RNN的seq2seq模型中的编码器在第1个时间步是如何工作：</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-6-rnn.gif?raw=true\" alt=\"rnn\"> 动态图：如图所示，RNN在第2个时间步，采用第1个时间步得到hidden state#10（隐藏层状态）和第2个时间步的输入向量input#1，来得到新的输出hidden state#1。</p>\n<p>看下面的动态图，让我们详细观察一下编码器如何在每个时间步得到hidden sate，并将最终的hidden state传输给解码器，解码器根据编码器所给予的最后一个hidden state信息解码处输出序列。注意，最后一个 hidden state实际上是我们上文提到的context向量。<br><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-6-seq2seq.gif?raw=true\"> 动态图：编码器逐步得到hidden state并传输最后一个hidden state给解码器。</p>\n<p>接着，结合编码器处理输入序列，一起来看下解码器如何一步步得到输出序列的l。与编码器类似，解码器在每个时间步也会得到 hidden state（隐藏层状态），而且也需要把 hidden state（隐藏层状态）从一个时间步传递到下一个时间步。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-6-seq2seq-decoder.gif?raw=true\"> 动态图：编码器首先按照时间步依次编码每个法语单词，最终将最后一个hidden state也就是context向量传递给解码器，解码器根据context向量逐步解码得到英文输出。</p>\n<p>目前为止，希望你已经明白了本文开头提出的前两个问题：1. 什么是seq2seq模型？2. seq2seq模型如何处理文本&#x2F;长文本序列？那么请思考第3、4个问题：3. seq2seq模型处理文本序列（特别是长文本序列）时会遇到什么问题？4.基于RNN的seq2seq模型如何结合attention来解决问题3并提升模型效果？</p>\n<h3 id=\"Attention\"><a href=\"#Attention\" class=\"headerlink\" title=\"Attention\"></a>Attention</h3><p>基于RNN的seq2seq模型编码器所有信息都编码到了一个context向量中，便是这类模型的瓶颈。一方面单个向量很难包含所有文本序列的信息，另一方面RNN递归地编码文本序列使得模型在处理长文本时面临非常大的挑战（比如RNN处理到第500个单词的时候，很难再包含1-499个单词中的所有信息了）。</p>\n<p>面对以上问题，Bahdanau等2014发布的<a href=\"https://arxiv.org/abs/1409.0473\">Neural Machine Translation by Jointly Learning to Align and Translate</a> 和 Luong等2015年发布的<a href=\"https://arxiv.org/abs/1508.04025\">Effective Approaches to Attention-based Neural Machine Translation\n</a>两篇论文中，提出了一种叫做注意力<strong>attetion</strong>的技术。通过attention技术，seq2seq模型极大地提高了机器翻译的质量。归其原因是：attention注意力机制，使得seq2seq模型可以有区分度、有重点地关注输入序列。</p>\n<p>下图依旧是机器翻译的例子：</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-7-attetion.png?raw=true\" alt=\"在第7个时间步，注意力机制使得解码器在产生英语翻译之前，可以将注意力集中在 &quot;student&quot; 这个词（在法语里，是 &quot;student&quot; 的意思）。这种从输入序列放大相关信号的能力，使得注意力模型，比没有注意力的模型，产生更好的结果。\"> 图：在第 7 个时间步，注意力机制使得解码器在产生英语翻译student英文翻译之前，可以将注意力集中在法语输入序列的：étudiant。这种有区分度得attention到输入序列的重要信息，使得模型有更好的效果。</p>\n<p>让我们继续来理解带有注意力的seq2seq模型：一个注意力模型与经典的seq2seq模型主要有2点不同：</p>\n<ul>\n<li><p>A. 首先，编码器会把更多的数据传递给解码器。编码器把所有时间步的 hidden state（隐藏层状态）传递给解码器，而不是只传递最后一个 hidden state（隐藏层状态），如下面的动态图所示:<br><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-6-mt-1.gif?raw=true\"> 动态图: 更多的信息传递给decoder</p>\n</li>\n<li><p>B. 注意力模型的解码器在产生输出之前，做了一个额外的attention处理。如下图所示，具体为：</p>\n<ul>\n<li><ol>\n<li>由于编码器中每个 hidden state（隐藏层状态）都对应到输入句子中一个单词，那么解码器要查看所有接收到的编码器的 hidden state（隐藏层状态）。</li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li>给每个 hidden state（隐藏层状态）计算出一个分数（我们先忽略这个分数的计算过程）。</li>\n</ol>\n</li>\n<li><ol start=\"3\">\n<li>所有hidden state（隐藏层状态）的分数经过softmax进行归一化。</li>\n</ol>\n</li>\n<li><ol start=\"4\">\n<li>将每个 hidden state（隐藏层状态）乘以所对应的分数，从而能够让高分对应的  hidden state（隐藏层状态）会被放大，而低分对应的  hidden state（隐藏层状态）会被缩小。</li>\n</ol>\n</li>\n<li><ol start=\"5\">\n<li>将所有hidden state根据对应分数进行加权求和，得到对应时间步的context向量。<br><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-7-attention-dec.gif?raw=true\"> 动态图：在第4个时间步，编码器结合attention得到context向量的5个步骤。</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n<p>所以，attention可以简单理解为：一种有效的加权求和技术，其艺术在于如何获得权重。</p>\n<p>现在，让我们把所有内容都融合到下面的图中，来看看结合注意力的seq2seq模型解码器全流程，动态图展示的是第4个时间步：</p>\n<ol>\n<li>注意力模型的解码器 RNN 的输入包括：一个word embedding 向量，和一个初始化好的解码器 hidden state，图中是$h_{init}$。</li>\n<li>RNN 处理上述的 2 个输入，产生一个输出和一个新的 hidden state，图中为h4。</li>\n<li>注意力的步骤：我们使用编码器的所有 hidden state向量和 h4 向量来计算这个时间步的context向量（C4）。</li>\n<li>我们把 h4 和 C4 拼接起来，得到一个橙色向量。</li>\n<li>我们把这个橙色向量输入一个前馈神经网络（这个网络是和整个模型一起训练的）。</li>\n<li>根据前馈神经网络的输出向量得到输出单词：假设输出序列可能的单词有N个，那么这个前馈神经网络的输出向量通常是N维的，每个维度的下标对应一个输出单词，每个维度的数值对应的是该单词的输出概率。</li>\n<li>在下一个时间步重复1-6步骤。<br><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-7-attention-pro.gif?raw=true\"> 动态图：解码器结合attention全过程</li>\n</ol>\n<p>到目前为止，希望你已经知道本文开头提出的3、4问题的答案啦：3、seq2seq处理长文本序列的挑战是什么？4、seq2seq是如何结合attention来解决问题3中的挑战的？</p>\n<p>最后，我们可视化一下注意力机制，看看在解码器在每个时间步关注了输入序列的哪些部分：<br><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-7-attention.gif?raw=true\"> 动态图：解码步骤时候attention关注的词</p>\n<p>需要注意的是：注意力模型不是无意识地把输出的第一个单词对应到输入的第一个单词，它是在训练阶段学习到如何对两种语言的单词进行对应（在我们的例子中，是法语和英语）。</p>\n<p>下图还展示了注意力机制的准确程度（图片来自于上面提到的论文）：<br><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-8-attention-vis.png?raw=true\" alt=\"你可以看到模型在输出 &quot;European Economic Area&quot; 时，注意力分布情况。在法语中，这些单词的顺序，相对于英语，是颠倒的（&quot;européenne économique zone&quot;）。而其他词的顺序是类似的。\"> 图：可以看到模型在输出 “European Economic Area” 时，注意力分布情况。在法语中，这些单词的顺序，相对于英语，是颠倒的（”européenne économique zone”）。而其他词的顺序是类似的。</p>\n<h2 id=\"transformer\"><a href=\"#transformer\" class=\"headerlink\" title=\"transformer\"></a>transformer</h2><p>我们知晓了attention为循环神经网络带来的优点。那么有没有一种神经网络结构直接基于attention构造，并且不再依赖RNN、LSTM或者CNN网络结构了呢？答案便是：Transformer。因此，我们将在本小节对Transformer所涉及的细节进行深入探讨。</p>\n<p>Transformer模型在2017年被google提出，直接基于Self-Attention结构，取代了之前NLP任务中常用的RNN神经网络结构，并在WMT2014 Englishto-German和WMT2014 English-to-French两个机器翻译任务上都取得了当时的SOTA。</p>\n<p>与RNN这类神经网络结构相比，Transformer一个巨大的优点是：<strong>模型在处理序列输入时，可以对整个序列输入进行并行计算，不需要按照时间步循环递归处理输入序列。</strong>。</p>\n<p>下图先便是Transformer整体结构图，与seq2seq模型类似，Transformer模型结构中的左半部分为编码器（encoder），右半部分为解码器（decoder），下面我们来一步步拆解 Transformer。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-transformer.png?raw=true\" alt=\"transformer\"><br>图：transformer模型结构</p>\n<h3 id=\"Transformer宏观结构\"><a href=\"#Transformer宏观结构\" class=\"headerlink\" title=\"Transformer宏观结构\"></a>Transformer宏观结构</h3><p>Transformer最开始提出来解决机器翻译任务，因此可以看作是seq2seq模型的一种。本小节先抛开Transformer模型中结构具体细节，先从seq2seq的角度对Transformer进行宏观结构的学习。以机器翻译任务为例，先将Transformer这种特殊的seqseq模型看作一个黑盒，黑盒的输入是法语文本序列，输出是英语文本序列（对比seq2seq框架知识我们可以发现，Transformer宏观结构属于seq2seq范畴，只是将之前seq2seq中的编码器和解码器，从RNN模型替换成了Transformer模型）。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-input-output.png?raw=true\" alt=\"input-output\"><br>图：Transformer黑盒输入和输出</p>\n<p>将上图中的中间部分“THE TRANSFORMER”拆开成seq2seq标准结构，得到下图：左边是编码部分encoders，右边是解码器部分decoders。<br><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-encoder-decoder.png?raw=true\" alt=\"encoder-decoder\"><br>图：encoder-decoder</p>\n<p>下面，再将上图中的编码器和解码器细节绘出，得到下图。我们可以看到，编码部分（encoders）由多层编码器(Encoder)组成（Transformer论文中使用的是6层编码器，这里的层数6并不是固定的，你也可以根据实验效果来修改层数）。同理，解码部分（decoders）也是由多层的解码器(Decoder)组成（论文里也使用了6层解码器）。每层编码器网络结构是一样的，每层解码器网络结构也是一样的。不同层编码器和解码器网络结构不共享参数。<br><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-2-encoder-detail.png?raw=true\" alt=\"翻译例子\"></p>\n<p>图：6层编码和6层解码器</p>\n<p>接下来，我们看一下单层encoder，单层encoder主要由以下两部分组成，如下图所示</p>\n<ul>\n<li>Self-Attention Layer</li>\n<li>Feed Forward Neural Network（前馈神经网络，缩写为 FFNN）</li>\n</ul>\n<p>编码器的输入文本序列$w_1, w_2,…,w_n$最开始需要经过embedding转换，得到每个单词的向量表示$x_1, x_2,…,x_n$，其中$x_i \\in \\mathbb{R}^{d}$是维度为$d$的向量，然后所有向量经过一个Self-Attention神经网络层进行变换和信息交互得到$h_1, h_2,…h_n$，其中$h_i \\in \\mathbb{R}^{d}$是维度为$d$的向量。self-attention层处理一个词向量的时候，不仅会使用这个词本身的信息，也会使用句子中其他词的信息（你可以类比为：当我们翻译一个词的时候，不仅会只关注当前的词，也会关注这个词的上下文的其他词的信息）。Self-Attention层的输出会经过前馈神经网络得到新的$x_1, x_2,..,x_n$，依旧是$n$个维度为$d$的向量。这些向量将被送入下一层encoder，继续相同的操作。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-encoder.png?raw=true\" alt=\"encoder\"></p>\n<p>图：单层encoder</p>\n<p>与编码器对应，如下图，解码器在编码器的self-attention和FFNN中间插入了一个Encoder-Decoder Attention层，这个层帮助解码器聚焦于输入序列最相关的部分（类似于seq2seq模型中的 Attention）。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-decoder.webp?raw=true\" alt=\"decoder\"></p>\n<p>图：单层decoder</p>\n<p>总结一下，我们基本了解了Transformer由编码部分和解码部分组成，而编码部分和解码部分又由多个网络结构相同的编码层和解码层组成。每个编码层由self-attention和FFNN组成，每个解码层由self-attention、FFN和encoder-decoder attention组成。</p>\n<p>以上便是Transformer的宏观结构啦，下面我们开始看宏观结构中的模型细节。</p>\n<h3 id=\"Transformer结构细节\"><a href=\"#Transformer结构细节\" class=\"headerlink\" title=\"Transformer结构细节\"></a>Transformer结构细节</h3><p>了解了Transformer的宏观结构之后。下面，让我们来看看Transformer如何将输入文本序列转换为向量表示，又如何逐层处理这些向量表示得到最终的输出。</p>\n<h4 id=\"输入处理\"><a href=\"#输入处理\" class=\"headerlink\" title=\"输入处理\"></a>输入处理</h4><h5 id=\"词向量\"><a href=\"#词向量\" class=\"headerlink\" title=\"词向量\"></a>词向量</h5><p>和常见的NLP 任务一样，我们首先会使用词嵌入算法（embedding algorithm），将输入文本序列的每个词转换为一个词向量。实际应用中的向量一般是 256 或者 512 维。但为了简化起见，我们这里使用4维的词向量来进行讲解。</p>\n<p>如下图所示，假设我们的输入文本是序列包含了3个词，那么每个词可以通过词嵌入算法得到一个4维向量，于是整个输入被转化成为一个向量序列。在实际应用中，我们通常会同时给模型输入多个句子，如果每个句子的长度不一样，我们会选择一个合适的长度，作为输入文本序列的最大长度：如果一个句子达不到这个长度，那么就填充先填充一个特殊的“padding”词；如果句子超出这个长度，则做截断。最大序列长度是一个超参数，通常希望越大越好，但是更长的序列往往会占用更大的训练显存&#x2F;内存，因此需要在模型训练时候视情况进行决定。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-x.png?raw=true\" alt=\" 个词向量\"><br>图：3个词和对应的词向量</p>\n<p>输入序列每个单词被转换成词向量表示还将加上位置向量来得到该词的最终向量表示。</p>\n<h5 id=\"位置向量\"><a href=\"#位置向量\" class=\"headerlink\" title=\"位置向量\"></a>位置向量</h5><p>如下图所示，Transformer模型对每个输入的词向量都加上了一个位置向量。这些向量有助于确定每个单词的位置特征，或者句子中不同单词之间的距离特征。词向量加上位置向量背后的直觉是：将这些表示位置的向量添加到词向量中，得到的新向量，可以为模型提供更多有意义的信息，比如词的位置，词之间的距离等。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-position.png?raw=true\" alt=\"位置编码\"><br>图：位置编码向量</p>\n<p>依旧假设词向量和位置向量的维度是4，我们在下图中展示一种可能的位置向量+词向量：</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-position2.png?raw=true\" alt=\"位置编码\"><br>图：位置编码向量</p>\n<p>那么带有位置编码信息的向量到底遵循什么模式？原始论文中给出的设计表达式为：<br>$$<br>PE_{(pos,2i)} &#x3D; sin(pos &#x2F; 10000^{2i&#x2F;d_{\\text{model}}}) \\                                                                       PE_{(pos,2i+1)} &#x3D; cos(pos &#x2F; 10000^{2i&#x2F;d_{\\text{model}}})<br>$$<br>上面表达式中的$pos$代表词的位置，$d_{model}$代表位置向量的维度，$i \\in [0, d_{model})$代表位置$d_{model}$维位置向量第$i$维。于是根据上述公式，我们可以得到第$pos$位置的$d_{model}$维位置向量。在下图中，我们画出了一种位置向量在第4、5、6、7维度、不同位置的的数值大小。横坐标表示位置下标，纵坐标表示数值大小。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-2-pos-embedding.png?raw=true\" alt=\"位置编码图示\"><br>图：位置编码在0-100位置，在4、5、6、7维的数值图示</p>\n<p>当然，上述公式不是唯一生成位置编码向量的方法。但这种方法的优点是：可以扩展到未知的序列长度。例如：当我们的模型需要翻译一个句子，而这个句子的长度大于训练集中所有句子的长度，这时，这种位置编码的方法也可以生成一样长的位置编码向量。</p>\n<h4 id=\"编码器encoder\"><a href=\"#编码器encoder\" class=\"headerlink\" title=\"编码器encoder\"></a>编码器encoder</h4><p>编码部分的输入文本序列经过输入处理之后得到了一个向量序列，这个向量序列将被送入第1层编码器，第1层编码器输出的同样是一个向量序列，再接着送入下一层编码器：第1层编码器的输入是融合位置向量的词向量，<em>更上层编码器的输入则是上一层编码器的输出</em>。</p>\n<p>下图展示了向量序列在单层encoder中的流动：融合位置信息的词向量进入self-attention层，self-attention的输出每个位置的向量再输入FFN神经网络得到每个位置的新向量。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-x-encoder.png?raw=true\" alt=\"输入encoder\"><br>图：单层encoder的序列向量流动</p>\n<p>下面再看一个2个单词的例子：<br><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-multi-encoder.webp?raw=true\" alt=\"一层传一层\"><br>图：2个单词的例子：$x_1, x_2 \\to z_1, z_2 \\to r_1, r_2$</p>\n<h4 id=\"Self-Attention层\"><a href=\"#Self-Attention层\" class=\"headerlink\" title=\"Self-Attention层\"></a>Self-Attention层</h4><p>下面来分析一下上图中Self-Attention层的具体机制。</p>\n<h5 id=\"Self-Attention概览\"><a href=\"#Self-Attention概览\" class=\"headerlink\" title=\"Self-Attention概览\"></a>Self-Attention概览</h5><p>假设我们想要翻译的句子是：<br>$$<br>The \\space\\space animal \\space\\space didn’t \\space\\space cross \\space\\space the \\space\\space street \\space\\space because \\space\\space it \\space\\space was \\space\\space too \\space\\space tired<br>$$<br>这个句子中的 <em>it</em> 是一个指代词，那么 <em>it</em> 指的是什么呢？它是指 <em>animal</em> 还是<em>street</em>？这个问题对人来说，是很简单的，但是对模型来说并不是那么容易。但是，如果模型引入了<em>Self Attention</em>机制之后，便能够让模型把it和animal关联起来了。同样的，当模型处理句子中其他词时，<em>Self Attentio</em>n机制也可以使得模型不仅仅关注当前位置的词，还会关注句子中其他位置的相关的词，进而可以更好地理解当前位置的词。</p>\n<p>与RNN对比一下：RNN 在处理序列中的一个词时，会考虑句子前面的词传过来的<em>hidden state</em>，而<em>hidden state</em>就包含了前面的词的信息；而<em>Self Attention</em>机制值得是，当前词会直接关注到自己句子中前后相关的所有词语，如下图 <em>it</em> 的例子：</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-attention-word.png?raw=true\" alt=\"一个词和其他词的attention\"></p>\n<p>图：一个词和其他词的attention</p>\n<p>上图所示的<em>it</em>是一个真实的例子，是当Transformer在第5层编码器编码“it”时的状态，可视化之后显示<em>it</em>有一部分注意力集中在了“The animal”上，并且把这两个词的信息融合到了”it”中。</p>\n<h5 id=\"Self-Attention细节\"><a href=\"#Self-Attention细节\" class=\"headerlink\" title=\"Self-Attention细节\"></a>Self-Attention细节</h5><p>先通过一个简单的例子来理解一下：什么是“self-attention自注意力机制”？假设一句话包含两个单词：Thinking Machines。自注意力的一种理解是：Thinking-Thinking，Thinking-Machines，Machines-Thinking，Machines-Machines，共$2^2$种两两attention。那么具体如何计算呢？假设Thinking、Machines这两个单词经过词向量算法得到向量是$X_1, X_2$​：<br>$$<br>\\begin{array}{l}<br>1: q_1 &#x3D; X_1 W^Q, q_2 &#x3D; X_2 W^Q; k_1 &#x3D; X_1 W^K, k_2 &#x3D; X_2 W^K\\\\<br>v_1 &#x3D; X_1 W^V, v_2 &#x3D; X_2 W^V, W^Q, W^K, W^K \\in \\mathbb{R}^{d_x \\times d_k}\\\\<br>2-3: score_{11} &#x3D; \\frac{q_1 \\cdot q_1}{\\sqrt{d_k}} , score_{12} &#x3D; \\frac{q_1 \\cdot q_2}{\\sqrt{d_k}} ;\\<br>score_{21} &#x3D; \\frac{q_2 \\cdot q_1}{\\sqrt{d_k}}, score_{22} &#x3D; \\frac{q_2 \\cdot q_2}{\\sqrt{d_k}}; \\\\<br>4: score_{11} &#x3D; \\frac{e^{score_{11}}}{e^{score_{11}} + e^{score_{12}}},score_{12} &#x3D; \\frac{e^{score_{12}}}{e^{score_{11}} + e^{score_{12}}}; \\\\<br>score_{21} &#x3D; \\frac{e^{score_{21}}}{e^{score_{21}} + e^{score_{22}}},score_{22} &#x3D; \\frac{e^{score_{22}}}{e^{score_{21}} + e^{score_{22}}} \\\\<br>5-6: z_1 &#x3D; v_1 \\times score_{11} + v_2 \\times score_{12}; \\\\<br>z_2 &#x3D; v_1 \\times score_{21} + v_2 \\times score_{22}<br>\\end{array}<br>$$<br>下面，我们将上诉self-attention计算的6个步骤进行可视化。</p>\n<p>第1步：对输入编码器的词向量进行线性变换得到：Query向量: $q_1, q_2$，Key向量: $k_1, k_2$，Value向量: $v_1, v_2$。这3个向量是词向量分别和3个参数矩阵相乘得到的，而这个矩阵也是是模型要学习的参数。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-qkv.png?raw=true\" alt=\"Q,K,V\">图：计算Query向量：$q_1, q_2$，Key向量: $k_1, k_2$，Value向量: $v_1, v_2$。</p>\n<p>Query 向量，Key 向量，Value 向量是什么含义呢？</p>\n<p>其实它们就是 3 个向量，给它们加上一个名称，可以让我们更好地理解 Self-Attention 的计算过程和逻辑。attention计算的逻辑常常可以描述为：<strong>query和key计算相关或者叫attention得分，然后根据attention得分对value进行加权求和。</strong></p>\n<p>第2步：计算Attention Score（注意力分数）。假设我们现在计算第一个词<em>Thinking</em> 的Attention Score（注意力分数），需要根据<em>Thinking</em> 对应的词向量，对句子中的其他词向量都计算一个分数。这些分数决定了我们在编码<em>Thinking</em>这个词时，需要对句子中其他位置的词向量的权重。</p>\n<p>Attention score是根据”<em>Thinking</em>“ 对应的 Query 向量和其他位置的每个词的 Key 向量进行点积得到的。Thinking的第一个Attention Score就是$q_1$和$k_1$的内积，第二个分数就是$q_1$和$k_2$的点积。这个计算过程在下图中进行了展示，下图里的具体得分数据是为了表达方便而自定义的。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-think.png?raw=true\" alt=\"Thinking计算\"><br>图：Thinking的Attention Score计算</p>\n<p>第3步：把每个分数除以 $\\sqrt{d_k}$，$d_{k}$是Key向量的维度。你也可以除以其他数，除以一个数是为了在反向传播时，求梯度时更加稳定。</p>\n<p>第4步：接着把这些分数经过一个Softmax函数，Softmax可以将分数归一化，这样使得分数都是正数并且加起来等于1， 如下图所示。<br>这些分数决定了Thinking词向量，对其他所有位置的词向量分别有多少的注意力。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-think2.png?raw=true\" alt=\"Thinking计算\"><br>图：Thinking的Attention Score计算</p>\n<p>第5步：得到每个词向量的分数后，将分数分别与对应的Value向量相乘。这种做法背后的直觉理解就是：对于分数高的位置，相乘后的值就越大，我们把更多的注意力放到了它们身上；对于分数低的位置，相乘后的值就越小，这些位置的词可能是相关性不大的。</p>\n<p>第6步：把第5步得到的Value向量相加，就得到了Self Attention在当前位置（这里的例子是第1个位置）对应的输出。</p>\n<p>最后，在下图展示了 对第一个位置词向量计算Self Attention 的全过程。最终得到的当前位置（这里的例子是第一个位置）词向量会继续输入到前馈神经网络。注意：上面的6个步骤每次只能计算一个位置的输出向量，在实际的代码实现中，Self Attention的计算过程是使用矩阵快速计算的，一次就得到所有位置的输出向量。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-sum.png?raw=true\" alt=\"Think计算\"><br>图：Thinking经过attention之后的向量表示$z_1$</p>\n<h5 id=\"Self-Attention矩阵计算\"><a href=\"#Self-Attention矩阵计算\" class=\"headerlink\" title=\"Self-Attention矩阵计算\"></a>Self-Attention矩阵计算</h5><p>将self-attention计算6个步骤中的向量放一起，比如$X&#x3D;[x_1;x_2]$​，便可以进行矩阵计算啦。下面，依旧按步骤展示self-attention的矩阵计算方法。<br>$$<br>\\begin{aligned}<br>&amp;X &#x3D; [X_1;X_2] \\\\<br>&amp;Q &#x3D; X W^Q, K &#x3D; X W^K, V&#x3D;X W^V \\\\<br>&amp;Z &#x3D; softmax(\\frac{QK^T}{\\sqrt{d_k}}) V<br>\\end{aligned}<br>$$<br>第1步：计算 Query，Key，Value 的矩阵。首先，我们把所有词向量放到一个矩阵X中，然后分别和3个权重矩阵$W^Q, W^K W^V$ 相乘，得到 Q，K，V 矩阵。矩阵X中的每一行，表示句子中的每一个词的词向量。Q，K，V 矩阵中的每一行表示 Query向量，Key向量，Value 向量，向量维度是$d_k$。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-qkv-multi.png?raw=true\">图：QKV矩阵乘法</p>\n<p>第2步：由于我们使用了矩阵来计算，我们可以把上面的第 2 步到第 6 步压缩为一步，直接得到 Self Attention 的输出。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-attention-output.webp?raw=true\" alt=\"输出\"><br>图：得到输出$Z$</p>\n<h5 id=\"多头注意力机制\"><a href=\"#多头注意力机制\" class=\"headerlink\" title=\"多头注意力机制\"></a>多头注意力机制</h5><p>Transformer 的论文通过增加多头注意力机制（一组注意力称为一个 attention head），进一步完善了Self-Attention。这种机制从如下两个方面增强了attention层的能力：</p>\n<ul>\n<li><strong>它扩展了模型关注不同位置的能力</strong>。在上面的例子中，第一个位置的输出$z_1$​包含了句子中其他每个位置的很小一部分信息，但$z_1$​仅仅是单个向量，所以可能仅由第1个位置的信息主导了。而当我们翻译句子：<code>The  animal  didn&#39;t  cross  the  street  because  it  was  too  tired</code>时，我们不仅希望模型关注到”it”本身，还希望模型关注到”The”和“animal”，甚至关注到”tired”。这时，多头注意力机制会有帮助。</li>\n<li><strong>多头注意力机制赋予attention层多个“子表示空间”</strong>。下面我们会看到，多头注意力机制会有多组$W^Q, W^K W^V$​ 的权重矩阵（在 Transformer 的论文中，使用了 8 组注意力),，因此可以将$X$​变换到更多种子空间进行表示。接下来我们也使用8组注意力头（attention heads））。每一组注意力的权重矩阵都是随机初始化的，但经过训练之后，每一组注意力的权重$W^Q, W^K W^V$​ 可以把输入的向量映射到一个对应的”子表示空间“。</li>\n</ul>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-multi-head.png?raw=true\" alt=\"多头注意力机制\"><br>图：多头注意力机制</p>\n<p>在多头注意力机制中，我们为每组注意力设定单独的 WQ, WK, WV 参数矩阵。将输入X和每组注意力的WQ, WK, WV 相乘，得到8组 Q, K, V 矩阵。</p>\n<p>接着，我们把每组 K, Q, V 计算得到每组的 Z 矩阵，就得到8个Z矩阵。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-8z.webp?raw=true\" alt=\"8 个 Z 矩阵\"><br>图：8 个 Z 矩阵</p>\n<p>由于前馈神经网络层接收的是 1 个矩阵（其中每行的向量表示一个词），而不是 8 个矩阵，所以我们直接把8个子矩阵拼接起来得到一个大的矩阵，然后和另一个权重矩阵$W^O$相乘做一次变换，映射到前馈神经网络层所需要的维度。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-to1.webp?raw=true\" alt=\"整合矩阵\"><br>图：拼接8个子矩阵并进行映射变换</p>\n<p>总结一下就是：</p>\n<ol>\n<li>把8个矩阵 {Z0,Z1…,Z7} 拼接起来</li>\n<li>把拼接后的矩阵和WO权重矩阵相乘</li>\n<li>得到最终的矩阵Z，这个矩阵包含了所有 attention heads（注意力头） 的信息。这个矩阵会输入到FFNN (Feed Forward Neural Network)层。</li>\n</ol>\n<p>以上就是多头注意力的全部内容。最后将所有内容放到一张图中：</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-put-together.webp?raw=true\" alt=\"放在一起\"><br>图：多头注意力机制的矩阵运算</p>\n<p>学习了多头注意力机制，让我们再来看下当我们前面提到的it例子，不同的attention heads （注意力头）对应的“it”attention了哪些内容。下图中的绿色和橙色线条分别表示2组不同的attentin heads：</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-it-attention.webp?raw=true\" alt=\"`it`的attention\"><br>图：<code>it</code>的attention</p>\n<p>当我们编码单词”it”时，其中一个 attention head （橙色注意力头）最关注的是”the animal”，另外一个绿色 attention head 关注的是”tired”。因此在某种意义上，”it”在模型中的表示，融合了”animal”和”tire”的部分表达。</p>\n<h5 id=\"残差连接\"><a href=\"#残差连接\" class=\"headerlink\" title=\"残差连接\"></a>残差连接</h5><p>到目前为止，我们计算得到了self-attention的输出向量。而单层encoder里后续还有两个重要的操作：残差链接、标准化。</p>\n<p>编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization），如下图所示。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-resnet.png?raw=true\" alt=\"残差连接\"><br>图：残差连接</p>\n<p>将 Self-Attention 层的层标准化（layer-normalization）和涉及的向量计算细节都进行可视化，如下所示：</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-lyn.png?raw=true\" alt=\"标准化\"><br>图：标准化细节</p>\n<p>编码器和和解码器的子层里面都有层标准化（layer-normalization）。假设一个 Transformer 是由 2 层编码器和两层解码器组成的，将全部内部细节展示起来如下图所示。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-2layer.png?raw=true\" alt=\"2层示意图\"><br>图：2层Transformer示意图</p>\n<h4 id=\"解码器\"><a href=\"#解码器\" class=\"headerlink\" title=\"解码器\"></a>解码器</h4><p>现在我们已经介绍了编码器中的大部分概念，我们也基本知道了编码器的原理。现在让我们来看下， 编码器和解码器是如何协同工作的。</p>\n<p>编码器一般有多层，第一个编码器的输入是一个序列文本，最后一个编码器输出是一组序列向量，这组序列向量会作为解码器的K、V输入，其中K&#x3D;V&#x3D;解码器输出的序列向量表示。这些注意力向量将会输入到每个解码器的Encoder-Decoder Attention层，这有助于解码器把注意力集中到输入序列的合适位置，如下图所示。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/transformer_decoding_1.gif?raw=true\"></p>\n<p>解码（decoding ）阶段的每一个时间步都输出一个翻译后的单词（这里的例子是英语翻译），解码器当前时间步的输出又重新作为输入Q和编码器的输出K、V共同作为下一个时间步解码器的输入。然后重复这个过程，直到输出一个结束符。如下图所示：</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-decoder.gif?raw=true\" alt=\"decoder动态图\"><br>动态图：decoder动态图</p>\n<p>解码器中的 Self Attention 层，和编码器中的 Self Attention 层的区别：</p>\n<ol>\n<li>在解码器里，Self Attention 层只允许关注到输出序列中早于当前位置之前的单词。具体做法是：在 Self Attention 分数经过 Softmax 层之前，屏蔽当前位置之后的那些位置（将attention score设置成-inf）。</li>\n<li>解码器 Attention层是使用前一层的输出来构造Query 矩阵，而Key矩阵和 Value矩阵来自于编码器最终的输出。</li>\n</ol>\n<h4 id=\"线性层和softmax\"><a href=\"#线性层和softmax\" class=\"headerlink\" title=\"线性层和softmax\"></a>线性层和softmax</h4><p>Decoder 最终的输出是一个向量，其中每个元素是浮点数。我们怎么把这个向量转换为单词呢？这是线性层和softmax完成的。</p>\n<p>线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更大的向量，这个向量称为 logits 向量：假设我们的模型有 10000 个英语单词（模型的输出词汇表），此 logits 向量便会有 10000 个数字，每个数表示一个单词的分数。</p>\n<p>然后，Softmax 层会把这些分数转换为概率（把所有的分数转换为正数，并且加起来等于 1）。然后选择最高概率的那个数字对应的词，就是这个时间步的输出单词。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-linear.png?raw=true\" alt=\"线性层\"><br>图：线性层</p>\n<h4 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h4><p>Transformer训练的时候，需要将解码器的输出和label一同送入损失函数，以获得loss，最终模型根据loss进行方向传播。这一小节，我们用一个简单的例子来说明训练过程的loss计算：把“merci”翻译为“thanks”。</p>\n<p>我们希望模型解码器最终输出的概率分布，会指向单词 ”thanks“（在“thanks”这个词的概率最高）。但是，一开始模型还没训练好，它输出的概率分布可能和我们希望的概率分布相差甚远，如下图所示，正确的概率分布应该是“thanks”单词的概率最大。但是，由于模型的参数都是随机初始化的，所示一开始模型预测所有词的概率几乎都是随机的。</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-loss.webp?raw=true\" alt=\"概率分布\"><br>图：概率分布</p>\n<p>只要Transformer解码器预测了组概率，我们就可以把这组概率和正确的输出概率做对比，然后使用反向传播来调整模型的权重，使得输出的概率分布更加接近整数输出。</p>\n<p>那我们要怎么比较两个概率分布呢？：我们可以简单的用两组概率向量的的空间距离作为loss（向量相减，然后求平方和，再开方），当然也可以使用交叉熵(cross-entropy)]和KL 散度(Kullback–Leibler divergence)。读者可以进一步检索阅读相关知识，损失函数的知识不在本小节展开。</p>\n<p>由于上面仅有一个单词的例子太简单了，我们可以再看一个复杂一点的句子。句子输入是：“je suis étudiant” ，输出是：“i am a student”。这意味着，我们的transformer模型解码器要多次输出概率分布向量：</p>\n<ul>\n<li>每次输出的概率分布都是一个向量，长度是 vocab_size（前面约定最大vocab size，也就是向量长度是 6，但实际中的vocab size更可能是 30000 或者 50000）</li>\n<li>第1次输出的概率分布中，最高概率对应的单词是 “i”</li>\n<li>第2次输出的概率分布中，最高概率对应的单词是 “am”</li>\n<li>以此类推，直到第 5 个概率分布中，最高概率对应的单词是 “&lt;eos&gt;”，表示没有下一个单词了</li>\n</ul>\n<p>于是我们目标的概率分布长下面这个样子：</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-target.png?raw=true\" alt=\"概率分布\"><br>图：目标概率分布</p>\n<p>我们用例子中的句子训练模型，希望产生图中所示的概率分布<br>我们的模型在一个足够大的数据集上，经过足够长时间的训练后，希望输出的概率分布如下图所示：</p>\n<p><img src=\"https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-trained.webp?raw=true\" alt=\"训练后概率分布\"><br>图：模型训练后输出的多个概率分布</p>\n<p>我们希望模型经过训练之后可以输出的概率分布也就对应了正确的翻译。当然，如果你要翻译的句子是训练集中的一部分，那输出的结果并不能说明什么。我们希望模型在没见过的句子上也能够准确翻译。</p>\n<p>额外提一下greedy decoding和beam search的概念：</p>\n<ul>\n<li>Greedy decoding：由于模型每个时间步只产生一个输出，我们这样看待：模型是从概率分布中选择概率最大的词，并且丢弃其他词。这种方法叫做贪婪解码（greedy decoding）。</li>\n<li>Beam search：每个时间步保留k个最高概率的输出词，然后在下一个时间步，根据上一个时间步保留的k个词来确定当前应该保留哪k个词。假设k&#x3D;2，第一个位置概率最高的两个输出的词是”I“和”a“，这两个词都保留，然后根据第一个词计算第2个位置的词的概率分布，再取出第2个位置上2个概率最高的词。对于第3个位置和第4个位置，我们也重复这个过程。这种方法称为集束搜索(beam search)。</li>\n</ul>\n<h1 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h1><h2 id=\"具体配置\"><a href=\"#具体配置\" class=\"headerlink\" title=\"具体配置\"></a>具体配置</h2><p>我从云服务器平台租GPU来完成模型训练，以下是设备型号和CUDA版本<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab13.png?raw=true\" alt=\"alt text\"><br>外部库版本如下:</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab14.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"导入所需包\"><a href=\"#导入所需包\" class=\"headerlink\" title=\"导入所需包\"></a>导入所需包</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchtext</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> Tensor</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn.utils.rnn <span class=\"keyword\">import</span> pad_sequence </span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> Counter</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchtext.vocab <span class=\"keyword\">import</span> Vocab</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer</span><br><span class=\"line\"><span class=\"keyword\">import</span> io</span><br><span class=\"line\"><span class=\"keyword\">import</span> time </span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"><span class=\"keyword\">import</span> sentencepiece <span class=\"keyword\">as</span> spm</span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>)</span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.get_device_name(<span class=\"number\">0</span>)) </span><br><span class=\"line\"><span class=\"comment\">## 如果你有GPU，请在你自己的电脑上尝试运行这一套代码</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab14-3.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"获取并行数据集\"><a href=\"#获取并行数据集\" class=\"headerlink\" title=\"获取并行数据集\"></a>获取并行数据集</h2><p>在本教程中，我们将使用从 JParaCrawl 下载的<a href=\"http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl\">日英并行数据集</a>二次加工后得到的数据集，由某位不知名的董老师加工后变为中日并行数据集,下面命名和en(English)有关的实际为cn(Chinese)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">&#x27;./zh-ja/zh-ja.bicleaner05.txt&#x27;</span></span><br><span class=\"line\">, sep=<span class=\"string\">&#x27;\\\\t&#x27;</span>, engine=<span class=\"string\">&#x27;python&#x27;</span>, header=<span class=\"literal\">None</span>)</span><br><span class=\"line\">trainen = df[<span class=\"number\">2</span>].values.tolist()<span class=\"comment\">#[:10000]</span></span><br><span class=\"line\">trainja = df[<span class=\"number\">3</span>].values.tolist()<span class=\"comment\">#[:10000]</span></span><br><span class=\"line\"><span class=\"comment\"># trainen.pop(5972)</span></span><br><span class=\"line\"><span class=\"comment\"># trainja.pop(5972)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(traincn[<span class=\"number\">500</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(trainja[<span class=\"number\">500</span>])</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab14-1.png?raw=true\" alt=\"alt text\"><br>我们还可以使用不同的并行数据集来遵循本文，只需确保我们可以将数据处理成两个字符串列表，如上所示，包含日语和英语句子。</p>\n<h2 id=\"准备分词器\"><a href=\"#准备分词器\" class=\"headerlink\" title=\"准备分词器\"></a>准备分词器</h2><p>与英语或其他字母语言不同，汉语和日语句子不包含空格来分隔单词。我们可以使用JParaCrawl提供的分词器，该分词器是使用SentencePiece创建的来切分汉语和日语句子</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">en_tokenizer = spm.SentencePieceProcessor(model_file=<span class=\"string\">&#x27;enja_spm_models/spm.en.nopretok.model&#x27;</span>)</span><br><span class=\"line\">ja_tokenizer = spm.SentencePieceProcessor(model_file=<span class=\"string\">&#x27;enja_spm_models/spm.ja.nopretok.model&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>使用它们来切分句子</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">en_tokenizer.encode(<span class=\"string\">&quot;All residents aged 20 to 59 years who live in Japan must enroll in public pension system.&quot;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab14-2.png?raw=true\" alt=\"alt text\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">ja_tokenizer.encode(<span class=\"string\">&quot;年金 日本に住んでいる20歳~60歳の全ての人は、公的年金制度に加入しなければなりません。&quot;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab14-4.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"构建-TorchText-Vocab-对象并将句子转换为-Torch-张量\"><a href=\"#构建-TorchText-Vocab-对象并将句子转换为-Torch-张量\" class=\"headerlink\" title=\"构建 TorchText Vocab 对象并将句子转换为 Torch 张量\"></a>构建 TorchText Vocab 对象并将句子转换为 Torch 张量</h2><p>使用分词器和原始句子，我们构建从 TorchText 导入的 Vocab 对象。此过程可能需要几秒钟或几分钟，具体取决于我们的数据集大小和计算能力。不同的分词器也会影响构建词汇所需的时间。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">build_vocab</span>(<span class=\"params\">sentences, tokenizer</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    构建词汇表函数</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">    - sentences (list): 句子列表，每个句子是一个字符串</span></span><br><span class=\"line\"><span class=\"string\">    - tokenizer (SentencePieceProcessor): 分词器对象，用于对句子进行分词和编码</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    - Vocab: 构建好的词汇表对象</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    counter = Counter()  <span class=\"comment\"># 创建计数器，用于统计词频</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> sentence <span class=\"keyword\">in</span> sentences:</span><br><span class=\"line\">        <span class=\"comment\"># 使用 tokenizer 对每个句子进行编码，并以字符串形式返回结果</span></span><br><span class=\"line\">        encoded_tokens = tokenizer.encode(sentence, out_type=<span class=\"built_in\">str</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 更新计数器，统计每个子词的出现次数</span></span><br><span class=\"line\">        counter.update(encoded_tokens)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 创建词汇表对象，传入计数器和特殊标记</span></span><br><span class=\"line\">    vocab = Vocab(counter, specials=[<span class=\"string\">&#x27;&lt;unk&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;pad&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;bos&gt;&#x27;</span>, <span class=\"string\">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> vocab</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用 build_vocab 函数构建日文和英文的词汇表</span></span><br><span class=\"line\">ja_vocab = build_vocab(trainja, ja_tokenizer)  <span class=\"comment\"># 构建日文词汇表</span></span><br><span class=\"line\">en_vocab = build_vocab(trainen, en_tokenizer)  <span class=\"comment\"># 构建英文词汇表</span></span><br></pre></td></tr></table></figure>\n<p>在有了词汇表对象之后，我们可以使用词汇表和分词器对象来构建训练数据的张量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">data_process</span>(<span class=\"params\">ja, en</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    数据处理函数，将日文和英文句子转换为张量形式</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">    - ja (list): 日文句子列表</span></span><br><span class=\"line\"><span class=\"string\">    - en (list): 英文句子列表</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    - list: 包含日文和英文句子张量对的列表</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    data = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (raw_ja, raw_en) <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(ja, en):</span><br><span class=\"line\">        <span class=\"comment\"># 使用 ja_vocab 将日文句子转换为张量形式</span></span><br><span class=\"line\">        ja_tensor_ = torch.tensor([ja_vocab[token] <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> ja_tokenizer.encode(raw_ja.rstrip(<span class=\"string\">&quot;\\n&quot;</span>), out_type=<span class=\"built_in\">str</span>)],</span><br><span class=\"line\">                                  dtype=torch.long)</span><br><span class=\"line\">        <span class=\"comment\"># 使用 en_vocab 将英文句子转换为张量形式</span></span><br><span class=\"line\">        en_tensor_ = torch.tensor([en_vocab[token] <span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> en_tokenizer.encode(raw_en.rstrip(<span class=\"string\">&quot;\\n&quot;</span>), out_type=<span class=\"built_in\">str</span>)],</span><br><span class=\"line\">                                  dtype=torch.long)</span><br><span class=\"line\">        <span class=\"comment\"># 将处理后的日文和英文句子张量对添加到 data 列表中</span></span><br><span class=\"line\">        data.append((ja_tensor_, en_tensor_))</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> data</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用 data_process 函数将训练数据集 trainja 和 trainen 转换为张量形式的训练数据</span></span><br><span class=\"line\">train_data = data_process(trainja, trainen)</span><br></pre></td></tr></table></figure>\n<h2 id=\"创建要在训练期间迭代的-DataLoader-对象\"><a href=\"#创建要在训练期间迭代的-DataLoader-对象\" class=\"headerlink\" title=\"创建要在训练期间迭代的 DataLoader 对象\"></a>创建要在训练期间迭代的 DataLoader 对象</h2><p>在这里，我将BATCH_SIZE设置为 16 以防止“cuda out of memory”，但这取决于各种因素，例如您的机器内存容量、数据大小等，因此请根据需要随意更改批处理大小（注意：PyTorch 的教程使用 Multi30k 德语-英语数据集将批处理大小设置为 128。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">PAD_IDX = ja_vocab[&#x27;&lt;pad&gt;&#x27;]  # 填充标记的索引</span><br><span class=\"line\">BOS_IDX = ja_vocab[&#x27;&lt;bos&gt;&#x27;]  # 句子开始标记的索引</span><br><span class=\"line\">EOS_IDX = ja_vocab[&#x27;&lt;eos&gt;&#x27;]  # 句子结束标记的索引</span><br><span class=\"line\"></span><br><span class=\"line\">def generate_batch(data_batch):</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    生成批量数据函数</span><br><span class=\"line\"></span><br><span class=\"line\">    Args:</span><br><span class=\"line\">    - data_batch (list): 包含日文和英文句子张量对的列表，每个元素为 (ja_item, en_item)</span><br><span class=\"line\"></span><br><span class=\"line\">    Returns:</span><br><span class=\"line\">    - ja_batch (Tensor): 日文句子批量张量，形状为 (max_seq_len_ja, BATCH_SIZE)</span><br><span class=\"line\">    - en_batch (Tensor): 英文句子批量张量，形状为 (max_seq_len_en, BATCH_SIZE)</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    ja_batch, en_batch = [], []  # 初始化日文和英文批量张量列表</span><br><span class=\"line\">    for (ja_item, en_item) in data_batch:</span><br><span class=\"line\">        # 在日文句子的开头和结尾添加 &lt;bos&gt; 和 &lt;eos&gt; 标记</span><br><span class=\"line\">        ja_batch.append(torch.cat([torch.tensor([BOS_IDX]), ja_item, torch.tensor([EOS_IDX])], dim=0))</span><br><span class=\"line\">        # 在英文句子的开头和结尾添加 &lt;bos&gt; 和 &lt;eos&gt; 标记</span><br><span class=\"line\">        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 对日文和英文句子批量进行填充，使它们的长度一致</span><br><span class=\"line\">    ja_batch = pad_sequence(ja_batch, padding_value=PAD_IDX)</span><br><span class=\"line\">    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)</span><br><span class=\"line\">    </span><br><span class=\"line\">    return ja_batch, en_batch</span><br><span class=\"line\"></span><br><span class=\"line\"># 使用 DataLoader 加载训练数据集 train_data，并生成批量数据</span><br><span class=\"line\">train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,</span><br><span class=\"line\">                        shuffle=True, collate_fn=generate_batch)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Seq2seq-Transformer\"><a href=\"#Seq2seq-Transformer\" class=\"headerlink\" title=\"Seq2seq Transformer\"></a>Seq2seq Transformer</h2><p>接下来的几个代码和文本说明（用斜体书写）取自<a href=\"https://pytorch.org/tutorials/beginner/translation_transformer.html\">原始的 PyTorch 教程</a>. 除了BATCH_SIZE之外，我没有做任何更改，de_vocabwhich 这个词被改成了ja_vocab。</p>\n<p>Transformer 是 “Attention is all you need” 论文中介绍的 Seq2Seq 模型，用于解决机器翻译任务。Transformer 模型由编码器和解码器块组成，每个块包含固定数量的层。</p>\n<p>编码器通过一系列多头注意力和前馈网络层传播输入序列来处理输入序列。编码器的输出称为内存，与目标张量一起馈送到解码器。编码器和解码器使用教师强制技术以端到端的方式进行训练。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Seq2SeqTransformer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_encoder_layers: <span class=\"built_in\">int</span>, num_decoder_layers: <span class=\"built_in\">int</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 emb_size: <span class=\"built_in\">int</span>, src_vocab_size: <span class=\"built_in\">int</span>, tgt_vocab_size: <span class=\"built_in\">int</span>,</span></span><br><span class=\"line\"><span class=\"params\">                 dim_feedforward:<span class=\"built_in\">int</span> = <span class=\"number\">512</span>, dropout:<span class=\"built_in\">float</span> = <span class=\"number\">0.1</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Seq2SeqTransformer, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Transformer编码器层</span></span><br><span class=\"line\">        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,</span><br><span class=\"line\">                                                dim_feedforward=dim_feedforward)</span><br><span class=\"line\">        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Transformer解码器层</span></span><br><span class=\"line\">        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,</span><br><span class=\"line\">                                                dim_feedforward=dim_feedforward)</span><br><span class=\"line\">        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 线性层，用于生成最终的目标词汇表输出</span></span><br><span class=\"line\">        self.generator = nn.Linear(emb_size, tgt_vocab_size)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 源语言和目标语言的词嵌入层</span></span><br><span class=\"line\">        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)  <span class=\"comment\"># TokenEmbedding是自定义的词嵌入类</span></span><br><span class=\"line\">        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 位置编码层</span></span><br><span class=\"line\">        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)  <span class=\"comment\"># PositionalEncoding是自定义的位置编码类</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, src: Tensor, trg: Tensor, src_mask: Tensor,</span></span><br><span class=\"line\"><span class=\"params\">                tgt_mask: Tensor, src_padding_mask: Tensor,</span></span><br><span class=\"line\"><span class=\"params\">                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 对源语言和目标语言进行位置编码</span></span><br><span class=\"line\">        src_emb = self.positional_encoding(self.src_tok_emb(src))</span><br><span class=\"line\">        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 编码阶段：通过Transformer编码器得到记忆(memory)</span></span><br><span class=\"line\">        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 解码阶段：通过Transformer解码器生成目标语言的输出</span></span><br><span class=\"line\">        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, <span class=\"literal\">None</span>,</span><br><span class=\"line\">                                        tgt_padding_mask, memory_key_padding_mask)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 使用线性层生成最终的目标词汇表输出</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.generator(outs)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">encode</span>(<span class=\"params\">self, src: Tensor, src_mask: Tensor</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 编码阶段：仅使用Transformer编码器对源语言进行编码，不涉及解码</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.transformer_encoder(self.positional_encoding(</span><br><span class=\"line\">                            self.src_tok_emb(src)), src_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">decode</span>(<span class=\"params\">self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 解码阶段：使用Transformer解码器生成目标语言的输出</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.transformer_decoder(self.positional_encoding(</span><br><span class=\"line\">                          self.tgt_tok_emb(tgt)), memory,</span><br><span class=\"line\">                          tgt_mask)</span><br></pre></td></tr></table></figure>\n<p>文本标记通过使用标记嵌入来表示。位置编码被添加到标记嵌入中，以引入词序的概念。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PositionalEncoding</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, emb_size: <span class=\"built_in\">int</span>, dropout, maxlen: <span class=\"built_in\">int</span> = <span class=\"number\">5000</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(PositionalEncoding, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 计算位置编码矩阵</span></span><br><span class=\"line\">        den = torch.exp(- torch.arange(<span class=\"number\">0</span>, emb_size, <span class=\"number\">2</span>) * math.log(<span class=\"number\">10000</span>) / emb_size)  <span class=\"comment\"># 计算分母</span></span><br><span class=\"line\">        pos = torch.arange(<span class=\"number\">0</span>, maxlen).reshape(maxlen, <span class=\"number\">1</span>)  <span class=\"comment\"># 生成位置编码的位置索引</span></span><br><span class=\"line\">        pos_embedding = torch.zeros((maxlen, emb_size))  <span class=\"comment\"># 初始化位置编码矩阵</span></span><br><span class=\"line\">        pos_embedding[:, <span class=\"number\">0</span>::<span class=\"number\">2</span>] = torch.sin(pos * den)  <span class=\"comment\"># 奇数维度上使用sin函数计算位置编码</span></span><br><span class=\"line\">        pos_embedding[:, <span class=\"number\">1</span>::<span class=\"number\">2</span>] = torch.cos(pos * den)  <span class=\"comment\"># 偶数维度上使用cos函数计算位置编码</span></span><br><span class=\"line\">        pos_embedding = pos_embedding.unsqueeze(-<span class=\"number\">2</span>)  <span class=\"comment\"># 在倒数第二维度上添加一个维度</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.dropout = nn.Dropout(dropout)  <span class=\"comment\"># 初始化Dropout层</span></span><br><span class=\"line\">        self.register_buffer(<span class=\"string\">&#x27;pos_embedding&#x27;</span>, pos_embedding)  <span class=\"comment\"># 注册位置编码矩阵为模型参数</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, token_embedding: Tensor</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播函数</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        Args:</span></span><br><span class=\"line\"><span class=\"string\">        - token_embedding (Tensor): 输入的词嵌入张量，形状为 (seq_len, batch_size, emb_size)。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">        - Tensor: 添加了位置编码后的张量，形状与输入相同。</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># 添加位置编码到词嵌入张量上，并应用Dropout</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.dropout(token_embedding +</span><br><span class=\"line\">                            self.pos_embedding[:token_embedding.size(<span class=\"number\">0</span>), :])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TokenEmbedding</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, vocab_size: <span class=\"built_in\">int</span>, emb_size</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(TokenEmbedding, self).__init__()</span><br><span class=\"line\">        self.embedding = nn.Embedding(vocab_size, emb_size)  <span class=\"comment\"># 定义词嵌入层</span></span><br><span class=\"line\">        self.emb_size = emb_size  <span class=\"comment\"># 记录词嵌入维度大小</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, tokens: Tensor</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播函数</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        Args:</span></span><br><span class=\"line\"><span class=\"string\">        - tokens (Tensor): 输入的词索引张量，形状为 (seq_len, batch_size)。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">        - Tensor: 对应的词嵌入张量，形状为 (seq_len, batch_size, emb_size)。</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># 将词索引转换为词嵌入张量，并乘以 sqrt(emb_size) 进行缩放</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.embedding(tokens.long()) * math.sqrt(self.emb_size)</span><br></pre></td></tr></table></figure>\n<p>我们创建一个后续单词掩码来阻止目标单词关注其后续单词。我们还创建掩码，用于屏蔽源和目标填充令牌</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">generate_square_subsequent_mask</span>(<span class=\"params\">sz</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    生成一个下三角形的mask矩阵，用于Transformer解码器中屏蔽未来信息。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">    - sz (int): 矩阵的大小，即序列长度。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    - torch.Tensor: 生成的mask矩阵，形状为 (sz, sz)。</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    mask = (torch.triu(torch.ones((sz, sz), device=device)) == <span class=\"number\">1</span>).transpose(<span class=\"number\">0</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># 生成下三角形矩阵</span></span><br><span class=\"line\">    mask = mask.<span class=\"built_in\">float</span>().masked_fill(mask == <span class=\"number\">0</span>, <span class=\"built_in\">float</span>(<span class=\"string\">&#x27;-inf&#x27;</span>)).masked_fill(mask == <span class=\"number\">1</span>, <span class=\"built_in\">float</span>(<span class=\"number\">0.0</span>))  <span class=\"comment\"># 将非零位置替换为-inf，零位置替换为0.0</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">create_mask</span>(<span class=\"params\">src, tgt</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    创建用于Transformer模型的掩码张量，包括源语言和目标语言的填充掩码。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">    - src (torch.Tensor): 源语言张量，形状为 (seq_len_src, batch_size)。</span></span><br><span class=\"line\"><span class=\"string\">    - tgt (torch.Tensor): 目标语言张量，形状为 (seq_len_tgt, batch_size)。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    - Tuple[torch.Tensor]: 包含四个掩码张量的元组：</span></span><br><span class=\"line\"><span class=\"string\">      - src_mask (torch.Tensor): 源语言掩码张量，形状为 (seq_len_src, seq_len_src)。</span></span><br><span class=\"line\"><span class=\"string\">      - tgt_mask (torch.Tensor): 目标语言解码器掩码张量，形状为 (seq_len_tgt, seq_len_tgt)。</span></span><br><span class=\"line\"><span class=\"string\">      - src_padding_mask (torch.Tensor): 源语言填充掩码张量，形状为 (batch_size, seq_len_src)。</span></span><br><span class=\"line\"><span class=\"string\">      - tgt_padding_mask (torch.Tensor): 目标语言填充掩码张量，形状为 (batch_size, seq_len_tgt)。</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    src_seq_len = src.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 获取源语言序列长度</span></span><br><span class=\"line\">    tgt_seq_len = tgt.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 获取目标语言序列长度</span></span><br><span class=\"line\"></span><br><span class=\"line\">    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)  <span class=\"comment\"># 生成目标语言解码器掩码张量</span></span><br><span class=\"line\">    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).<span class=\"built_in\">type</span>(torch.<span class=\"built_in\">bool</span>)  <span class=\"comment\"># 创建源语言掩码张量，全为False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    src_padding_mask = (src == PAD_IDX).transpose(<span class=\"number\">0</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># 创建源语言填充掩码张量，True表示填充位置</span></span><br><span class=\"line\">    tgt_padding_mask = (tgt == PAD_IDX).transpose(<span class=\"number\">0</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># 创建目标语言填充掩码张量，True表示填充位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> src_mask, tgt_mask, src_padding_mask, tgt_padding_mask</span><br></pre></td></tr></table></figure>\n<p>当你使用自己的GPU的时候，NUM_ENCODER_LAYERS 和 NUM_DECODER_LAYERS 设置为3或者更高，NHEAD设置8，EMB_SIZE设置为512。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">SRC_VOCAB_SIZE = <span class=\"built_in\">len</span>(ja_vocab)  <span class=\"comment\"># 源语言词汇表大小</span></span><br><span class=\"line\">TGT_VOCAB_SIZE = <span class=\"built_in\">len</span>(en_vocab)  <span class=\"comment\"># 目标语言词汇表大小</span></span><br><span class=\"line\">EMB_SIZE = <span class=\"number\">512</span>  <span class=\"comment\"># 词嵌入维度大小</span></span><br><span class=\"line\">NHEAD = <span class=\"number\">8</span>  <span class=\"comment\"># 注意力头数</span></span><br><span class=\"line\">FFN_HID_DIM = <span class=\"number\">512</span>  <span class=\"comment\"># FeedForward层隐藏单元数</span></span><br><span class=\"line\">BATCH_SIZE = <span class=\"number\">16</span>  <span class=\"comment\"># 批量大小</span></span><br><span class=\"line\">NUM_ENCODER_LAYERS = <span class=\"number\">3</span>  <span class=\"comment\"># 编码器层数</span></span><br><span class=\"line\">NUM_DECODER_LAYERS = <span class=\"number\">3</span>  <span class=\"comment\"># 解码器层数</span></span><br><span class=\"line\">NUM_EPOCHS = <span class=\"number\">16</span>  <span class=\"comment\"># 训练轮数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化Transformer模型</span></span><br><span class=\"line\">transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,</span><br><span class=\"line\">                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,</span><br><span class=\"line\">                                 FFN_HID_DIM)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用Xavier初始化所有模型参数</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> transformer.parameters():</span><br><span class=\"line\">    <span class=\"keyword\">if</span> p.dim() &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">        nn.init.xavier_uniform_(p)</span><br><span class=\"line\"></span><br><span class=\"line\">transformer = transformer.to(device)  <span class=\"comment\"># 将模型移动到GPU上（如果可用）</span></span><br><span class=\"line\"></span><br><span class=\"line\">loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)  <span class=\"comment\"># 定义交叉熵损失函数，忽略填充位置</span></span><br><span class=\"line\"></span><br><span class=\"line\">optimizer = torch.optim.Adam(</span><br><span class=\"line\">    transformer.parameters(), lr=<span class=\"number\">0.0001</span>, betas=(<span class=\"number\">0.9</span>, <span class=\"number\">0.98</span>), eps=<span class=\"number\">1e-9</span></span><br><span class=\"line\">)  <span class=\"comment\"># 定义Adam优化器</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch</span>(<span class=\"params\">model, train_iter, optimizer</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    训练模型一个epoch，并返回平均损失值。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">    - model (Seq2SeqTransformer): Transformer模型实例。</span></span><br><span class=\"line\"><span class=\"string\">    - train_iter (DataLoader): 训练数据迭代器。</span></span><br><span class=\"line\"><span class=\"string\">    - optimizer (torch.optim.Adam): 模型优化器。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    - float: 平均训练损失。</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    model.train()  <span class=\"comment\"># 设置模型为训练模式</span></span><br><span class=\"line\">    losses = <span class=\"number\">0</span>  <span class=\"comment\"># 初始化损失值</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx, (src, tgt) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(train_iter):</span><br><span class=\"line\">        src = src.to(device)  <span class=\"comment\"># 将源语言数据移到GPU（如果可用）</span></span><br><span class=\"line\">        tgt = tgt.to(device)  <span class=\"comment\"># 将目标语言数据移到GPU（如果可用）</span></span><br><span class=\"line\"></span><br><span class=\"line\">        tgt_input = tgt[:-<span class=\"number\">1</span>, :]  <span class=\"comment\"># 获取目标语言输入序列（不包括末尾的EOS）</span></span><br><span class=\"line\"></span><br><span class=\"line\">        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)  <span class=\"comment\"># 创建掩码张量</span></span><br><span class=\"line\"></span><br><span class=\"line\">        logits = model(src, tgt_input, src_mask, tgt_mask,</span><br><span class=\"line\">                       src_padding_mask, tgt_padding_mask, src_padding_mask)  <span class=\"comment\"># 前向传播计算logits</span></span><br><span class=\"line\"></span><br><span class=\"line\">        optimizer.zero_grad()  <span class=\"comment\"># 梯度清零</span></span><br><span class=\"line\"></span><br><span class=\"line\">        tgt_out = tgt[<span class=\"number\">1</span>:, :]  <span class=\"comment\"># 获取目标语言输出序列（不包括开头的BOS）</span></span><br><span class=\"line\">        loss = loss_fn(logits.reshape(-<span class=\"number\">1</span>, logits.shape[-<span class=\"number\">1</span>]), tgt_out.reshape(-<span class=\"number\">1</span>))  <span class=\"comment\"># 计算损失</span></span><br><span class=\"line\">        loss.backward()  <span class=\"comment\"># 反向传播</span></span><br><span class=\"line\"></span><br><span class=\"line\">        optimizer.step()  <span class=\"comment\"># 更新模型参数</span></span><br><span class=\"line\">        losses += loss.item()  <span class=\"comment\"># 累加损失值</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> losses / <span class=\"built_in\">len</span>(train_iter)  <span class=\"comment\"># 返回平均损失</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate</span>(<span class=\"params\">model, val_iter</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    评估模型在验证集上的性能，并返回平均损失值。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">    - model (Seq2SeqTransformer): Transformer模型实例。</span></span><br><span class=\"line\"><span class=\"string\">    - val_iter (DataLoader): 验证数据迭代器。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    - float: 平均验证损失。</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    model.<span class=\"built_in\">eval</span>()  <span class=\"comment\"># 设置模型为评估模式</span></span><br><span class=\"line\">    losses = <span class=\"number\">0</span>  <span class=\"comment\"># 初始化损失值</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx, (src, tgt) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(valid_iter):</span><br><span class=\"line\">        src = src.to(device)  <span class=\"comment\"># 将源语言数据移到GPU（如果可用）</span></span><br><span class=\"line\">        tgt = tgt.to(device)  <span class=\"comment\"># 将目标语言数据移到GPU（如果可用）</span></span><br><span class=\"line\"></span><br><span class=\"line\">        tgt_input = tgt[:-<span class=\"number\">1</span>, :]  <span class=\"comment\"># 获取目标语言输入序列（不包括末尾的EOS）</span></span><br><span class=\"line\"></span><br><span class=\"line\">        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)  <span class=\"comment\"># 创建掩码张量</span></span><br><span class=\"line\"></span><br><span class=\"line\">        logits = model(src, tgt_input, src_mask, tgt_mask,</span><br><span class=\"line\">                       src_padding_mask, tgt_padding_mask, src_padding_mask)  <span class=\"comment\"># 前向传播计算logits</span></span><br><span class=\"line\"></span><br><span class=\"line\">        tgt_out = tgt[<span class=\"number\">1</span>:, :]  <span class=\"comment\"># 获取目标语言输出序列（不包括开头的BOS）</span></span><br><span class=\"line\">        loss = loss_fn(logits.reshape(-<span class=\"number\">1</span>, logits.shape[-<span class=\"number\">1</span>]), tgt_out.reshape(-<span class=\"number\">1</span>))  <span class=\"comment\"># 计算损失</span></span><br><span class=\"line\">        losses += loss.item()  <span class=\"comment\"># 累加损失值</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> losses / <span class=\"built_in\">len</span>(val_iter)  <span class=\"comment\"># 返回平均损失</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"开始训练\"><a href=\"#开始训练\" class=\"headerlink\" title=\"开始训练\"></a>开始训练</h2><p>最后，在准备了必要的类和函数之后，我们准备训练我们的模型。这是不言而喻的，但完成训练所需的时间可能会有很大差异，具体取决于很多因素，例如计算能力、参数和数据集的大小。</p>\n<p>当我使用 JParaCrawl 的完整句子列表（每种语言大约有 590 万个句子）训练模型时，使用单个 NVIDIA GeForce RTX 4090 GPU 每16个 epoch 大约需要 55分钟。</p>\n<p>代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> tqdm.tqdm(<span class=\"built_in\">range</span>(<span class=\"number\">1</span>, NUM_EPOCHS+<span class=\"number\">1</span>)):</span><br><span class=\"line\">    start_time = time.time()  <span class=\"comment\"># 记录当前epoch开始时间</span></span><br><span class=\"line\">    train_loss = train_epoch(transformer, train_iter, optimizer)  <span class=\"comment\"># 训练一个epoch</span></span><br><span class=\"line\">    end_time = time.time()  <span class=\"comment\"># 记录当前epoch结束时间</span></span><br><span class=\"line\">    <span class=\"comment\"># 打印训练信息：当前epoch数、训练损失、当前epoch所用时间</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>((<span class=\"string\">f&quot;Epoch: <span class=\"subst\">&#123;epoch&#125;</span>, Train loss: <span class=\"subst\">&#123;train_loss:<span class=\"number\">.3</span>f&#125;</span>, &quot;</span></span><br><span class=\"line\">           <span class=\"string\">f&quot;Epoch time = <span class=\"subst\">&#123;(end_time - start_time):<span class=\"number\">.3</span>f&#125;</span>s&quot;</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab14-5.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"尝试使用经过训练的模型翻译日语句子\"><a href=\"#尝试使用经过训练的模型翻译日语句子\" class=\"headerlink\" title=\"尝试使用经过训练的模型翻译日语句子\"></a>尝试使用经过训练的模型翻译日语句子</h2><p>首先，我们创建翻译新句子的函数，包括获取日语句子、标记化、转换为张量、推理等步骤，然后将结果解码回句子，但这次是中文。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">greedy_decode</span>(<span class=\"params\">model, src, src_mask, max_len, start_symbol</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    使用贪婪解码策略生成翻译结果。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">    - model (Seq2SeqTransformer): Transformer模型实例。</span></span><br><span class=\"line\"><span class=\"string\">    - src (Tensor): 输入源语言序列张量。</span></span><br><span class=\"line\"><span class=\"string\">    - src_mask (Tensor): 输入源语言掩码张量。</span></span><br><span class=\"line\"><span class=\"string\">    - max_len (int): 生成的最大长度。</span></span><br><span class=\"line\"><span class=\"string\">    - start_symbol (int): 目标语言起始符号索引。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    - ys (Tensor): 生成的目标语言序列张量。</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    src = src.to(device)</span><br><span class=\"line\">    src_mask = src_mask.to(device)</span><br><span class=\"line\">    memory = model.encode(src, src_mask)  <span class=\"comment\"># 编码源语言序列</span></span><br><span class=\"line\">    ys = torch.ones(<span class=\"number\">1</span>, <span class=\"number\">1</span>).fill_(start_symbol).<span class=\"built_in\">type</span>(torch.long).to(device)  <span class=\"comment\"># 初始化目标语言序列起始符号</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_len-<span class=\"number\">1</span>):</span><br><span class=\"line\">        memory = memory.to(device)</span><br><span class=\"line\">        memory_mask = torch.zeros(ys.shape[<span class=\"number\">0</span>], memory.shape[<span class=\"number\">0</span>]).to(device).<span class=\"built_in\">type</span>(torch.<span class=\"built_in\">bool</span>)</span><br><span class=\"line\">        tgt_mask = (generate_square_subsequent_mask(ys.size(<span class=\"number\">0</span>))</span><br><span class=\"line\">                    .<span class=\"built_in\">type</span>(torch.<span class=\"built_in\">bool</span>)).to(device)</span><br><span class=\"line\">        out = model.decode(ys, memory, tgt_mask)  <span class=\"comment\"># 解码生成下一个目标语言单词概率分布</span></span><br><span class=\"line\">        out = out.transpose(<span class=\"number\">0</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        prob = model.generator(out[:, -<span class=\"number\">1</span>])  <span class=\"comment\"># 生成下一个目标语言单词的概率分布</span></span><br><span class=\"line\">        _, next_word = torch.<span class=\"built_in\">max</span>(prob, dim=<span class=\"number\">1</span>)  <span class=\"comment\"># 获取概率最大的单词索引</span></span><br><span class=\"line\">        next_word = next_word.item()  <span class=\"comment\"># 获取单词索引值</span></span><br><span class=\"line\">        ys = torch.cat([ys,</span><br><span class=\"line\">                        torch.ones(<span class=\"number\">1</span>, <span class=\"number\">1</span>).type_as(src.data).fill_(next_word)], dim=<span class=\"number\">0</span>)  <span class=\"comment\"># 将预测的单词添加到目标语言序列中</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> next_word == EOS_IDX:</span><br><span class=\"line\">            <span class=\"keyword\">break</span>  <span class=\"comment\"># 如果预测的单词为结束符号，则停止生成</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> ys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">translate</span>(<span class=\"params\">model, src, src_vocab, tgt_vocab, src_tokenizer</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    对源语言文本进行翻译，并返回翻译结果。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">    - model (Seq2SeqTransformer): Transformer模型实例。</span></span><br><span class=\"line\"><span class=\"string\">    - src (str): 源语言文本。</span></span><br><span class=\"line\"><span class=\"string\">    - src_vocab (Vocab): 源语言词汇表。</span></span><br><span class=\"line\"><span class=\"string\">    - tgt_vocab (Vocab): 目标语言词汇表。</span></span><br><span class=\"line\"><span class=\"string\">    - src_tokenizer (spm.SentencePieceProcessor): 源语言分词器。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    - str: 翻译后的目标语言文本。</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    model.<span class=\"built_in\">eval</span>()  <span class=\"comment\"># 设置模型为评估模式</span></span><br><span class=\"line\">    tokens = [BOS_IDX] + [src_vocab.stoi[tok] <span class=\"keyword\">for</span> tok <span class=\"keyword\">in</span> src_tokenizer.encode(src, out_type=<span class=\"built_in\">str</span>)] + [EOS_IDX]  <span class=\"comment\"># 将源语言文本编码为索引序列</span></span><br><span class=\"line\">    num_tokens = <span class=\"built_in\">len</span>(tokens)</span><br><span class=\"line\">    src = torch.LongTensor(tokens).reshape(num_tokens, <span class=\"number\">1</span>)  <span class=\"comment\"># 将索引序列转换为Tensor，并reshape为(num_tokens, 1)</span></span><br><span class=\"line\">    src_mask = (torch.zeros(num_tokens, num_tokens)).<span class=\"built_in\">type</span>(torch.<span class=\"built_in\">bool</span>)  <span class=\"comment\"># 生成源语言掩码张量</span></span><br><span class=\"line\">    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens + <span class=\"number\">5</span>, start_symbol=BOS_IDX).flatten()  <span class=\"comment\"># 使用贪婪解码生成目标语言索引序列</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&quot; &quot;</span>.join([tgt_vocab.itos[tok] <span class=\"keyword\">for</span> tok <span class=\"keyword\">in</span> tgt_tokens]).replace(<span class=\"string\">&quot;&lt;bos&gt;&quot;</span>, <span class=\"string\">&quot;&quot;</span>).replace(<span class=\"string\">&quot;&lt;eos&gt;&quot;</span>, <span class=\"string\">&quot;&quot;</span>)  <span class=\"comment\"># 将生成的目标语言索引序列转换为文本并返回</span></span><br></pre></td></tr></table></figure>\n<p>然后，我们可以调用 translate 函数并传递所需的参数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(translate(transformer, <span class=\"string\">&quot;ライトガイドワイヤーおよびライトガイドワイヤー；&quot;</span>, ja_vocab, en_vocab, ja_tokenizer))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(traincn.pop(<span class=\"number\">5</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(trainja.pop(<span class=\"number\">5</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab14-6.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"保存-Vocab-对象和训练的模型\"><a href=\"#保存-Vocab-对象和训练的模型\" class=\"headerlink\" title=\"保存 Vocab 对象和训练的模型\"></a>保存 Vocab 对象和训练的模型</h2><p>最后，在训练完成后，我们将首先使用 Pickle 保存 Vocab 对象（en_vocab 和 ja_vocab）。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"comment\"># open a file, where you want to store the data</span></span><br><span class=\"line\">file = <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;en_vocab.pkl&#x27;</span>, <span class=\"string\">&#x27;wb&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># dump information to that file</span></span><br><span class=\"line\">pickle.dump(en_vocab, file)</span><br><span class=\"line\">file.close()</span><br><span class=\"line\">file = <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;ja_vocab.pkl&#x27;</span>, <span class=\"string\">&#x27;wb&#x27;</span>)</span><br><span class=\"line\">pickle.dump(ja_vocab, file)</span><br><span class=\"line\">file.close()</span><br></pre></td></tr></table></figure>\n<p>最后，我们还可以使用 PyTorch save 和 load 函数保存模型以供以后使用。通常，有两种方法可以保存模型，具体取决于我们以后要使用它们的内容。第一个仅用于推理，我们可以稍后加载模型并使用它从日语翻译成英语。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># save model for inference</span></span><br><span class=\"line\">torch.save(transformer.state_dict(), <span class=\"string\">&#x27;inference_model&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>第二个既可以用于推理，也可以用于我们稍后想要加载模型并想要恢复训练时。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># save model + checkpoint to resume training later</span></span><br><span class=\"line\">torch.save(&#123;</span><br><span class=\"line\">  <span class=\"string\">&#x27;epoch&#x27;</span>: NUM_EPOCHS,</span><br><span class=\"line\">  <span class=\"string\">&#x27;model_state_dict&#x27;</span>: transformer.state_dict(),</span><br><span class=\"line\">  <span class=\"string\">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class=\"line\">  <span class=\"string\">&#x27;loss&#x27;</span>: train_loss,</span><br><span class=\"line\">  &#125;, <span class=\"string\">&#x27;model_checkpoint.tar&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"加载推理模型\"><a href=\"#加载推理模型\" class=\"headerlink\" title=\"加载推理模型\"></a>加载推理模型</h2><p>我们载入已经保存的模型，让它翻译日语句子</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;en_vocab.pkl&#x27;</span>, <span class=\"string\">&#x27;rb&#x27;</span>) <span class=\"keyword\">as</span> file:</span><br><span class=\"line\">    en_vocab = pickle.load(file)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;ja_vocab.pkl&#x27;</span>, <span class=\"string\">&#x27;rb&#x27;</span>) <span class=\"keyword\">as</span> file:</span><br><span class=\"line\">    ja_vocab = pickle.load(file)</span><br><span class=\"line\">transformer.load_state_dict(torch.load(<span class=\"string\">&#x27;inference_model&#x27;</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(translate(transformer, </span><br><span class=\"line\"><span class=\"string\">&quot;ライトガイドワイヤーおよびライガイドワイヤー；&quot;</span>, </span><br><span class=\"line\">ja_vocab, en_vocab, ja_tokenizer))</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/lab14-7.png?raw=true\" alt=\"alt text\"></p>\n","categories":["Deep Learning"]},{"title":"基于前馈神经网络的姓氏分类","url":"/2024/06/08/lab4/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>感知机（英语：Perceptron）是Frank Rosenblatt在1957年就职于康奈尔航空实验室（Cornell Aeronautical Laboratory）时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。</p>\n<p>Frank Rosenblatt给出了相应的感知机学习算法，常用的有感知机学习、最小二乘法和梯度下降法。譬如，感知机利用梯度下降法对损失函数进行极小化，求出可将训练数据进行线性划分的分离超平面，从而求得感知机模型。<span id=\"more\"></span></p>\n<p>感知机是生物神经细胞的简单抽象。神经细胞结构大致可分为：树突、突触、细胞体及轴突。单个神经细胞可被视为一种只有两种状态的机器——激活时为‘是’，而未激活时为‘否’。神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激活，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。</p>\n<p>在人工神经网络领域中，感知机也被指为单层的人工神经网络，以区别于较复杂的多层感知机（Multilayer Perceptron）。作为一种线性分类器，（单层）感知机可说是最简单的前向人工神经网络形式。尽管结构简单，感知机能够学习并解决相当复杂的问题。感知机主要的本质缺陷是它不能处理线性不可分问题。</p>\n<h2 id=\"单层感知机\"><a href=\"#单层感知机\" class=\"headerlink\" title=\"单层感知机\"></a>单层感知机</h2><h3 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h3><p>单层感知机的模型可以简单表示为：<br>$$ f(x) &#x3D; sign(w*x+b) $$<br>对于具有 $n$ 个输入 $x_{i}$ 以及对应连接权重系数为 $w_j$ 的感知机，首先通过线性加权得到输入数据的累加结果 $z$：$z&#x3D;w_1 x_1+w_2 x_2+ … +b$。这里 $x_1,x_2,…,x_n$ 为感知机的输入，$w_1,w_2,…,w_n$为网络的权重系数，$b$ 为偏置项（$bias$）。然后将 $z$ 作为激活函数 $\\varPhi(\\cdot)$ 的输入，这里激活函数 $\\varPhi(\\cdot)$为 $sign$ 函数，其表达式为：</p>\n<p>$$<br>sign(x) &#x3D;<br>\\begin{cases}<br>+1 \\qquad &amp; x \\geq 0 \\\\<br>-1 \\qquad &amp; x \\lt 0<br>\\end{cases}<br>$$<br> $\\varPhi(\\cdot)$会将 $z$ 与某一阈值（此例中，阈值为$0$）进行比较，如果大于等于该阈值则感知器输出为 $1$，否则输出为 $-1$。通过这样的操作，输入数据被分类为 $1$ 或 $-1$ 这两个不同类别。<br> 其结构为<img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image.png?raw=true\" alt=\"alt text\"></p>\n<h3 id=\"单层感知机存在的问题\"><a href=\"#单层感知机存在的问题\" class=\"headerlink\" title=\"单层感知机存在的问题\"></a>单层感知机存在的问题</h3><p><img src=\"http://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-1.png?raw=true\" alt=\"alt text\"><br>单层感知机可被用来区分线性可分数据。在图中，逻辑与(AND)、逻辑与非(NAND)和逻辑或(OR)为线性可分函数，所以可利用单层感知机来模拟这些逻辑函数。但是，由于逻辑异或（XOR）是非线性可分的逻辑函数，因此单层感知机无法模拟逻辑异或函数的功能。</p>\n<h2 id=\"多层感知机\"><a href=\"#多层感知机\" class=\"headerlink\" title=\"多层感知机\"></a>多层感知机</h2><h3 id=\"多层感知机的结构\"><a href=\"#多层感知机的结构\" class=\"headerlink\" title=\"多层感知机的结构\"></a>多层感知机的结构</h3><p>由于无法模拟诸如异或以及其他复杂函数的功能，使得单层感知机的应用较为单一。一个简单的想法是，如果能在感知机模型中增加若干隐藏层，增强神经网络的非线性表达能力，就会让神经网络具有更强拟合能力。因此，由多个隐藏层构成的多层感知机被提出。</p>\n<p>多层感知机由输入层、输出层和至少一层的隐藏层构成。网络中各个隐藏层中神经元可接收相邻前序隐藏层中所有神经元传递而来的信息，经过加工处理后将信息输出给相邻后续隐藏层中所有神经元。<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-2.png?raw=true\" alt=\"alt text\"><br>在多层感知机中，相邻层所包含的神经元之间通常使用“全连接”方式进行连接。所谓“全连接”是指两个相邻层之间的神经元相互成对连接，但同一层内神经元之间没有连接。多层感知机可以模拟复杂非线性函数功能，所模拟函数的复杂性取决于网络隐藏层数目和各层中神经元数目。</p>\n<h3 id=\"隐藏层\"><a href=\"#隐藏层\" class=\"headerlink\" title=\"隐藏层\"></a>隐藏层</h3><p>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前L-1层看作表示，把最后一层看作线性预测器。 这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。 下面，我们以图的方式描述了多层感知机<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-4.png?raw=true\" alt=\"alt text\"><br>这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。因此，这个多层感知机中的层数为2。注意，这两个层都是全连接的。每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。</p>\n<h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><h4 id=\"RELU函数\"><a href=\"#RELU函数\" class=\"headerlink\" title=\"RELU函数\"></a>RELU函数</h4><p>函数定义：</p>\n<p>$$f(x)&#x3D;\\begin{cases} \\begin{matrix} 0 &amp; x&lt;0 \\end{matrix} \\\\ \\begin{matrix} x &amp; x\\ge 0 \\end{matrix} \\end{cases}$$</p>\n<p>导数：</p>\n<p>$${ f }^{ ‘ }(x)&#x3D;\\begin{cases} \\begin{matrix} 0 &amp; x&lt;0 \\end{matrix} \\\\ \\begin{matrix} 1 &amp; x\\ge 0 \\end{matrix} \\end{cases}$$<br>函数图如图所示：<img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-7.png?raw=true\"><br>导函数图如图所示：<img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-10.png?raw=true\" alt=\"alt text\"></p>\n<h4 id=\"sigmoid\"><a href=\"#sigmoid\" class=\"headerlink\" title=\"sigmoid\"></a>sigmoid</h4><p>函数定义：</p>\n<p>$${ f }(x)&#x3D;\\sigma (x)&#x3D;\\frac { 1 }{ 1+{ e }^{ -x } } $$</p>\n<p>导数：</p>\n<p>$${ f }^{ ‘ }(x)&#x3D;f(x)(1-f(x))$$<br>函数图形如图所示：<img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-5.png?raw=true\" alt=\"alt text\"></p>\n<p>导函数图如图所示：<img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-11.png?raw=true\" alt=\"alt text\"><br>优点：</p>\n<ol>\n<li>$sigmoid$ 函数的输出映射在 $(0,1)$ 之间，单调连续，输出范围有限，优化稳定，可以用作输出层；</li>\n<li>求导容易；</li>\n</ol>\n<p>缺点：</p>\n<ol>\n<li>由于其软饱和性，一旦落入饱和区梯度就会接近于0，根据反向传播的链式法则，容易产生梯度消失，导致训练出现问题；</li>\n<li>Sigmoid函数的输出恒大于0。非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢；</li>\n<li>计算时，由于具有幂运算，计算复杂度较高，运算速度较慢。</li>\n</ol>\n<h4 id=\"tanh\"><a href=\"#tanh\" class=\"headerlink\" title=\"tanh\"></a>tanh</h4><p>函数定义：</p>\n<p>$${ f }(x)&#x3D;tanh(x)&#x3D;\\frac { { e }^{ x }-{ e }^{ -x } }{ { e }^{ x }+{ e }^{ -x } }$$</p>\n<p>导数：</p>\n<p>$${ f }^{ ‘ }(x)&#x3D;1-f(x)^{ 2 }$$<br>函数图形如图所示：<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-6.png?raw=true\" alt=\"alt text\"><br>导函数图如图所示：<img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-12.png?raw=true\" alt=\"alt text\"><br>优点：</p>\n<ol>\n<li>$tanh$ 比 $sigmoid$ 函数收敛速度更快；</li>\n<li>相比 $sigmoid$ 函数，$tanh$ 是以 $0$ 为中心的；</li>\n</ol>\n<p>缺点：</p>\n<ol>\n<li>与 $sigmoid$ 函数相同，由于饱和性容易产生的梯度消失；</li>\n<li>与 $sigmoid$ 函数相同，由于具有幂运算，计算复杂度较高，运算速度较慢。</li>\n</ol>\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><h4 id=\"均方差损失（Mean-Square-Error，MSE）\"><a href=\"#均方差损失（Mean-Square-Error，MSE）\" class=\"headerlink\" title=\"均方差损失（Mean Square Error，MSE）\"></a>均方差损失（Mean Square Error，MSE）</h4><p>均方误差损失又称为二次损失、L2损失，常用于回归预测任务中。均方误差函数通过计算预测值和实际值之间距离（即误差）的平方来衡量模型优劣。即预测值和真实值越接近，两者的均方差就越小。</p>\n<h5 id=\"计算方式\"><a href=\"#计算方式\" class=\"headerlink\" title=\"计算方式\"></a>计算方式</h5><p>假设有 $n$ 个训练数据 $x_i$，每个训练数据 $x_i$ 的真实输出为 $y_i$，模型对 $x_i$ 的预测值为 $\\hat{y}_i$。该模型在 $n$ 个训练数据下所产生的均方误差损失可定义如下： </p>\n<p>$$<br>MSE&#x3D;\\frac{1}{n}\\sum_{i&#x3D;1}^n{\\left( y_i-\\hat{y}_i \\right) ^2}<br>$$<br>假设真实目标值为100，预测值在-10000到10000之间，我们绘制MSE函数曲线如图所示。可以看到，当预测值越接近100时，MSE损失值越小。MSE损失的范围为0到$\\infty$ 。<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master//photo/image-13.png?raw=true\" alt=\"alt text\"></p>\n<h4 id=\"交叉熵损失（cross-entropy-loss-）\"><a href=\"#交叉熵损失（cross-entropy-loss-）\" class=\"headerlink\" title=\"交叉熵损失（cross-entropy loss ）\"></a>交叉熵损失（cross-entropy loss ）</h4><p>在物理学中，“熵”被用来表示热力学系统所呈现的无序程度。香农将这一概念引入信息论领域，提出了“信息熵”概念，通过对数函数来测量信息的不确定性。</p>\n<p>交叉熵（cross entropy）是信息论中的重要概念，主要用来度量两个概率分布间的差异。假定 $p$ 和 $q$ 是数据 $x$ 的两个概率分布，通过 $q$ 来表示 $p$ 的交叉熵可如下计算：</p>\n<p>$$<br>H\\left( p,q \\right) &#x3D;-\\sum_x{p\\left( x \\right) \\log q\\left( x \\right)}<br>$$</p>\n<p>交叉熵刻画了两个概率分布之间的距离，旨在描绘通过概率分布 $q$ 来表达概率分布 $p$ 的困难程度。根据公式不难理解，交叉熵越小，两个概率分布 $p$ 和 $q$ 越接近。</p>\n<p>这里仍然以三类分类问题为例，假设数据 $x$ 属于类别 $1$。记数据x的类别分布概率为 $y$，显然 $y&#x3D;(1,0,0)$代表数据 $x$ 的实际类别分布概率。记 $\\hat{y}<br>$ 代表模型预测所得类别分布概率。</p>\n<p>那么对于数据 $x$ 而言，其实际类别分布概率 $y$ 和模型预测类别分布概率 $\\hat{y}$ 的交叉熵损失函数定义为：</p>\n<p>$$<br>cross\\ entropy&#x3D;-y\\times \\log \\left( \\hat{y} \\right)<br>$$</p>\n<p>很显然，一个良好的神经网络要尽量保证对于每一个输入数据，神经网络所预测类别分布概率与实际类别分布概率之间的差距越小越好，即交叉熵越小越好。于是，可将交叉熵作为损失函数来训练神经网络。</p>\n<p><img src=\"https://github.com/PaddlePaddle/awesome-DeepLearning/raw/master/docs/images/deep_learning/loss_functions/CrossEntropy.png\">三类分类问题中输入x的交叉熵损失示意图（x 属于第一类）</p>\n<p>上图给出了一个三个类别分类的例子。由于输入数据 $x$ 属于类别 $1$，因此其实际类别概率分布值为 $y&#x3D;(y_1,y_2,y_3)&#x3D;(1,0,0)$。经过神经网络的变换，得到了输入数据 $x$ 相对于三个类别的预测中间值 $(z1,z2,z3)$。然后，经过 $Softmax$ 函数映射，得到神经网络所预测的输入数据 $x$ 的类别分布概率 $\\hat{y}&#x3D;\\left( \\hat{y}_1,\\hat{y}_2,\\hat{y}_3 \\right)$。根据前面的介绍，$\\hat{y}_1$、$\\hat{y}_2$ 和 $\\hat{y}_3$ 为 $(0,1)$ 范围之间的一个概率值。由于样本 $x$ 属于第一个类别，因此希望神经网络所预测得到的 $\\hat{y}_1$取值要远远大于 $\\hat{y}_2$ 和 $\\hat{y}_3$ 的取值。为了得到这样的神经网络，在训练中可利用如下交叉熵损失函数来对模型参数进行优化：<br>$$<br>cross\\ entropy&#x3D;-\\left( y_1\\times \\log \\left( \\hat{y}_1 \\right) +y_2\\times \\log \\left( \\hat{y}_2 \\right) +y_3\\times \\log \\left( \\hat{y}_3 \\right) \\right)<br>$$</p>\n<p>在上式中，$y_2$ 和 $y_3$ 均为 $0$、$y_1$ 为 $1$，因此交叉熵损失函数简化为：<br>$$<br>-y_1\\times \\log \\left( \\hat{y}_1 \\right) &#x3D;-\\log \\left( \\hat{y}_1 \\right)<br>$$</p>\n<p>在神经网络训练中，要将输入数据实际的类别概率分布与模型预测的类别概率分布之间的误差（即损失）从输出端向输入端传递，以便来优化模型参数。下面简单介绍根据交叉熵计算得到的误差从 $\\hat{y}_1$ 传递给 $z_1$ 和 $z_2$（$z_3$ 的推导与 $z_2$ 相同）的情况。</p>\n<p>$$<br>\\frac{\\partial \\hat{y}_1}{\\partial z_1}&#x3D;\\frac{\\partial \\left( \\frac{e^{z_1}}{\\sum_k{e^{z_k}}} \\right)}{\\partial z_1}&#x3D;\\frac{\\left( e^{z_1} \\right) ^{‘}\\times \\sum_k{e^{z_k}-e^{z_1}\\times e^{z_1}}}{\\left( \\sum_k{e^{z_k}} \\right) ^2}&#x3D;\\frac{e^{z_1}}{\\sum_k{e^{z_k}}}-\\frac{e^{z_1}}{\\sum_k{e^{z_k}}}\\times \\frac{e^{z_1}}{\\sum_k{e^{z_k}}}&#x3D;\\hat{y}_1\\left( 1-\\hat{y}_1 \\right)<br>$$</p>\n<p>由于交叉熵损失函数 $-\\log \\left( \\hat{y}_1 \\right)$ 对 $\\hat{y}_1$ 求导的结果为 $-\\frac{1}{\\hat{y}_1}$，$\\hat{y}_1\\left( 1-\\hat{y}_1 \\right)$ 与 $-\\frac{1}{\\hat{y}_1}$ 相乘为 $\\hat{y}_1-1$。这说明一旦得到模型预测输出 $\\hat{y}_1$，将该输出减去1就是交叉损失函数相对于 $z_1$ 的偏导结果。</p>\n<p>$$<br>\\frac{\\partial \\hat{y}_1}{\\partial z_2}&#x3D;\\frac{\\partial \\left( \\frac{e^{z_1}}{\\sum_k{e^{z_k}}} \\right)}{\\partial z_2}&#x3D;\\frac{0\\times \\sum_k{e^{z_k}-e^{z_1}\\times e^{z_2}}}{\\left( \\sum_k{e^{z_k}} \\right) ^2}&#x3D;-\\frac{e^{z_1}}{\\sum_k{e^{z_k}}}\\times \\frac{e^{z_2}}{\\sum_k{e^{z_k}}}&#x3D;-\\hat{y}_1\\hat{y}_2<br>$$</p>\n<p>同理，交叉熵损失函数导数为 $-\\frac{1}{\\hat{y}_1}$，$-\\hat{y}_1\\hat{y}_2$ 与 $-\\frac{1}{\\hat{y}_1}$ 相乘结果为 $\\hat{y}_2$。这意味对于除第一个输出节点以外的节点进行偏导，在得到模型预测输出后，只要将其保存，就是交叉损失函数相对于其他节点的偏导结果。在 $z_1$、$z_2$ 和 $z_3$得到偏导结果后，再通过链式法则（后续介绍）将损失误差继续往输入端传递即可。</p>\n<p>在上面的例子中，假设所预测中间值 $(z_1,z_2,z_3)$ 经过 $Softmax$ 映射后所得结果为 $(0.34,0.46,0.20)$。由于已知输入数据 $x$ 属于第一类，显然这个输出不理想而需要对模型参数进行优化。如果选择交叉熵损失函数来优化模型，则 $(z_1,z_2,z_3)$ 这一层的偏导值为 $(0.34-1,0.46,0.20)&#x3D; (-0.66,0.46,0.20)$。</p>\n<p>可以看出，$Softmax$ 和交叉熵损失函数相互结合，为偏导计算带来了极大便利。偏导计算使得损失误差从输出端向输入端传递，来对模型参数进行优化。在这里，交叉熵与$Softmax$ 函数结合在一起，因此也叫 $Softmax$ 损失（Softmax with cross-entropy loss）。</p>\n<h1 id=\"基于多层感知机的姓氏分类网络\"><a href=\"#基于多层感知机的姓氏分类网络\" class=\"headerlink\" title=\"基于多层感知机的姓氏分类网络\"></a>基于多层感知机的姓氏分类网络</h1><p>我们将MLP应用于将姓氏分类到其原籍国的任务。从公开观察到的数据推断人口统计信息(如国籍)具有从产品推荐到确保不同人口统计用户获得公平结果的应用。人口统计和其他自我识别信息统称为“受保护属性”。“在建模和产品中使用这些属性时，必须小心。”我们首先对每个姓氏的字符进行拆分，并像对待“示例:将餐馆评论的情绪分类”中的单词一样对待它们。除了数据上的差异，字符层模型在结构和实现上与基于单词的模型基本相似.我们将通过描述姓氏分类器模型及其设计背后的思想过程来继续。</p>\n<h2 id=\"The-Surname-Dataset\"><a href=\"#The-Surname-Dataset\" class=\"headerlink\" title=\"The Surname Dataset\"></a>The Surname Dataset</h2><p>姓氏数据集，它收集了来自18个不同国家的10,000个姓氏，这些姓氏是作者从互联网上不同的姓名来源收集的。该数据集将在本课程实验的几个示例中重用，并具有一些使其有趣的属性。第一个性质是它是相当不平衡的。排名前三的课程占数据的60%以上:27%是英语，21%是俄语，14%是阿拉伯语。剩下的15个民族的频率也在下降——这也是语言特有的特性。第二个特点是，在国籍和姓氏正字法(拼写)之间有一种有效和直观的关系。有些拼写变体与原籍国联系非常紧密(比如“O ‘Neill”、“Antonopoulos”、“Nagasawa”或“Zhu”)。</p>\n<p>为了创建最终的数据集，我们从一个比课程补充材料中包含的版本处理更少的版本开始，并执行了几个数据集修改操作。第一个目的是减少这种不平衡——原始数据集中70%以上是俄文，这可能是由于抽样偏差或俄文姓氏的增多。为此，我们通过选择标记为俄语的姓氏的随机子集对这个过度代表的类进行子样本。接下来，我们根据国籍对数据集进行分组，并将数据集分为三个部分:70%到训练数据集，15%到验证数据集，最后15%到测试数据集，以便跨这些部分的类标签分布具有可比性。</p>\n<p>SurnameDataset的实现与“Example: classification of Sentiment of Restaurant Reviews”中的ReviewDataset几乎相同，只是在getitem方法的实现方式上略有不同。回想一下，本课程中呈现的数据集类继承自PyTorch的数据集类，因此，我们需要实现两个函数:<code>__getitem</code>方法，它在给定索引时返回一个数据点;以及len方法，该方法返回数据集的长度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SurnameDataset</span>(<span class=\"title class_ inherited__\">Dataset</span>):</span><br><span class=\"line\">    <span class=\"comment\"># Implementation is nearly identical to Section 3.5</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, index</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;PyTorch 数据集的主要入口方法</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        Args:</span></span><br><span class=\"line\"><span class=\"string\">            index (int): 数据点的索引 </span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">            一个包含数据点的字典:</span></span><br><span class=\"line\"><span class=\"string\">                特征 (x_surname)</span></span><br><span class=\"line\"><span class=\"string\">                标签 (y_nationality)</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        row = self._target_df.iloc[index]</span><br><span class=\"line\"></span><br><span class=\"line\">        surname_vector = self._vectorizer.vectorize(row.surname)</span><br><span class=\"line\"></span><br><span class=\"line\">        nationality_index = self._vectorizer.nationality_vocab.lookup_token(row.nationality)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> &#123;<span class=\"string\">&#x27;x_surname&#x27;</span>: surname_vector,</span><br><span class=\"line\">                <span class=\"string\">&#x27;y_nationality&#x27;</span>: nationality_index&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Vocabulary-Vectorizer-and-DataLoader\"><a href=\"#Vocabulary-Vectorizer-and-DataLoader\" class=\"headerlink\" title=\"Vocabulary, Vectorizer, and DataLoader\"></a>Vocabulary, Vectorizer, and DataLoader</h2><p>为了使用字符对姓氏进行分类，我们使用词汇表、向量化器和DataLoader将姓氏字符串转换为向量化的minibatches。这些数据结构与“Example: Classifying Sentiment of Restaurant Reviews”中使用的数据结构相同，它们举例说明了一种多态性，这种多态性将姓氏的字符标记与Yelp评论的单词标记相同对待。数据不是通过将字令牌映射到整数来向量化的，而是通过将字符映射到整数来向量化的。</p>\n<h3 id=\"THE-VOCABULARY-CLASS\"><a href=\"#THE-VOCABULARY-CLASS\" class=\"headerlink\" title=\"THE VOCABULARY CLASS\"></a>THE VOCABULARY CLASS</h3><p>本例中使用的词汇类与“example: Classifying Sentiment of Restaurant Reviews”中的词汇完全相同，该词汇类将Yelp评论中的单词映射到对应的整数。简要概述一下，词汇表是两个Python字典的协调，这两个字典在令牌(在本例中是字符)和整数之间形成一个双射;也就是说，第一个字典将字符映射到整数索引，第二个字典将整数索引映射到字符。add_token方法用于向词汇表中添加新的令牌，lookup_token方法用于检索索引，lookup_index方法用于检索给定索引的令牌(在推断阶段很有用)。与Yelp评论的词汇表不同，我们使用的是one-hot词汇表，不计算字符出现的频率，只对频繁出现的条目进行限制。这主要是因为数据集很小，而且大多数字符足够频繁。</p>\n<h3 id=\"THE-SURNAMEVECTORIZER\"><a href=\"#THE-SURNAMEVECTORIZER\" class=\"headerlink\" title=\"THE SURNAMEVECTORIZER\"></a>THE SURNAMEVECTORIZER</h3><p>虽然词汇表将单个令牌(字符)转换为整数，但SurnameVectorizer负责应用词汇表并将姓氏转换为向量。实例化和使用非常类似于“示例:对餐馆评论的情绪进行分类”中的ReviewVectorizer，但有一个关键区别:字符串没有在空格上分割。姓氏是字符的序列，每个字符在我们的词汇表中是一个单独的标记。然而，在“卷积神经网络”出现之前，我们将忽略序列信息，通过迭代字符串输入中的每个字符来创建输入的收缩one-hot向量表示。我们为以前未遇到的字符指定一个特殊的令牌，即UNK。由于我们仅从训练数据实例化词汇表，而且验证或测试数据中可能有惟一的字符，所以在字符词汇表中仍然使用UNK符号。</p>\n<p>虽然我们在这个示例中使用了收缩的one-hot，但是在后面的实验中，将了解其他向量化方法，它们是one-hot编码的替代方法，有时甚至更好。具体来说，在“示例:使用CNN对姓氏进行分类”中，将看到一个热门矩阵，其中每个字符都是矩阵中的一个位置，并具有自己的热门向量。然后，在实验5中，将学习嵌入层，返回整数向量的向量化，以及如何使用它们创建密集向量矩阵。看一下示例4-6中SurnameVectorizer的代码。</p>\n<p>Example 4-6. Implementing SurnameVectorizer</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SurnameVectorizer</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;协调词汇表并将它们应用到使用的向量化器&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, surname_vocab, nationality_vocab, max_surname_length</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            surname_vocab (Vocabulary): 将字符映射到整数的词汇表</span></span><br><span class=\"line\"><span class=\"string\">            nationality_vocab (Vocabulary): 将国籍映射到整数的词汇表</span></span><br><span class=\"line\"><span class=\"string\">            max_surname_length (int): 最长姓氏的长度</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        self.surname_vocab = surname_vocab</span><br><span class=\"line\">        self.nationality_vocab = nationality_vocab</span><br><span class=\"line\">        self._max_surname_length = max_surname_length</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">vectorize</span>(<span class=\"params\">self, surname</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            surname (str): 姓氏</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            one_hot_matrix (np.ndarray): 一个one-hot向量的矩阵</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        one_hot_matrix_size = (<span class=\"built_in\">len</span>(self.surname_vocab), self._max_surname_length)</span><br><span class=\"line\">        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)</span><br><span class=\"line\">                               </span><br><span class=\"line\">        <span class=\"keyword\">for</span> position_index, character <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(surname):</span><br><span class=\"line\">            character_index = self.surname_vocab.lookup_token(character)</span><br><span class=\"line\">            one_hot_matrix[character_index][position_index] = <span class=\"number\">1</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> one_hot_matrix</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">    @classmethod</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">from_dataframe</span>(<span class=\"params\">cls, surname_df</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;从数据集DataFrame实例化向量化器</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            surname_df (pandas.DataFrame): 姓氏数据集</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            SurnameVectorizer的实例</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        surname_vocab = Vocabulary(unk_token=<span class=\"string\">&quot;@&quot;</span>)</span><br><span class=\"line\">        nationality_vocab = Vocabulary(add_unk=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        max_surname_length = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> index, row <span class=\"keyword\">in</span> surname_df.iterrows():</span><br><span class=\"line\">            max_surname_length = <span class=\"built_in\">max</span>(max_surname_length, <span class=\"built_in\">len</span>(row.surname))</span><br><span class=\"line\">            <span class=\"keyword\">for</span> letter <span class=\"keyword\">in</span> row.surname:</span><br><span class=\"line\">                surname_vocab.add_token(letter)</span><br><span class=\"line\">            nationality_vocab.add_token(row.nationality)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> cls(surname_vocab, nationality_vocab, max_surname_length)</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"The-Surname-Classifier-Model\"><a href=\"#The-Surname-Classifier-Model\" class=\"headerlink\" title=\"The Surname Classifier Model\"></a>The Surname Classifier Model</h2><p>SurnameClassifier是本实验前面介绍的MLP的实现(示例4-7)。第一个线性层将输入向量映射到中间向量，并对该向量应用非线性。第二线性层将中间向量映射到预测向量。</p>\n<p>在最后一步中，可选地应用softmax操作，以确保输出和为1;这就是所谓的“概率”。它是可选的原因与我们使用的损失函数的数学公式有关——交叉熵损失。我们研究了“损失函数”中的交叉熵损失。回想一下，交叉熵损失对于多类分类是最理想的，但是在训练过程中软最大值的计算不仅浪费而且在很多情况下并不稳定。</p>\n<p>Example 4-7. The SurnameClassifier as an MLP</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SurnameClassifier</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, initial_num_channels, num_classes, num_channels</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        创建一个姓氏分类器。</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            initial_num_channels (int): 输入特征向量的大小</span></span><br><span class=\"line\"><span class=\"string\">            num_classes (int): 输出预测向量的大小</span></span><br><span class=\"line\"><span class=\"string\">            num_channels (int): 网络中要始终使用的恒定通道大小</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(SurnameClassifier, self).__init__()</span><br><span class=\"line\">        </span><br><span class=\"line\">        self.convnet = nn.Sequential(</span><br><span class=\"line\">            nn.Conv1d(in_channels=initial_num_channels, </span><br><span class=\"line\">                      out_channels=num_channels, kernel_size=<span class=\"number\">3</span>),</span><br><span class=\"line\">            nn.ELU(),</span><br><span class=\"line\">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class=\"line\">                      kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">            nn.ELU(),</span><br><span class=\"line\">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class=\"line\">                      kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">            nn.ELU(),</span><br><span class=\"line\">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class=\"line\">                      kernel_size=<span class=\"number\">3</span>),</span><br><span class=\"line\">            nn.ELU()</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.fc = nn.Linear(num_channels, num_classes)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x_surname, apply_softmax=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;分类器的前向传播</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x_surname (torch.Tensor): 输入数据张量。</span></span><br><span class=\"line\"><span class=\"string\">                x_surname.shape 应为 (batch, initial_num_channels, max_surname_length)</span></span><br><span class=\"line\"><span class=\"string\">            apply_softmax (bool): softmax 激活的标志</span></span><br><span class=\"line\"><span class=\"string\">                如果与交叉熵损失一起使用，则应为 False</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            结果张量。tensor.shape 应为 (batch, num_classes)</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        features = self.convnet(x_surname).squeeze(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">       </span><br><span class=\"line\">        prediction_vector = self.fc(features)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> apply_softmax:</span><br><span class=\"line\">            prediction_vector = F.softmax(prediction_vector, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> prediction_vector</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"The-Training-Routine\"><a href=\"#The-Training-Routine\" class=\"headerlink\" title=\"The Training Routine\"></a>The Training Routine</h2><p>虽然我们使用了不同的模型、数据集和损失函数，但是训练例程是相同的。因此，在例4-8中，我们只展示了args以及本例中的训练例程与“示例:餐厅评论情绪分类”中的示例之间的主要区别。</p>\n<p>Example 4-8. The args for classifying surnames with an MLP</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">args = Namespace(</span><br><span class=\"line\">    <span class=\"comment\"># 数据和路径信息</span></span><br><span class=\"line\">    surname_csv=<span class=\"string\">&quot;data/surnames/surnames_with_splits.csv&quot;</span>,</span><br><span class=\"line\">    vectorizer_file=<span class=\"string\">&quot;vectorizer.json&quot;</span>,</span><br><span class=\"line\">    model_state_file=<span class=\"string\">&quot;model.pth&quot;</span>,</span><br><span class=\"line\">    save_dir=<span class=\"string\">&quot;model_storage/ch4/cnn&quot;</span>,</span><br><span class=\"line\">    <span class=\"comment\"># 模型超参数</span></span><br><span class=\"line\">    hidden_dim=<span class=\"number\">100</span>,</span><br><span class=\"line\">    num_channels=<span class=\"number\">256</span>,</span><br><span class=\"line\">    <span class=\"comment\"># 训练超参数</span></span><br><span class=\"line\">    seed=<span class=\"number\">1337</span>,</span><br><span class=\"line\">    learning_rate=<span class=\"number\">0.001</span>,</span><br><span class=\"line\">    batch_size=<span class=\"number\">128</span>,</span><br><span class=\"line\">    num_epochs=<span class=\"number\">100</span>,</span><br><span class=\"line\">    early_stopping_criteria=<span class=\"number\">5</span>,</span><br><span class=\"line\">    dropout_p=<span class=\"number\">0.1</span>,</span><br><span class=\"line\">    <span class=\"comment\"># 运行时选项</span></span><br><span class=\"line\">    cuda=<span class=\"literal\">False</span>,</span><br><span class=\"line\">    reload_from_files=<span class=\"literal\">False</span>,</span><br><span class=\"line\">    expand_filepaths_to_save_dir=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    catch_keyboard_interrupt=<span class=\"literal\">True</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p>训练中最显著的差异与模型中输出的种类和使用的损失函数有关。在这个例子中，输出是一个多类预测向量，可以转换为概率。正如在模型描述中所描述的，这种输出的损失类型仅限于CrossEntropyLoss和NLLLoss。由于它的简化，我们使用了CrossEntropyLoss。</p>\n<p>在例4-9中，我们展示了数据集、模型、损失函数和优化器的实例化。这些实例应该看起来与“示例:将餐馆评论的情绪分类”中的实例几乎相同。事实上，在本课程后面的实验中，这种模式将对每个示例进行重复。</p>\n<p>Example 4-9. Instantiating the dataset, model, loss, and optimizer</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)</span><br><span class=\"line\">vectorizer = dataset.get_vectorizer()</span><br><span class=\"line\"></span><br><span class=\"line\">classifier = SurnameClassifier(input_dim=<span class=\"built_in\">len</span>(vectorizer.surname_vocab),</span><br><span class=\"line\">                               hidden_dim=args.hidden_dim,</span><br><span class=\"line\">                               output_dim=<span class=\"built_in\">len</span>(vectorizer.nationality_vocab))</span><br><span class=\"line\"></span><br><span class=\"line\">classifier = classifier.to(args.device)    </span><br><span class=\"line\"></span><br><span class=\"line\">loss_func = nn.CrossEntropyLoss(dataset.class_weights)</span><br><span class=\"line\">optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"THE-TRAINING-LOOP\"><a href=\"#THE-TRAINING-LOOP\" class=\"headerlink\" title=\"THE TRAINING LOOP\"></a>THE TRAINING LOOP</h3><p>与“Example: Classifying Sentiment of Restaurant Reviews”中的训练循环相比，本例的训练循环除了变量名以外几乎是相同的。具体来说，示例4-10显示了使用不同的key从batch_dict中获取数据。除了外观上的差异，训练循环的功能保持不变。利用训练数据，计算模型输出、损失和梯度。然后，使用梯度来更新模型。</p>\n<p>Example 4-10. A snippet of the training loop</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># the training routine is these 5 steps:</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># --------------------------------------</span></span><br><span class=\"line\"><span class=\"comment\"># step 1. zero the gradients</span></span><br><span class=\"line\">optimizer.zero_grad()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># step 2. compute the output</span></span><br><span class=\"line\">y_pred = classifier(batch_dict[<span class=\"string\">&#x27;x_surname&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># step 3. compute the loss</span></span><br><span class=\"line\">loss = loss_func(y_pred, batch_dict[<span class=\"string\">&#x27;y_nationality&#x27;</span>])</span><br><span class=\"line\">loss_batch = loss.to(<span class=\"string\">&quot;cpu&quot;</span>).item()</span><br><span class=\"line\">running_loss += (loss_batch - running_loss) / (batch_index + <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># step 4. use loss to produce gradients</span></span><br><span class=\"line\">loss.backward()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># step 5. use optimizer to take gradient step</span></span><br><span class=\"line\">optimizer.step()</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"Model-Evaluation-and-Prediction\"><a href=\"#Model-Evaluation-and-Prediction\" class=\"headerlink\" title=\"Model Evaluation and Prediction\"></a>Model Evaluation and Prediction</h2><p>要理解模型的性能，应该使用定量和定性方法分析模型。定量测量出的测试数据的误差，决定了分类器能否推广到不可见的例子。定性地说，可以通过查看分类器的top-k预测来为一个新示例开发模型所了解的内容的直觉。</p>\n<h3 id=\"EVALUATING-ON-THE-TEST-DATASET\"><a href=\"#EVALUATING-ON-THE-TEST-DATASET\" class=\"headerlink\" title=\"EVALUATING ON THE TEST DATASET\"></a>EVALUATING ON THE TEST DATASET</h3><p>评价SurnameClassifier测试数据,我们执行相同的常规的routine文本分类的例子“餐馆评论的例子:分类情绪”:我们将数据集设置为遍历测试数据,调用<code>classifier.eval()</code>方法,并遍历测试数据以同样的方式与其他数据。在这个例子中，调用<code>classifier.eval()</code>可以防止PyTorch在使用测试&#x2F;评估数据时更新模型参数。</p>\n<p>该模型对测试数据的准确性达到50%左右。如果在附带的notebook中运行训练例程，会注意到在训练数据上的性能更高。这是因为模型总是更适合它所训练的数据，所以训练数据的性能并不代表新数据的性能。如果遵循代码，你可以尝试隐藏维度的不同大小，应该注意到性能的提高。然而，这种增长不会很大(尤其是与“用CNN对姓氏进行分类的例子”中的模型相比)。其主要原因是收缩的onehot向量化方法是一种弱表示。虽然它确实简洁地将每个姓氏表示为单个向量，但它丢弃了字符之间的顺序信息，这对于识别起源非常重要。</p>\n<h3 id=\"CLASSIFYING-A-NEW-SURNAME\"><a href=\"#CLASSIFYING-A-NEW-SURNAME\" class=\"headerlink\" title=\"CLASSIFYING A NEW SURNAME\"></a>CLASSIFYING A NEW SURNAME</h3><p>示例4-11显示了分类新姓氏的代码。给定一个姓氏作为字符串，该函数将首先应用向量化过程，然后获得模型预测。注意，我们包含了apply_softmax标志，所以结果包含概率。模型预测，在多项式的情况下，是类概率的列表。我们使用PyTorch张量最大函数来得到由最高预测概率表示的最优类。</p>\n<p>Example 4-11. A function for performing nationality prediction</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict_nationality</span>(<span class=\"params\">surname, classifier, vectorizer</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;预测姓氏的国籍</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">        surname (str): 要分类的姓氏</span></span><br><span class=\"line\"><span class=\"string\">        classifier (SurnameClassifer): 分类器的实例</span></span><br><span class=\"line\"><span class=\"string\">        vectorizer (SurnameVectorizer): 相应的向量化器</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        dict: 包含最可能的国籍及其概率的字典</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 向量化姓氏</span></span><br><span class=\"line\">    vectorized_surname = vectorizer.vectorize(surname)</span><br><span class=\"line\">    <span class=\"comment\"># 将向量化的姓氏转换为PyTorch张量，并添加批次维度</span></span><br><span class=\"line\">    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 使用分类器进行预测，并对输出进行softmax处理</span></span><br><span class=\"line\">    result = classifier(vectorized_surname, apply_softmax=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 获取概率最高的国籍及其对应的索引</span></span><br><span class=\"line\">    probability_values, indices = result.<span class=\"built_in\">max</span>(dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">    index = indices.item()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 通过索引查找预测的国籍，并获取其概率值</span></span><br><span class=\"line\">    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)</span><br><span class=\"line\">    probability_value = probability_values.item()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 返回预测结果的字典</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> &#123;<span class=\"string\">&#x27;nationality&#x27;</span>: predicted_nationality, <span class=\"string\">&#x27;probability&#x27;</span>: probability_value&#125;</span><br></pre></td></tr></table></figure>\n<p>示例：<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-15.png?raw=true\" alt=\"alt text\"></p>\n<h3 id=\"RETRIEVING-THE-TOP-K-PREDICTIONS-FOR-A-NEW-SURNAME\"><a href=\"#RETRIEVING-THE-TOP-K-PREDICTIONS-FOR-A-NEW-SURNAME\" class=\"headerlink\" title=\"RETRIEVING THE TOP-K PREDICTIONS FOR A NEW SURNAME\"></a>RETRIEVING THE TOP-K PREDICTIONS FOR A NEW SURNAME</h3><p>不仅要看最好的预测，还要看更多的预测。例如，NLP中的标准实践是采用k-best预测并使用另一个模型对它们重新排序。PyTorch提供了一个torch.topk函数，它提供了一种方便的方法来获得这些预测，如示例4-12所示。</p>\n<p>Example 4-12. Predicting the top-k nationalities</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict_topk_nationality</span>(<span class=\"params\">surname, classifier, vectorizer, k=<span class=\"number\">5</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;预测姓氏的前K个可能的国籍</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">        surname (str): 要分类的姓氏</span></span><br><span class=\"line\"><span class=\"string\">        classifier (SurnameClassifer): 分类器的实例</span></span><br><span class=\"line\"><span class=\"string\">        vectorizer (SurnameVectorizer): 相应的向量化器</span></span><br><span class=\"line\"><span class=\"string\">        k (int): 要返回的前K个国籍数量</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        list: 字典的列表，每个字典包含一个国籍和其对应的概率</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 向量化姓氏</span></span><br><span class=\"line\">    vectorized_surname = vectorizer.vectorize(surname)</span><br><span class=\"line\">    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 使用分类器进行预测，并对输出进行softmax处理，获取前K个国籍及其概率</span></span><br><span class=\"line\">    prediction_vector = classifier(vectorized_surname, apply_softmax=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    probability_values, indices = torch.topk(prediction_vector, k=k)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 获取预测结果中概率最高的K个国籍及其概率值</span></span><br><span class=\"line\">    probability_values = probability_values[<span class=\"number\">0</span>].detach().numpy()</span><br><span class=\"line\">    indices = indices[<span class=\"number\">0</span>].detach().numpy()</span><br><span class=\"line\">    </span><br><span class=\"line\">    results = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> kth_index <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(k):</span><br><span class=\"line\">        <span class=\"comment\"># 通过索引查找国籍，并获取其对应的概率值，将结果存储为字典列表</span></span><br><span class=\"line\">        nationality = vectorizer.nationality_vocab.lookup_index(indices[kth_index])</span><br><span class=\"line\">        probability_value = probability_values[kth_index]</span><br><span class=\"line\">        results.append(&#123;<span class=\"string\">&#x27;nationality&#x27;</span>: nationality, </span><br><span class=\"line\">                        <span class=\"string\">&#x27;probability&#x27;</span>: probability_value&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> results</span><br></pre></td></tr></table></figure>\n<p>示例：<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-16.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"Regularizing-MLPs-Weight-Regularization-and-Structural-Regularization-or-Dropout\"><a href=\"#Regularizing-MLPs-Weight-Regularization-and-Structural-Regularization-or-Dropout\" class=\"headerlink\" title=\"Regularizing MLPs: Weight Regularization and Structural Regularization (or Dropout)\"></a>Regularizing MLPs: Weight Regularization and Structural Regularization (or Dropout)</h2><p>在实验3中，我们解释了正则化是如何解决过拟合问题的，并研究了两种重要的权重正则化类型——L1和L2。这些权值正则化方法也适用于MLPs和卷积神经网络，我们将在本实验后面介绍。除权值正则化外，对于深度模型(即例如本实验讨论的前馈网络，一种称为dropout的结构正则化方法变得非常重要。</p>\n<h3 id=\"DROPOUT\"><a href=\"#DROPOUT\" class=\"headerlink\" title=\"DROPOUT\"></a>DROPOUT</h3><p>简单地说，在训练过程中，dropout有一定概率使属于两个相邻层的单元之间的连接减弱。这有什么用呢?我们从斯蒂芬•梅里蒂(Stephen Merity)的一段直观(且幽默)的解释开始：“Dropout，简单地说，是指如果你能在喝醉的时候反复学习如何做一件事，那么你应该能够在清醒的时候做得更好。这一见解产生了许多最先进的结果和一个新兴的领域。”</p>\n<p>神经网络——尤其是具有大量分层的深层网络——可以在单元之间创建有趣的相互适应。“Coadaptation”是神经科学中的一个术语，但在这里它只是指一种情况，即两个单元之间的联系变得过于紧密，而牺牲了其他单元之间的联系。这通常会导致模型与数据过拟合。通过概率地丢弃单元之间的连接，我们可以确保没有一个单元总是依赖于另一个单元，从而产生健壮的模型。dropout不会向模型中添加额外的参数，但是需要一个超参数——“drop probability”。drop probability，它是单位之间的连接drop的概率。通常将下降概率设置为0.5。例4-13给出了一个带dropout的MLP的重新实现。</p>\n<p>Example 4-13. MLP with dropout</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MultilayerPerceptron</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_dim, hidden_dim, output_dim</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Args:</span></span><br><span class=\"line\"><span class=\"string\">            input_dim (int): the size of the input vectors</span></span><br><span class=\"line\"><span class=\"string\">            hidden_dim (int): the output size of the first Linear layer</span></span><br><span class=\"line\"><span class=\"string\">            output_dim (int): the output size of the second Linear layer</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultilayerPerceptron, self).__init__()</span><br><span class=\"line\">        self.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class=\"line\">        self.fc2 = nn.Linear(hidden_dim, output_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x_in, apply_softmax=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;The forward pass of the MLP</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        Args:</span></span><br><span class=\"line\"><span class=\"string\">            x_in (torch.Tensor): an input data tensor.</span></span><br><span class=\"line\"><span class=\"string\">                x_in.shape should be (batch, input_dim)</span></span><br><span class=\"line\"><span class=\"string\">            apply_softmax (bool): a flag for the softmax activation</span></span><br><span class=\"line\"><span class=\"string\">                should be false if used with the Cross Entropy losses</span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">            the resulting tensor. tensor.shape should be (batch, output_dim)</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        intermediate = F.relu(self.fc1(x_in))</span><br><span class=\"line\">        output = self.fc2(F.dropout(intermediate, p=<span class=\"number\">0.5</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> apply_softmax:</span><br><span class=\"line\">            output = F.softmax(output, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br></pre></td></tr></table></figure>\n<p>请注意，dropout只适用于训练期间，不适用于评估期间。作为练习，可以尝试带有dropout的SurnameClassifier模型，看看它如何更改结果。</p>\n<h1 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h1><p>在本实验的第一部分中，我们深入研究了MLPs、由一系列线性层和非线性函数构建的神经网络。mlp不是利用顺序模式的最佳工具。例如，在姓氏数据集中，姓氏可以有(不同长度的)段，这些段可以显示出相当多关于其起源国家的信息(如“O’Neill”中的“O”、“Antonopoulos”中的“opoulos”、“Nagasawa”中的“sawa”或“Zhu”中的“Zh”)。这些段的长度可以是可变的，挑战是在不显式编码的情况下捕获它们。</p>\n<p>在本节中，我们将介绍卷积神经网络(CNN)，这是一种非常适合检测空间子结构(并因此创建有意义的空间子结构)的神经网络。CNNs通过使用少量的权重来扫描输入数据张量来实现这一点。通过这种扫描，它们产生表示子结构检测(或不检测)的输出张量。</p>\n<p>在本节的其余部分中，我们首先描述CNN的工作方式，以及在设计CNN时应该考虑的问题。我们深入研究CNN超参数，目的是提供直观的行为和这些超参数对输出的影响。最后，我们通过几个简单的例子逐步说明CNNs的机制。在“示例:使用CNN对姓氏进行分类”中，我们将深入研究一个更广泛的示例。</p>\n<h2 id=\"简介-1\"><a href=\"#简介-1\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>卷积神经网络（英语：convolutional neural network，缩写：CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。</p>\n<p>CNNs的名称和基本功能源于经典的数学运算卷积。卷积已经应用于各种工程学科，包括数字信号处理和计算机图形学。一般来说，卷积使用程序员指定的参数。这些参数被指定来匹配一些功能设计，如突出边缘或抑制高频声音。事实上，许多Photoshop滤镜都是应用于图像的固定卷积运算。然而，在深度学习和本实验中，我们从数据中学习卷积滤波器的参数，因此它对于解决当前的任务是最优的。</p>\n<p>卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。</p>\n<p>卷积神经网络的灵感来自于动物视觉皮层组织的神经连接方式。单个神经元只对有限区域内的刺激作出反应，不同神经元的感知区域相互重叠从而覆盖整个视野</p>\n<h2 id=\"模型结构-1\"><a href=\"#模型结构-1\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h2><p>典型的 CNN 由3个部分构成：</p>\n<p>1.卷积层</p>\n<p>2.池化层</p>\n<p>3.全连接层</p>\n<p>如果简单来描述的话：</p>\n<p>卷积层负责提取图像中的局部特征；池化层用来大幅降低参数量级(降维)；全连接层类似传统神经网络的部分，用来输出想要的结果。</p>\n<h3 id=\"Convolutional-layer\"><a href=\"#Convolutional-layer\" class=\"headerlink\" title=\"Convolutional layer\"></a>Convolutional layer</h3><p>卷积层的运算过程如下图，用一个卷积核扫完整张图片：<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/f144f-2019-06-19-juanji.gif?raw=true\" alt=\"请使用魔法\"></p>\n<p>这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。</p>\n<p>在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-14.png?raw=true\" alt=\"alt text\"></p>\n<h3 id=\"Pooling-Layer\"><a href=\"#Pooling-Layer\" class=\"headerlink\" title=\"Pooling Layer\"></a>Pooling Layer</h3><p>池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/3fd53-2019-06-19-chihua.gif?raw=true\" alt=\"alt text\"><br>上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。</p>\n<p>之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。</p>\n<p>池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。</p>\n<h3 id=\"Fully-connected-layer\"><a href=\"#Fully-connected-layer\" class=\"headerlink\" title=\"Fully connected layer\"></a>Fully connected layer</h3><p>这个部分就是最后一步了，经过卷积层和池化层处理过的数据输入到全连接层，得到最终想要的结果。</p>\n<p>经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/c1a6d-2019-06-19-quanlianjie.png.webp?raw=true\" alt=\"alt text\"><br>典型的 CNN 并非只是上面提到的3层结构，而是多层结构，例如 LeNet-5 的结构就如下图所示：</p>\n<p>卷积层 – 池化层- 卷积层 – 池化层 – 卷积层 – 全连接层<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/a8f0b-2019-06-19-lenet.png.webp?raw=true\" alt=\"alt text\"></p>\n<h3 id=\"DIMENSION-OF-THE-CONVOLUTION-OPERATION\"><a href=\"#DIMENSION-OF-THE-CONVOLUTION-OPERATION\" class=\"headerlink\" title=\"DIMENSION OF THE CONVOLUTION OPERATION\"></a>DIMENSION OF THE CONVOLUTION OPERATION</h3><p>首先要理解的概念是卷积运算的维数。在本节，我们使用二维卷积进行说明，但是根据数据的性质，还有更适合的其他维度的卷积。在PyTorch中，卷积可以是一维、二维或三维的，分别由Conv1d、Conv2d和Conv3d模块实现。一维卷积对于每个时间步都有一个特征向量的时间序列非常有用。在这种情况下，我们可以在序列维度上学习模式。NLP中的卷积运算大多是一维的卷积。另一方面，二维卷积试图捕捉数据中沿两个方向的时空模式;例如，在图像中沿高度和宽度维度——为什么二维卷积在图像处理中很流行。类似地，在三维卷积中，模式是沿着数据中的三维捕获的。例如，在视频数据中，信息是三维的，二维表示图像的帧，时间维表示帧的序列。我们主要使用Conv1d。</p>\n<h3 id=\"CHANNELS\"><a href=\"#CHANNELS\" class=\"headerlink\" title=\"CHANNELS\"></a>CHANNELS</h3><p>非正式地，通道(channel)是指沿输入中的每个点的特征维度。例如，在图像中，对应于RGB组件的图像中的每个像素有三个通道。在使用卷积时，文本数据也可以采用类似的概念。从概念上讲，如果文本文档中的“像素”是单词，那么通道的数量就是词汇表的大小。如果我们更细粒度地考虑字符的卷积，通道的数量就是字符集的大小(在本例中刚好是词汇表)。在PyTorch卷积实现中，输入通道的数量是in_channels参数。卷积操作可以在输出(out_channels)中产生多个通道。您可以将其视为卷积运算符将输入特征维“映射”到输出特征维。下图说明了这个概念。<br><img src=\"https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596033.png\"><br><font color=\"grey\"><center> 卷积运算用两个输入矩阵（两个输入通道）表示相应的核也有两层，它将每层分别相乘，然后对结果求和。参数配置：input_channels&#x3D;2, output_channels&#x3D;1, kernel_size&#x3D;2, tride&#x3D;1, padding&#x3D;0, and dilation&#x3D;1.</center></font><br><img src=\"https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596054.png\"><br><font color=\"grey\"><center> 一种具有一个输入矩阵（一个输入通道）和两个卷积的卷积运算核（两个输出通道）。这些核分别应用于输入矩阵，并堆叠在输出张量。参数配置：input_channels&#x3D;1, output_channels&#x3D;2, kernel_size&#x3D;2, tride&#x3D;1,\tpadding&#x3D;0, and dilation&#x3D;1.</center></font></p>\n<p>很难立即知道有多少输出通道适合当前的问题。为了简化这个困难，我们假设边界是1,1,024——我们可以有一个只有一个通道的卷积层，也可以有一个只有1,024个通道的卷积层。现在我们有了边界，接下来要考虑的是有多少个输入通道。一种常见的设计模式是，从一个卷积层到下一个卷积层，通道数量的缩减不超过2倍。这不是一个硬性的规则，但是它应该让您了解适当数量的out_channels是什么样子的。</p>\n<h3 id=\"KERNEL-SIZE\"><a href=\"#KERNEL-SIZE\" class=\"headerlink\" title=\"KERNEL SIZE\"></a>KERNEL SIZE</h3><p>核矩阵的宽度称为核大小(PyTorch中的<code>kernel_size</code>)。我们分别给出一个2x2和3x3的核。卷积将输入中的空间(或时间)本地信息组合在一起，每个卷积的本地信息量由内核大小控制。然而，通过增加核的大小，也会减少输出的大小(Dumoulin和Visin, 2016)。这就是为什么当核大小为3时，输出矩阵2x2，而当核大小为2时，输出矩阵3x3。</p>\n<p><img src=\"https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596071.png\"><br><font color=\"grey\"><center> 将kernel_size&#x3D;3的卷积应用于输入矩阵。结果是一个折衷的结果：在每次将内核应用于矩阵时，都会使用更多的局部信息，但输出的大小会更小.</center></font></p>\n<p>此外，可以将NLP应用程序中核大小的行为看作类似于通过查看单词组捕获语言模式的n-gram的行为。使用较小的核大小，可以捕获较小的频繁模式，而较大的核大小会导致较大的模式，这可能更有意义，但是发生的频率更低。较小的核大小会导致输出中的细粒度特性，而较大的核大小会导致粗粒度特性。</p>\n<h3 id=\"STRIDE\"><a href=\"#STRIDE\" class=\"headerlink\" title=\"STRIDE\"></a>STRIDE</h3><p>Stride控制卷积之间的步长。如果步长与核相同，则内核计算不会重叠。另一方面，如果跨度为1，则内核重叠最大。输出张量可以通过增加步幅的方式被有意的压缩来总结信息，如下图所示。</p>\n<p><img src=\"https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596080.png\"><br><font color=\"grey\"><center> 应用于具有超参数步长的输入的kernel_size&#x3D;2的卷积核等于2。这会导致内核采取更大的步骤，从而产生更小的输出矩阵。对于更稀疏地对输入矩阵进行二次采样非常有用。</center></font></p>\n<h3 id=\"PADDING\"><a href=\"#PADDING\" class=\"headerlink\" title=\"PADDING\"></a>PADDING</h3><p>即使stride和kernel_size允许控制每个计算出的特征值有多大范围，它们也有一个有害的、有时是无意的副作用，那就是缩小特征映射的总大小(卷积的输出)。为了抵消这一点，输入数据张量被人为地增加了长度(如果是一维、二维或三维)、高度(如果是二维或三维)和深度(如果是三维)，方法是在每个维度上附加和前置0。这意味着CNN将执行更多的卷积，但是输出形状可以控制，而不会影响所需的核大小、步幅或扩展。下图展示了正在运行的填充。</p>\n<p><img src=\"https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596142.png\"><br><font color=\"grey\"><center> 应用于高度和宽度等于的输入矩阵的kernel_size&#x3D;2的卷积2。但是，由于填充（用深灰色正方形表示），输入矩阵的高度和宽度可以被放大。这通常与大小为3的内核一起使用，这样输出矩阵将等于输入矩阵的大小。</center></font></p>\n<h3 id=\"DILATION\"><a href=\"#DILATION\" class=\"headerlink\" title=\"DILATION\"></a>DILATION</h3><p>膨胀控制卷积核如何应用于输入矩阵。在下图中，我们显示，将膨胀从1(默认值)增加到2意味着当应用于输入矩阵时，核的元素彼此之间是两个空格。另一种考虑这个问题的方法是在核中跨跃——在核中的元素或核的应用之间存在一个step size，即存在“holes”。这对于在不增加参数数量的情况下总结输入空间的更大区域是有用的。当卷积层被叠加时，扩张卷积被证明是非常有用的。连续扩张的卷积指数级地增大了“接受域”的大小；即网络在做出预测之前所看到的输入空间的大小。</p>\n<p><img src=\"https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596131.png\"><br><font color=\"grey\"><center> 应用于超参数dilation&#x3D;2的输入矩阵的kernel_size&#x3D;2的卷积。从默认值开始膨胀的增加意味着核矩阵的元素在与输入矩阵相乘时进一步分散开来。进一步增大扩张会加剧这种扩散。</center></font></p>\n<h1 id=\"基于卷积神经网络的姓氏分类\"><a href=\"#基于卷积神经网络的姓氏分类\" class=\"headerlink\" title=\"基于卷积神经网络的姓氏分类\"></a>基于卷积神经网络的姓氏分类</h1><h2 id=\"Implementing-CNNs-in-PyTorch\"><a href=\"#Implementing-CNNs-in-PyTorch\" class=\"headerlink\" title=\"Implementing CNNs in PyTorch\"></a>Implementing CNNs in PyTorch</h2><p>在本节中，我们将通过端到端示例来利用上一节中介绍的概念。一般来说，神经网络设计的目标是找到一个能够完成任务的超参数组态。我们再次考虑在“示例:带有多层感知器的姓氏分类”中引入的现在很熟悉的姓氏分类任务，但是我们将使用CNNs而不是MLP。我们仍然需要应用最后一个线性层，它将学会从一系列卷积层创建的特征向量创建预测向量。这意味着目标是确定卷积层的配置，从而得到所需的特征向量。所有CNN应用程序都是这样的:首先有一组卷积层，它们提取一个feature map，然后将其作为上游处理的输入。在分类中，上游处理几乎总是应用线性(或fc)层。</p>\n<p>本课程中的实现遍历设计决策，以构建一个特征向量。我们首先构造一个人工数据张量，以反映实际数据的形状。数据张量的大小是三维的——这是向量化文本数据的最小批大小。如果你对一个字符序列中的每个字符使用onehot向量，那么onehot向量序列就是一个矩阵，而onehot矩阵的小批量就是一个三维张量。使用卷积的术语，每个onehot(通常是词汇表的大小)的大小是”input channels”的数量，字符序列的长度是“width”。</p>\n<p>在例4-14中，构造特征向量的第一步是将PyTorch的Conv1d类的一个实例应用到三维数据张量。通过检查输出的大小，你可以知道张量减少了多少。</p>\n<p>Example 4-14. Artificial data and using a Conv1d class</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SurnameDataset</span>(<span class=\"title class_ inherited__\">Dataset</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, index</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;PyTorch数据集的主要入口方法</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            index (int): 数据点的索引 </span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            一个字典，保存数据点的特征（x_data）和标签（y_target）</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        row = self._target_df.iloc[index]</span><br><span class=\"line\"></span><br><span class=\"line\">        surname_matrix = \\</span><br><span class=\"line\">            self._vectorizer.vectorize(row.surname)</span><br><span class=\"line\"></span><br><span class=\"line\">        nationality_index = \\</span><br><span class=\"line\">            self._vectorizer.nationality_vocab.lookup_token(row.nationality)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> &#123;<span class=\"string\">&#x27;x_surname&#x27;</span>: surname_matrix,</span><br><span class=\"line\">                <span class=\"string\">&#x27;y_nationality&#x27;</span>: nationality_index&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Vocabulary-Vectorizer-and-DataLoader-1\"><a href=\"#Vocabulary-Vectorizer-and-DataLoader-1\" class=\"headerlink\" title=\"Vocabulary, Vectorizer, and DataLoader\"></a>Vocabulary, Vectorizer, and DataLoader</h2><p>在本例中，尽管词汇表和DataLoader的实现方式与“示例:带有多层感知器的姓氏分类”中的示例相同，但Vectorizer的vectorize()方法已经更改，以适应CNN模型的需要。具体来说，正如我们在示例4-18中的代码中所示，该函数将字符串中的每个字符映射到一个整数，然后使用该整数构造一个由onehot向量组成的矩阵。重要的是，矩阵中的每一列都是不同的onehot向量。主要原因是，我们将使用的Conv1d层要求数据张量在第0维上具有批处理，在第1维上具有通道，在第2维上具有特性。</p>\n<p>除了更改为使用onehot矩阵之外，我们还修改了矢量化器，以便计算姓氏的最大长度并将其保存为max_surname_length</p>\n<p>Example 4-18. Implementing the Surname Vectorizer for CNNs</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SurnameVectorizer</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;协调词汇表并将它们应用到使用的向量化器&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, surname_vocab, nationality_vocab, max_surname_length</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            surname_vocab (Vocabulary): 将字符映射到整数的词汇表</span></span><br><span class=\"line\"><span class=\"string\">            nationality_vocab (Vocabulary): 将国籍映射到整数的词汇表</span></span><br><span class=\"line\"><span class=\"string\">            max_surname_length (int): 最长姓氏的长度</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        self.surname_vocab = surname_vocab</span><br><span class=\"line\">        self.nationality_vocab = nationality_vocab</span><br><span class=\"line\">        self._max_surname_length = max_surname_length</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">vectorize</span>(<span class=\"params\">self, surname</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            surname (str): 姓氏</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            one_hot_matrix (np.ndarray): 一个one-hot向量的矩阵</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        one_hot_matrix_size = (<span class=\"built_in\">len</span>(self.surname_vocab), self._max_surname_length)</span><br><span class=\"line\">        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)</span><br><span class=\"line\">                               </span><br><span class=\"line\">        <span class=\"keyword\">for</span> position_index, character <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(surname):</span><br><span class=\"line\">            character_index = self.surname_vocab.lookup_token(character)</span><br><span class=\"line\">            one_hot_matrix[character_index][position_index] = <span class=\"number\">1</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> one_hot_matrix</span><br></pre></td></tr></table></figure>\n<h2 id=\"Reimplementing-the-SurnameClassifier-with-Convolutional-Networks\"><a href=\"#Reimplementing-the-SurnameClassifier-with-Convolutional-Networks\" class=\"headerlink\" title=\"Reimplementing the SurnameClassifier with Convolutional Networks\"></a>Reimplementing the SurnameClassifier with Convolutional Networks</h2><p>我们在本例中使用的模型是使用我们在“卷积神经网络”中介绍的方法构建的。实际上，我们在该部分中创建的用于测试卷积层的“人工”数据与姓氏数据集中使用本例中的矢量化器的数据张量的大小完全匹配。正如在示例4-19中所看到的，它与我们在“卷积神经网络”中引入的Conv1d序列既有相似之处，也有需要解释的新添加内容。具体来说，该模型类似于“卷积神经网络”，它使用一系列一维卷积来增量地计算更多的特征，从而得到一个单特征向量。</p>\n<p>然而，本例中的新内容是使用sequence和ELU PyTorch模块。序列模块是封装线性操作序列的方便包装器。在这种情况下，我们使用它来封装Conv1d序列的应用程序。ELU是类似于实验3中介绍的ReLU的非线性函数，但是它不是将值裁剪到0以下，而是对它们求幂。ELU已经被证明是卷积层之间使用的一种很有前途的非线性(Clevert et al.， 2015)。</p>\n<p>在本例中，我们将每个卷积的通道数与num_channels超参数绑定。我们可以选择不同数量的通道分别进行卷积运算。这样做需要优化更多的超参数。我们发现256足够大，可以使模型达到合理的性能。</p>\n<p>Example 4-19. The CNN-based SurnameClassifier</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SurnameClassifier</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, initial_num_channels, num_classes, num_channels</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        创建一个姓氏分类器。</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            initial_num_channels (int): 输入特征向量的大小</span></span><br><span class=\"line\"><span class=\"string\">            num_classes (int): 输出预测向量的大小</span></span><br><span class=\"line\"><span class=\"string\">            num_channels (int): 网络中要始终使用的恒定通道大小</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(SurnameClassifier, self).__init__()</span><br><span class=\"line\">        </span><br><span class=\"line\">        self.convnet = nn.Sequential(</span><br><span class=\"line\">            nn.Conv1d(in_channels=initial_num_channels, </span><br><span class=\"line\">                      out_channels=num_channels, kernel_size=<span class=\"number\">3</span>),</span><br><span class=\"line\">            nn.ELU(),</span><br><span class=\"line\">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class=\"line\">                      kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">            nn.ELU(),</span><br><span class=\"line\">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class=\"line\">                      kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">            nn.ELU(),</span><br><span class=\"line\">            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, </span><br><span class=\"line\">                      kernel_size=<span class=\"number\">3</span>),</span><br><span class=\"line\">            nn.ELU()</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.fc = nn.Linear(num_channels, num_classes)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x_surname, apply_softmax=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;分类器的前向传播</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x_surname (torch.Tensor): 输入数据张量。</span></span><br><span class=\"line\"><span class=\"string\">                x_surname.shape 应为 (batch, initial_num_channels, max_surname_length)</span></span><br><span class=\"line\"><span class=\"string\">            apply_softmax (bool): softmax 激活的标志</span></span><br><span class=\"line\"><span class=\"string\">                如果与交叉熵损失一起使用，则应为 False</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            结果张量。tensor.shape 应为 (batch, num_classes)</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        features = self.convnet(x_surname).squeeze(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\">       </span><br><span class=\"line\">        prediction_vector = self.fc(features)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> apply_softmax:</span><br><span class=\"line\">            prediction_vector = F.softmax(prediction_vector, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> prediction_vector</span><br></pre></td></tr></table></figure>\n<h2 id=\"The-Training-Routine-1\"><a href=\"#The-Training-Routine-1\" class=\"headerlink\" title=\"The Training Routine\"></a>The Training Routine</h2><p>训练程序包括以下似曾相识的的操作序列:实例化数据集,实例化模型,实例化损失函数,实例化优化器,遍历数据集的训练分区和更新模型参数,遍历数据集的验证分区和测量性能,然后重复数据集迭代一定次数。此时，这是本书到目前为止的第三个训练例程实现，应该将这个操作序列内部化。对于这个例子，我们将不再详细描述具体的训练例程，因为它与“示例:带有多层感知器的姓氏分类”中的例程完全相同。但是，输入参数是不同的，可以在示例4-20中看到。</p>\n<p>Example 4-20. Input arguments to the CNN surname classifier</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">args = Namespace(</span><br><span class=\"line\">    <span class=\"comment\"># 数据和路径信息</span></span><br><span class=\"line\">    surname_csv=<span class=\"string\">&quot;data/surnames/surnames_with_splits.csv&quot;</span>,</span><br><span class=\"line\">    vectorizer_file=<span class=\"string\">&quot;vectorizer.json&quot;</span>,</span><br><span class=\"line\">    model_state_file=<span class=\"string\">&quot;model.pth&quot;</span>,</span><br><span class=\"line\">    save_dir=<span class=\"string\">&quot;model_storage/ch4/cnn&quot;</span>,</span><br><span class=\"line\">    <span class=\"comment\"># 模型超参数</span></span><br><span class=\"line\">    hidden_dim=<span class=\"number\">100</span>,</span><br><span class=\"line\">    num_channels=<span class=\"number\">256</span>,</span><br><span class=\"line\">    <span class=\"comment\"># 训练超参数</span></span><br><span class=\"line\">    seed=<span class=\"number\">1337</span>,</span><br><span class=\"line\">    learning_rate=<span class=\"number\">0.001</span>,</span><br><span class=\"line\">    batch_size=<span class=\"number\">128</span>,</span><br><span class=\"line\">    num_epochs=<span class=\"number\">100</span>,</span><br><span class=\"line\">    early_stopping_criteria=<span class=\"number\">5</span>,</span><br><span class=\"line\">    dropout_p=<span class=\"number\">0.1</span>,</span><br><span class=\"line\">    <span class=\"comment\"># 运行时选项</span></span><br><span class=\"line\">    cuda=<span class=\"literal\">False</span>,</span><br><span class=\"line\">    reload_from_files=<span class=\"literal\">False</span>,</span><br><span class=\"line\">    expand_filepaths_to_save_dir=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    catch_keyboard_interrupt=<span class=\"literal\">True</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Model-Evaluation-and-Prediction-1\"><a href=\"#Model-Evaluation-and-Prediction-1\" class=\"headerlink\" title=\"Model Evaluation and Prediction\"></a>Model Evaluation and Prediction</h2><p>要理解模型的性能，需要对性能进行定量和定性的度量。下面将描述这两个度量的基本组件。</p>\n<h3 id=\"Evaluating-on-the-Test-Dataset\"><a href=\"#Evaluating-on-the-Test-Dataset\" class=\"headerlink\" title=\"Evaluating on the Test Dataset\"></a>Evaluating on the Test Dataset</h3><p>正如“示例:带有多层感知器的姓氏分类”中的示例与本示例之间的训练例程没有变化一样，执行评估的代码也没有变化。总之，调用分类器的<code>eval()</code>方法来防止反向传播，并迭代测试数据集。与 MLP 约 50% 的性能相比，该模型的测试集性能准确率约为56%。尽管这些性能数字绝不是这些特定架构的上限，但是通过一个相对简单的CNN模型获得的改进应该足以让您在文本数据上尝试CNNs。</p>\n<h3 id=\"Classifying-or-retrieving-top-predictions-for-a-new-surname\"><a href=\"#Classifying-or-retrieving-top-predictions-for-a-new-surname\" class=\"headerlink\" title=\"Classifying or retrieving top predictions for a new surname\"></a>Classifying or retrieving top predictions for a new surname</h3><p>在本例中，<code>predict_nationality()</code>函数的一部分发生了更改，如例4-21:我们没有使用视图方法重塑新创建的数据张量以添加批处理维度，而是使用PyTorch的<code>unsqueeze()</code>函数在批处理应该在的位置添加大小为1的维度。相同的更改反映在<code>predict_topk_nationality()</code>函数中。</p>\n<p>Example 4-21. Using the trained model to make predictions</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict_nationality</span>(<span class=\"params\">surname, classifier, vectorizer</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;预测姓氏的国籍</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">        surname (str): 要分类的姓氏</span></span><br><span class=\"line\"><span class=\"string\">        classifier (SurnameClassifer): 分类器的实例</span></span><br><span class=\"line\"><span class=\"string\">        vectorizer (SurnameVectorizer): 相应的向量化器</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        dict: 包含最可能的国籍及其概率的字典</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 向量化姓氏</span></span><br><span class=\"line\">    vectorized_surname = vectorizer.vectorize(surname)</span><br><span class=\"line\">    <span class=\"comment\"># 将向量化的姓氏转换为PyTorch张量，并添加批次维度</span></span><br><span class=\"line\">    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 使用分类器进行预测，并对输出进行softmax处理</span></span><br><span class=\"line\">    result = classifier(vectorized_surname, apply_softmax=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 获取概率最高的国籍及其对应的索引</span></span><br><span class=\"line\">    probability_values, indices = result.<span class=\"built_in\">max</span>(dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">    index = indices.item()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 通过索引查找预测的国籍，并获取其概率值</span></span><br><span class=\"line\">    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)</span><br><span class=\"line\">    probability_value = probability_values.item()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 返回预测结果的字典</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> &#123;<span class=\"string\">&#x27;nationality&#x27;</span>: predicted_nationality, <span class=\"string\">&#x27;probability&#x27;</span>: probability_value&#125;</span><br></pre></td></tr></table></figure>\n<p>示例：<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-17.png?raw=true\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict_topk_nationality</span>(<span class=\"params\">surname, classifier, vectorizer, k=<span class=\"number\">5</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;预测姓氏的前K个可能的国籍</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">        surname (str): 要分类的姓氏</span></span><br><span class=\"line\"><span class=\"string\">        classifier (SurnameClassifer): 分类器的实例</span></span><br><span class=\"line\"><span class=\"string\">        vectorizer (SurnameVectorizer): 相应的向量化器</span></span><br><span class=\"line\"><span class=\"string\">        k (int): 要返回的前K个国籍数量</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        list: 字典的列表，每个字典包含一个国籍和其对应的概率</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 向量化姓氏</span></span><br><span class=\"line\">    vectorized_surname = vectorizer.vectorize(surname)</span><br><span class=\"line\">    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 使用分类器进行预测，并对输出进行softmax处理，获取前K个国籍及其概率</span></span><br><span class=\"line\">    prediction_vector = classifier(vectorized_surname, apply_softmax=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    probability_values, indices = torch.topk(prediction_vector, k=k)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 获取预测结果中概率最高的K个国籍及其概率值</span></span><br><span class=\"line\">    probability_values = probability_values[<span class=\"number\">0</span>].detach().numpy()</span><br><span class=\"line\">    indices = indices[<span class=\"number\">0</span>].detach().numpy()</span><br><span class=\"line\">    </span><br><span class=\"line\">    results = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> kth_index <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(k):</span><br><span class=\"line\">        <span class=\"comment\"># 通过索引查找国籍，并获取其对应的概率值，将结果存储为字典列表</span></span><br><span class=\"line\">        nationality = vectorizer.nationality_vocab.lookup_index(indices[kth_index])</span><br><span class=\"line\">        probability_value = probability_values[kth_index]</span><br><span class=\"line\">        results.append(&#123;<span class=\"string\">&#x27;nationality&#x27;</span>: nationality, </span><br><span class=\"line\">                        <span class=\"string\">&#x27;probability&#x27;</span>: probability_value&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> results</span><br><span class=\"line\"></span><br><span class=\"line\">new_surname = <span class=\"built_in\">input</span>(<span class=\"string\">&quot;Enter a surname to classify: &quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">k = <span class=\"built_in\">int</span>(<span class=\"built_in\">input</span>(<span class=\"string\">&quot;How many of the top predictions to see? &quot;</span>))</span><br><span class=\"line\"><span class=\"keyword\">if</span> k &gt; <span class=\"built_in\">len</span>(vectorizer.nationality_vocab):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Sorry! That&#x27;s more than the # of nationalities we have.. defaulting you to max size :)&quot;</span>)</span><br><span class=\"line\">    k = <span class=\"built_in\">len</span>(vectorizer.nationality_vocab)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># 调用函数进行预测并打印结果</span></span><br><span class=\"line\">predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Top &#123;&#125; predictions:&quot;</span>.<span class=\"built_in\">format</span>(k))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;===================&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> prediction <span class=\"keyword\">in</span> predictions:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;&#123;&#125; -&gt; &#123;&#125; (p=&#123;:0.2f&#125;)&quot;</span>.<span class=\"built_in\">format</span>(new_surname,</span><br><span class=\"line\">                                        prediction[<span class=\"string\">&#x27;nationality&#x27;</span>],</span><br><span class=\"line\">                                        prediction[<span class=\"string\">&#x27;probability&#x27;</span>]))</span><br></pre></td></tr></table></figure>\n<p>示例：<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/image-18.png?raw=true\"></p>\n<h2 id=\"Miscellaneous-Topics-in-CNNs\"><a href=\"#Miscellaneous-Topics-in-CNNs\" class=\"headerlink\" title=\"Miscellaneous Topics in CNNs\"></a>Miscellaneous Topics in CNNs</h2><p>为了结束我们的讨论，我们概述了几个其他的主题，这些主题是CNNs的核心，但在它们的共同使用中起着主要作用。特别是，你将看到Pooling操作、batch Normalization、network-in-network connection和residual connections的描述。</p>\n<h3 id=\"Pooling-Operation\"><a href=\"#Pooling-Operation\" class=\"headerlink\" title=\"Pooling Operation\"></a>Pooling Operation</h3><p>Pooling是将高维特征映射总结为低维特征映射的操作。卷积的输出是一个特征映射。feature map中的值总结了输入的一些区域。由于卷积计算的重叠性，许多计算出的特征可能是冗余的。Pooling是一种将高维(可能是冗余的)特征映射总结为低维特征映射的方法。在形式上，池是一种像sum、mean或max这样的算术运算符，系统地应用于feature map中的局部区域，得到的池操作分别称为sum pooling、average pooling和max pooling。池还可以作为一种方法，将较大但较弱的feature map的统计强度改进为较小但较强的feature map。下图说明了Pooling。</p>\n<p><img src=\"https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596120.png\"><br><font color=\"grey\"><center> 这里所示的池操作在功能上与卷积相同：它应用于输入矩阵中的不同位置。然而，池操作不是将输入矩阵的值相乘和求和，而是应用一些函数G来汇集这些值。G可以是任何运算，但求和、求最大值和计算平均值是最常见的。</center></font></p>\n<h3 id=\"Batch-Normalization-BatchNorm\"><a href=\"#Batch-Normalization-BatchNorm\" class=\"headerlink\" title=\"Batch Normalization (BatchNorm)\"></a>Batch Normalization (BatchNorm)</h3><p>批处理标准化是设计网络时经常使用的一种工具。BatchNorm对CNN的输出进行转换，方法是将激活量缩放为零均值和单位方差。它用于Z-transform的平均值和方差值每批更新一次，这样任何单个批中的波动都不会太大地移动或影响它。BatchNorm允许模型对参数的初始化不那么敏感，并且简化了学习速率的调整(Ioffe and Szegedy, 2015)。在PyTorch中，批处理规范是在nn模块中定义的。例4-22展示了如何用卷积和线性层实例化和使用批处理规范。</p>\n<p>Example 4-22. Using s Conv1D layer with batch normalization.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">    self.conv1 = nn.Conv1d(in_channels=<span class=\"number\">1</span>, out_channels=<span class=\"number\">10</span>,</span><br><span class=\"line\">                           kernel_size=<span class=\"number\">5</span>,</span><br><span class=\"line\">                           stride=<span class=\"number\">1</span>)</span><br><span class=\"line\">    self.conv1_bn = nn.BatchNorm1d(num_features=<span class=\"number\">10</span>)</span><br><span class=\"line\">    <span class=\"comment\"># ...</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">   <span class=\"comment\"># ...</span></span><br><span class=\"line\">   x = F.relu(self.conv1(x))</span><br><span class=\"line\">   x = self.conv1_bn(x)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Network-in-Network-Connections-1x1-Convolutions\"><a href=\"#Network-in-Network-Connections-1x1-Convolutions\" class=\"headerlink\" title=\"Network-in-Network Connections (1x1 Convolutions)\"></a>Network-in-Network Connections (1x1 Convolutions)</h3><p>Network-in-Network (NiN)连接是具有<code>kernel_size=1</code>的卷积内核，具有一些有趣的特性。具体来说，1x1卷积就像通道之间的一个完全连通的线性层。这在从多通道feature map映射到更浅的feature map时非常有用。在下图中，我们展示了一个应用于输入矩阵的NiN连接。它将两个通道简化为一个通道。因此，NiN或1x1卷积提供了一种廉价的方法来合并参数较少的额外非线性(Lin et al.， 2013)。</p>\n<p><img src=\"https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596110.png\"><br><font color=\"grey\"><center> 一个1×1卷积运算的例子。观察1×1卷积是如何进行的操作将通道数从两个减少到一个。</center></font></p>\n<h3 id=\"Residual-Connections-Residual-Block\"><a href=\"#Residual-Connections-Residual-Block\" class=\"headerlink\" title=\"Residual Connections&#x2F;Residual Block\"></a>Residual Connections&#x2F;Residual Block</h3><p>CNNs中最重要的趋势之一是Residual connection，它支持真正深层的网络(超过100层)。它也称为skip connection。如果将卷积函数表示为conv，则residual block的输出如下:</p>\n<p>$$output &#x3D; conv ( input ) + input$$</p>\n<p>然而，这个操作有一个隐含的技巧，如图下所示。对于要添加到卷积输出的输入，它们必须具有相同的形状。为此，标准做法是在卷积之前应用填充。在图4-15中，填充尺寸为1，卷积大小为3。</p>\n<p><img src=\"https://test.educg.net/userfiles/markdown/exp/2020_8/2034ll1597596102.png\"><br><font color=\"grey\"><center> 残差连接是一种将原始矩阵加到卷积输出上的方法。当将卷积层应用于输入矩阵并将结果添加到输入矩阵时，以上直观地描述了这一点。创建与输入大小相同的输出的通用超参数设置是让kernel_size&#x3D;3和padding&#x3D;1。一般来说，任何带 adding&#x3D;(floor(kernel_size)&#x2F;2-1) 的奇数内核大小都将导致与输入大小相同的输出。卷积层产生的矩阵被加到输入端，最后的结果是剩余连接计算的输出端。</center></font></p>\n","categories":["Deep Learning"]},{"title":"线性回归实验","url":"/2023/11/20/mllab1/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h1><ol>\n<li>   熟悉和掌握单变量线性回归算法</li>\n<li>   熟悉和掌握批处理梯度下降算法</li>\n<li>   熟悉和掌握多变量线性回归算法</li>\n</ol>\n<h1 id=\"实验要求\"><a href=\"#实验要求\" class=\"headerlink\" title=\"实验要求\"></a>实验要求</h1><ol>\n<li>   采用Python、Matlab等高级语言进行编程，推荐优先选用Python语言</li>\n<li>   核心模型和算法需自主编程实现，不得直接调用Scikit-learn、PyTorch等成熟框架的第三方实现</li>\n<li>   代码可读性强：变量、函数、类等命名可读性强，包含必要的注释</li>\n</ol>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理</h1><h2 id=\"一元线性回归模型\"><a href=\"#一元线性回归模型\" class=\"headerlink\" title=\"一元线性回归模型\"></a>一元线性回归模型<span id=\"more\"></span></h2><p>即 $D&#x3D;\\left\\{\\left(x_i, y_i\\right)\\right\\}_{i&#x3D;1}^m$, 其中 $x_i \\in R$. 对离散属性, 若属性值间存在 “序” 关系, 可以通过连续化将其转化为连续值。线性回归试图学得<br>$$<br>f\\left(x_i\\right)&#x3D;w x_i+b \\text {, 使得 } f\\left(x_i\\right) \\cong y_i<br>$$</p>\n<p>均方误差是回归任务中最常用的性能度量, 因此我们可试图让均方误差最小化, 即:<br>$$<br>\\left(w^*, b^*\\right)&#x3D;\\underset{(w, b)}{\\arg \\min } \\sum_{i&#x3D;1}^m\\left(f\\left(x_i\\right)-y_i\\right)^2&#x3D;\\underset{(w, b)}{\\arg \\min } \\sum_{i&#x3D;1}^m\\left(w x_i+b-y_i\\right)^2<br>$$</p>\n<p>均方误差对应了常用的欧几里得距离。基于均方误差最小化来进行模型求解的方法称为 “最小二乘法”。在线性回归中, 最小二乘法就是试图找到一条直线, 使所有样本到直线上的欧氏距离之和最小。</p>\n<p>求解 $w$ 和 $b$ 使 $E_{(w, b)}&#x3D;\\sum_{i&#x3D;1}^m\\left(w x_i+b-y_i\\right)^2$ 最小化的过程, 称为线性回归模型的最小二乘 “参数估计”。我们可将 $E_{(w, b)}$ 分别对 $w$ 和 $b$ 求导, 得到:<br>$$<br>\\begin{gathered}<br>\\frac{\\partial E_{(w, b)}}{\\partial w}&#x3D;2\\left(w \\sum_{i&#x3D;1}^m x_i^2-\\sum_{i&#x3D;1}^m\\left(y_i-b\\right) x_i\\right), \\\\<br>\\frac{\\partial E_{(w, b)}}{\\partial w}&#x3D;2\\left(m b-\\sum_{i&#x3D;1}^m\\left(y_i-w x_i\\right)\\right),<br>\\end{gathered}<br>$$</p>\n<p>然后令上式为 0 , 可以得到 $w$ 和 $b$ 最优解的闭式解:<br>$$<br>\\begin{gathered}<br>w&#x3D;\\frac{\\sum_{i&#x3D;1}^m y_i\\left(x_i-\\bar{x}\\right)}{\\sum_{i&#x3D;1}^m x_i^2-\\frac{1}{m}\\left(\\sum_{i&#x3D;1}^m x\\right)^2}, \\<br>b&#x3D;\\frac{1}{m} \\sum_{i&#x3D;1}^m\\left(y_i-w x_i\\right),<br>\\end{gathered}<br>$$</p>\n<p>其中, $\\bar{x}&#x3D;\\frac{1}{m} \\sum_{i&#x3D;1}^m x$ 为 $x$ 的均值。</p>\n<h2 id=\"多元线性回归模型\"><a href=\"#多元线性回归模型\" class=\"headerlink\" title=\"多元线性回归模型\"></a>多元线性回归模型</h2><p>样本由 $d$ 个属性描述, 此时我们试图学得<br>$$<br>f\\left(x_i\\right)&#x3D;w^T x_i+b \\text {, 使得 } f\\left(x_i\\right) \\cong y_i<br>$$</p>\n<p>类似的, 可利用最小二乘法来对 $w$ 和 $b$ 进行估计。<br>$$<br>X&#x3D;\\left(\\begin{array}{ccccc}<br>x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1 d} &amp; 1 \\\\<br>x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2 d} &amp; 1 \\\\<br>\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; 1 \\\\<br>x_{m 1} &amp; x_{m 2} &amp; \\cdots &amp; x_{m d} &amp; 1<br>\\end{array}\\right)&#x3D;\\left(\\begin{array}{cc}<br>x_1^T &amp; 1 \\\\<br>x_2^T &amp; 1 \\\\<br>\\vdots &amp; \\vdots \\\\<br>x_m^T &amp; 1<br>\\end{array}\\right)<br>$$</p>\n<p>再把标记也写成向量形式 $y&#x3D;\\left(y_1 ; y_2 ; \\ldots ; y_m\\right)$, 有<br>$$<br>\\widehat{w}^*&#x3D;\\underset{\\widehat{w}^*}{\\arg \\min } \\sum_{i&#x3D;1}^m(y-X \\widehat{w})^T(y-X \\widehat{w})<br>$$</p>\n<p>令 $E_{\\widehat{w}}&#x3D;(y-X \\widehat{w})^T(y-X \\widehat{w})$, 对 $\\widehat{w}$ 求导得到<br>$$<br>\\frac{\\partial E_{\\hat{w}}}{\\partial \\widehat{w}}&#x3D;2 X^T(X \\widehat{w}-y) \\text {. }<br>$$</p>\n<p>令上式为 0 可得 $\\widehat{w}$ 的最优解的闭式解, 但由于设计矩阵逆的计算, 比单变量情形要复杂一些。下面我们做一个简单的讨论。<br>当 $X^T X$ 为满秩矩阵或正定矩阵时, 令上式为 0 , 可得<br>$$<br>\\widehat{w}&#x3D;\\left(X^T X\\right)^{-1} X^T y<br>$$</p>\n<p>其中 $\\left(X^T X\\right)^{-1}$ 是矩阵 $\\left(X^T X\\right)$ 的逆矩阵, 令 $\\hat{x}_i&#x3D;\\left(x_i ; 1\\right)$. 则最终学得的多元线性回归模型为<br>$$<br>f\\left(\\hat{x}_i\\right)&#x3D;\\hat{x}_i^T\\left(X^T X\\right)^{-1} X^T y<br>$$</p>\n<p>然而, 现实任务中 $X^T X$ 往往不是满秩矩阵, 例如再许多任务中我们会遇到大量的变量,其数目甚至超过样例数, 导致 $X$ 的列数多于行数, $X^T X$ 显然不满秩。此时可解出多个 $\\widehat{w}$, 它们都能使均方误差最小化。常见的方法还有引入正则项。</p>\n<h1 id=\"实验内容\"><a href=\"#实验内容\" class=\"headerlink\" title=\"实验内容\"></a>实验内容</h1><h2 id=\"单变量线性回归\"><a href=\"#单变量线性回归\" class=\"headerlink\" title=\"单变量线性回归\"></a>单变量线性回归</h2><ol>\n<li>采用数据集 “data&#x2F;regress_data1.csv”进行单变量线性回归实验</li>\n<li>借助 matplotlib 画出原始数据分布的散点图（x&#x3D;”人口”, $y&#x3D;$ “收益”）</li>\n<li>以最小平方误差为目标函数, 构造模型的损失（误差）计算函数:<br>$$<br>J(w)&#x3D;\\frac{1}{2 m} \\sum_{i&#x3D;1}^m\\left(h\\left(x^{(i)}\\right)-y^{(i)}\\right)^2<br>$$</li>\n</ol>\n<p>其中, $h(x)&#x3D;w^T X&#x3D;w_0 x_0+w_1 x_1+w_2 x_2+\\ldots+w_n x_n$</p>\n<ol start=\"4\">\n<li><p>实现批量梯度下降算法（Batch Gradient Decent）用于优化线性回归模型:<br>$$<br>w_j&#x3D;w_j-\\alpha \\frac{1}{m} \\sum_{i&#x3D;1}^m \\frac{\\partial}{\\partial w_j} J(w)<br>$$<br>其中, $w_j$ 为参数向量, $m$ 样本量。<br>提示:<br>批量梯度下降算法可以参考 第 5 章神经网络的优化算法（P104）<br>线性回归模型的偏置 $\\mathrm{b}$ 可吸收进参数向量 $\\mathrm{w}$, 从而采用向量形式 (P55)</p>\n</li>\n<li><p>采用上述批量梯度下降法, 优化单变量线性回归模型。其中, 迭代轮数 epoch 设定为 1000 轮, 学习率设定为 0.01 , 参数初始化为 0 。</p>\n</li>\n</ol>\n<p>代码输出:</p>\n<ul>\n<li><p>优化结束时的损失值和模型参数</p>\n</li>\n<li><p>将模型拟合的直线和原始数据散点图画到同一张图中</p>\n</li>\n</ul>\n<h2 id=\"多变量线性回归\"><a href=\"#多变量线性回归\" class=\"headerlink\" title=\"多变量线性回归\"></a>多变量线性回归</h2><ol>\n<li>采用数据集 “data&#x2F;regress_data2.csv”进行多变量线性回归实验, 通过房子的大小和房间数量两个变量 回归房子的价格。</li>\n<li>对数据进行特征归一化： $z_i&#x3D;\\frac{x_i-\\mu}{\\sigma}$</li>\n<li>采用上述批量梯度下降法, 优化多变量线性回归模型。其中, 迭代轮数 epoch 设定为 1000 轮, 学习率设定为 0.01 , 参数初始化为 0 。</li>\n</ol>\n<p>代码输出:</p>\n<ul>\n<li><p>优化结束时的损失值和模型参数</p>\n</li>\n<li><p>画图输出训练误差（损失）随着迭代轮数 epoch 的变化曲线</p>\n</li>\n</ul>\n<h1 id=\"实验代码和结果\"><a href=\"#实验代码和结果\" class=\"headerlink\" title=\"实验代码和结果\"></a>实验代码和结果</h1><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">compute_loss</span>(<span class=\"params\">X, Y, theta</span>): <span class=\"comment\">#损失函数的计算</span></span><br><span class=\"line\">    m = Y.size</span><br><span class=\"line\">    loss = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, m):</span><br><span class=\"line\">        x = X[i, <span class=\"number\">1</span>]</span><br><span class=\"line\">        y = Y[i]</span><br><span class=\"line\">        loss += (y - (theta[<span class=\"number\">1</span>] * x + theta[<span class=\"number\">0</span>])) ** <span class=\"number\">2</span></span><br><span class=\"line\">    loss = loss / (<span class=\"number\">2.0</span> * m)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">gradient_descent</span>(<span class=\"params\">X, Y, theta, lr, epoch</span>): <span class=\"comment\">#BGD实现 </span></span><br><span class=\"line\">    m = Y.size</span><br><span class=\"line\">    t = theta.size</span><br><span class=\"line\">    J_history = np.zeros(epoch)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> <span class=\"built_in\">iter</span> <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epoch):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(t):</span><br><span class=\"line\">            theta[j] = theta[j] - lr * np.<span class=\"built_in\">sum</span>((np.dot(X, theta) - Y) * X[:, j]) / m</span><br><span class=\"line\"></span><br><span class=\"line\">        J_history[<span class=\"built_in\">iter</span>] = compute_loss(X, Y, theta)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> theta, J_history</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">path = <span class=\"string\">&#x27;regress_data1.csv&#x27;</span></span><br><span class=\"line\">data = pd.read_csv(path)</span><br><span class=\"line\">X = data[[<span class=\"string\">&#x27;人口&#x27;</span>]]</span><br><span class=\"line\">Y = data[[<span class=\"string\">&#x27;收益&#x27;</span>]].values.ravel()  <span class=\"comment\"># Flatten Y to 1D array</span></span><br><span class=\"line\">m = X.size</span><br><span class=\"line\">X = np.c_[np.ones(m), X]  <span class=\"comment\"># Add a column of ones for the bias term</span></span><br><span class=\"line\">theta = np.zeros(<span class=\"number\">2</span>)</span><br><span class=\"line\">epoch = <span class=\"number\">1000</span></span><br><span class=\"line\">lr = <span class=\"number\">0.01</span></span><br><span class=\"line\">theta, J_history = gradient_descent(X, Y, theta, lr, epoch)</span><br><span class=\"line\">ine1,=plt.plot(X[:,<span class=\"number\">1</span>],np.dot(X,theta),color=<span class=\"string\">&#x27;red&#x27;</span>)</span><br><span class=\"line\">plt.scatter(data[[<span class=\"string\">&#x27;人口&#x27;</span>]],data[[<span class=\"string\">&#x27;收益&#x27;</span>]]);plt.xlabel(<span class=\"string\">&quot;population&quot;</span>);plt.ylabel(<span class=\"string\">&quot;profit&quot;</span>);</span><br><span class=\"line\"><span class=\"comment\">#plt.plot(range(1000),J_history)</span></span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(theta,J_history[<span class=\"number\">999</span>])</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu.png?raw=true\" alt=\"alt text\"></p>\n<p>$$<br>\\begin{aligned}<br>&amp; b&#x3D;-3.25088222 \\\\<br>&amp; w&#x3D;1.12836314<br>\\end{aligned}<br>$$</p>\n<p>训练 1000 次后的损失为 4.514833339953508</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-1.png?raw=true\" alt=\"alt text\"></p>\n<p>我们只需对前面的代码做简单的修改即可用于多变量线性回归，将计算损失函数的改为</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">loss += (y - (theta[<span class=\"number\">1</span>] * x1 + theta[<span class=\"number\">0</span>]+theta[<span class=\"number\">2</span>]*x2)) ** <span class=\"number\">2</span></span><br></pre></td></tr></table></figure>\n<p>对数据进行归一化</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">X=(X-np.average(X))/np.std(X)</span><br><span class=\"line\">Y=(Y-np.average(Y))/np.std(Y)</span><br><span class=\"line\">Z=(Z-np.average(Z))/np.std(Z)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-2.png?raw=true\" alt=\"alt text\"></p>\n<p>$$<br>\\begin{aligned}<br>&amp; \\mathrm{b}&#x3D;-8.21860310 \\mathrm{e}-17 \\\\<br>&amp; \\mathrm{w} 1&#x3D;8.79149410 \\mathrm{e}-01 \\\\<br>&amp; \\mathrm{w} 2&#x3D;4.75747766 \\mathrm{e}-02<br>\\end{aligned}<br>$$</p>\n<p>迭代 1000 次的损失为 0.1335413413355335</p>\n<h1 id=\"小结或讨论\"><a href=\"#小结或讨论\" class=\"headerlink\" title=\"小结或讨论\"></a>小结或讨论</h1><p>线性回归是基于假设目标变量与特征变量之间存在线性关系，然后通过拟合最佳的线性函数来预测目标变量。</p>\n<p>线性回归建立了输入特征（自变量）与输出变量（因变量）之间的线性关系，通过拟合最佳的线性模型来预测输出变量的值。</p>\n<p>线性回归的主要目标是通过学习样本数据集中的特征和目标变量之间的线性关系，来进行预测和解释。线性回归可以用于预测连续数值型的目标变量，也可以用于分析特征对目标变量的影响程度和方向。线性回归的目标是找到最佳的权重和截距，使得模型的预测值与实际值之间的差异最小化。</p>\n<p>梯度下降法：①梯度下降法是一种迭代优化算法，通过不断调整模型参数来最小化损失函数。在线性回归中，梯度下降法通过计算损失函数对模型参数的梯度，并根据梯度的方向和大小更新参数值，直到达到最小化损失的目标。②梯度下降法使用学习率来控制每次迭代中参数的更新步长。学习率需要手动选择，并且影响算法的收敛速度和稳定性。③梯度下降法可能陷入局部最优解而不是全局最优解，特别是在非凸的损失函数中。这可以通过选择合适的学习率和初始化参数来缓解。④梯度下降法的收敛速度取决于学习率的选择和损失函数的形状。较小的学习率可能导致收敛速度较慢，而较大的学习率可能导致无法收敛或发散。⑤梯度下降法相对于最小二乘法对异常值具有一定的鲁棒性，但仍然可能受到大的异常值的影响。⑥梯度下降法可以主要分为批量梯度下降法、随机梯度下降法和mini-batch梯度下降法，这三种梯度下降法的选择取决于数据集的规模、计算资源的限制以及优化效果的要求。⑦梯度下降法主要适用于目标变量和特征变量之间存在近似线性关系，数据集规模较大，无法直接计算矩阵的逆矩阵，数据集中存在噪声或异常值等情况。</p>\n","categories":["Machine Learning"]},{"title":"基于改进UNet算法的农业图像分割","url":"/2024/05/23/mllab10/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"主要目标\"><a href=\"#主要目标\" class=\"headerlink\" title=\"主要目标\"></a>主要目标</h1><p>基于改进的U-Net算法实现农业遥感图像语义分割训练</p>\n<h1 id=\"实验过程\"><a href=\"#实验过程\" class=\"headerlink\" title=\"实验过程\"></a>实验过程</h1><h2 id=\"基本原理\"><a href=\"#基本原理\" class=\"headerlink\" title=\"基本原理\"></a>基本原理</h2><p>图像分割是将图片根据内容分割成不同的块，它需要对每个像素点进行分类，物体的轮廓是精准勾勒的，而不是像检测那样给出边界框。按照分割目的来进行划分，图像分割可以分为以下三种：</p>\n<p>普通分割：不关注物体是什么，只是划定不同物体之间的界限，将分属不同物体的像素区域划分开来，比如将前景和背景分开。</p>\n<p>语义分割：在普通分割的基础上，分类出每一块区域的标签，识别出该区域的物体。</p>\n<span id=\"more\"></span>\n<p>实例分割：在语义分割的基础上，将每个物体都分别开来，同类的物体也有不同的编号，比如识别场景中的不同的人。</p>\n<p>本实验目标：我们本次实验要做的就是对农业大数据图像进行语义分割，将不同区域的农作物进行分类。无论是哪种图像分割，整体流程都是差不多的，下面介绍一下图像分割的基本流程和训练过程。</p>\n<h3 id=\"图像分割的基本流程\"><a href=\"#图像分割的基本流程\" class=\"headerlink\" title=\"图像分割的基本流程\"></a>图像分割的基本流程</h3><h4 id=\"下采样与上采样\"><a href=\"#下采样与上采样\" class=\"headerlink\" title=\"下采样与上采样\"></a>下采样与上采样</h4><p>Convolution（卷积）：通过卷积层提取图像的多层次特征。<br>Deconvolution（反卷积）或Resize（重采样）：通过反卷积或重采样恢复图像的空间分辨率。</p>\n<h4 id=\"多尺度特征融合\"><a href=\"#多尺度特征融合\" class=\"headerlink\" title=\"多尺度特征融合\"></a>多尺度特征融合</h4><p>特征逐点相加：不同层的特征图逐点相加，融合多层特征。<br>特征channel维度拼接：不同层的特征图在channel维度拼接，保留多层特征信息。</p>\n<h4 id=\"获得像素级别的segmentation-map\"><a href=\"#获得像素级别的segmentation-map\" class=\"headerlink\" title=\"获得像素级别的segmentation map\"></a>获得像素级别的segmentation map</h4><p>对每一个像素点进行类别判断，生成每个像素点的分类标签。</p>\n<h3 id=\"训练过程\"><a href=\"#训练过程\" class=\"headerlink\" title=\"训练过程\"></a>训练过程</h3><p>输入：image + label（输入图像和标签）</p>\n<p>前向计算：out &#x3D; model(image)（通过模型进行前向计算，输出预测结果）</p>\n<p>计算损失：loss &#x3D; loss_func(out, label)（通过损失函数计算预测结果与标签之间的差异）</p>\n<p>反向传播：loss.backward()（计算梯度，进行反向传播）</p>\n<p>更新权重：optimizer.minimize(loss)（使用优化器更新模型权重）</p>\n<h3 id=\"导入组件包\"><a href=\"#导入组件包\" class=\"headerlink\" title=\"导入组件包\"></a>导入组件包</h3><p>在实现基于改进的U-Net算法进行农业遥感图像语义分割的过程中，需要导入一系列用于数据处理、可视化和模型训练的工具包。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib</span><br><span class=\"line\">matplotlib.use(<span class=\"string\">&quot;Agg&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Sequential  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers <span class=\"keyword\">import</span> *  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.utils.np_utils <span class=\"keyword\">import</span> to_categorical  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.preprocessing.image <span class=\"keyword\">import</span> img_to_array  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.callbacks <span class=\"keyword\">import</span> ModelCheckpoint  </span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> LabelEncoder  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> Model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> load_model</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.layers.merge <span class=\"keyword\">import</span> concatenate</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image  </span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt  </span><br><span class=\"line\"><span class=\"keyword\">import</span> cv2</span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm <span class=\"keyword\">import</span> tqdm  </span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class=\"string\">&quot;4&quot;</span></span><br><span class=\"line\">%matplotlib inline</span><br></pre></td></tr></table></figure>\n<h2 id=\"准备数据\"><a href=\"#准备数据\" class=\"headerlink\" title=\"准备数据\"></a>准备数据</h2><h3 id=\"大尺度农作物卫星遥感数据\"><a href=\"#大尺度农作物卫星遥感数据\" class=\"headerlink\" title=\"大尺度农作物卫星遥感数据\"></a>大尺度农作物卫星遥感数据</h3><p>数据来源于无人机遥感影像。无人机遥感测量技术作为空间信息技术的重要组成部分，既能作为星载遥感影像的重要补充，又能有效替代人工实地调查。凭借着降低地面人工调查强度和调查成本、快速获取实时高分辨数据的优势，它成为农业统计调查工作中的一大创新点，同时也是精准农业的重要方向之一。</p>\n<h3 id=\"数据来源地址\"><a href=\"#数据来源地址\" class=\"headerlink\" title=\"数据来源地址\"></a>数据来源地址</h3><p><a href=\"https://tianchi.aliyun.com/competition/entrance/231717/\">天池大赛 - 农作物遥感影像数据集</a></p>\n<h3 id=\"数据可视化\"><a href=\"#数据可视化\" class=\"headerlink\" title=\"数据可视化\"></a>数据可视化</h3><p>标注作物类型图片是一张unint8单通道图像。每个像素点值表示原始图片中对应位置所属类别，其中“烤烟”像素值 1，“玉米”像素值 2，“薏仁米”像素值 3，“人造建筑”像素值 4，其余所有位置视为“其他”像素值 0。</p>\n<p>随机选择训练集中的18张图像，并且利用matplotlib库对其进行可视化，展示数据集的基本情况。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">directory = <span class=\"string\">&quot;/root/data/Lab4/Lab4-data/UNetP_data/train_data&quot;</span></span><br><span class=\"line\">images = random.choices(os.listdir(directory), k=<span class=\"number\">18</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">fig = plt.figure(figsize=(<span class=\"number\">20</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\">columns = <span class=\"number\">6</span></span><br><span class=\"line\">rows = <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> x, i <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(images):</span><br><span class=\"line\">    path = os.path.join(directory, i)</span><br><span class=\"line\">    img = plt.imread(path)</span><br><span class=\"line\">    fig.add_subplot(rows, columns, x + <span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.imshow(img)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml.png?raw=true\" alt=\"alt text\"></p>\n<h3 id=\"设置数据和标签\"><a href=\"#设置数据和标签\" class=\"headerlink\" title=\"设置数据和标签\"></a>设置数据和标签</h3><p>这里采用labelEncoder方法进行标签的处理，标准化标签，将标签值统一转换成range范围内。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">width = <span class=\"number\">512</span>  </span><br><span class=\"line\">height = <span class=\"number\">512</span>  </span><br><span class=\"line\">classes = [<span class=\"number\">0.</span> ,  <span class=\"number\">1.</span>,  <span class=\"number\">2.</span>,   <span class=\"number\">3.</span>  , <span class=\"number\">4.</span>]  </span><br><span class=\"line\">labelencoder = LabelEncoder()  </span><br><span class=\"line\">labelencoder.fit(classes) </span><br></pre></td></tr></table></figure>\n<h3 id=\"图片加载函数\"><a href=\"#图片加载函数\" class=\"headerlink\" title=\"图片加载函数\"></a>图片加载函数</h3><p>对指定路径的图片进行加载，如果是灰度图，直接读取，如果是RGB图像，则对图像进行归一化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_img</span>(<span class=\"params\">path, grayscale=<span class=\"literal\">False</span></span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> grayscale:</span><br><span class=\"line\">        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        img = cv2.imread(path)</span><br><span class=\"line\">        img = np.array(img,dtype=<span class=\"string\">&quot;float&quot;</span>) / <span class=\"number\">255.0</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"定义训练参数和数据\"><a href=\"#定义训练参数和数据\" class=\"headerlink\" title=\"定义训练参数和数据\"></a>定义训练参数和数据</h3><p>设置随机数种子</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">seed = <span class=\"number\">7</span>  </span><br><span class=\"line\">np.random.seed(seed)  </span><br></pre></td></tr></table></figure>\n<p>设置验证集获取规则，将训练集的1&#x2F;4用作验证集。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_train_val</span>(<span class=\"params\">val_rate=<span class=\"number\">0.25</span></span>):</span><br><span class=\"line\">    train_url = []</span><br><span class=\"line\">    train_set = []</span><br><span class=\"line\">    val_set = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> pic <span class=\"keyword\">in</span> os.listdir(<span class=\"string\">&#x27;/root/data/Lab4/Lab4-data/UNetP_data/train_data/&#x27;</span>):</span><br><span class=\"line\">        train_url.append(pic)</span><br><span class=\"line\">    random.shuffle(train_url)</span><br><span class=\"line\">    total_num = <span class=\"built_in\">len</span>(train_url)</span><br><span class=\"line\">    val_num = <span class=\"built_in\">int</span>(val_rate * total_num)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(train_url)):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> i &lt; val_num:</span><br><span class=\"line\">            val_set.append(train_url[i])</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            train_set.append(train_url[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_set, val_set</span><br></pre></td></tr></table></figure>\n<p>设置训练集生成函数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">generateData</span>(<span class=\"params\">batch_size, data=[]</span>):</span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">        train_data = []</span><br><span class=\"line\">        train_label = []</span><br><span class=\"line\">        batch = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> (<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(data))):</span><br><span class=\"line\">            url = data[i]</span><br><span class=\"line\">            batch += <span class=\"number\">1</span></span><br><span class=\"line\">            img = load_img(<span class=\"string\">&#x27;/root/data/Lab4/Lab4-data/UNetP_data/train_data/&#x27;</span> + url)</span><br><span class=\"line\">            img = img_to_array(img)</span><br><span class=\"line\">            train_data.append(img)</span><br><span class=\"line\">            label = load_img(<span class=\"string\">&#x27;/root/data/Lab4/Lab4-data/UNetP_data/train_label/&#x27;</span> + url, grayscale=<span class=\"literal\">True</span>)</span><br><span class=\"line\">            label = img_to_array(label)</span><br><span class=\"line\">            train_label.append(label)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> batch % batch_size == <span class=\"number\">0</span>:</span><br><span class=\"line\">                train_data = np.array(train_data)</span><br><span class=\"line\">                train_label = np.array(train_label)</span><br><span class=\"line\">                <span class=\"keyword\">yield</span> (train_data, train_label)</span><br><span class=\"line\">                train_data = []</span><br><span class=\"line\">                train_label = []</span><br><span class=\"line\">                batch = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<p>设置验证集生成函数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">generateValidData</span>(<span class=\"params\">batch_size, data=[]</span>):</span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">        valid_data = []</span><br><span class=\"line\">        valid_label = []</span><br><span class=\"line\">        batch = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> (<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(data))):</span><br><span class=\"line\">            url = data[i]</span><br><span class=\"line\">            batch += <span class=\"number\">1</span></span><br><span class=\"line\">            img = load_img(<span class=\"string\">&#x27;/root/data/Lab4/Lab4-data/UNetP_data/train_data/&#x27;</span> + url)</span><br><span class=\"line\">            img = img_to_array(img)</span><br><span class=\"line\">            valid_data.append(img)</span><br><span class=\"line\">            label = load_img(<span class=\"string\">&#x27;/root/data/Lab4/Lab4-data/UNetP_data/train_label/&#x27;</span> + url, grayscale=<span class=\"literal\">True</span>)</span><br><span class=\"line\">            label = img_to_array(label)</span><br><span class=\"line\">            valid_label.append(label)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> batch % batch_size == <span class=\"number\">0</span>:</span><br><span class=\"line\">                valid_data = np.array(valid_data)</span><br><span class=\"line\">                valid_label = np.array(valid_label)</span><br><span class=\"line\">                <span class=\"keyword\">yield</span> (valid_data, valid_label)</span><br><span class=\"line\">                valid_data = []</span><br><span class=\"line\">                valid_label = []</span><br><span class=\"line\">                batch = <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"模型搭建训练和改进\"><a href=\"#模型搭建训练和改进\" class=\"headerlink\" title=\"模型搭建训练和改进\"></a>模型搭建训练和改进</h2><h3 id=\"UNet网络的介绍\"><a href=\"#UNet网络的介绍\" class=\"headerlink\" title=\"UNet网络的介绍\"></a>UNet网络的介绍</h3><p>Unet网络结构如其名呈现一个U字形，即由卷积和池化单元构成。左半边为编码器，即如传统的分类网络是“下采样阶段”；右半边为解码器，是“上采样阶段”。中间的灰色箭头为跳跃连接，将浅层的特征与深层的特征拼接。因为浅层通常可以抓取图像的一些简单的特征，比如边界、颜色，而深层经过的卷积操作多抓取到图像的一些抽象特征，将浅深同时利用起来会起到比较好的效果。Unet的效果图如下：</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-1.png?raw=true\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-2.png?raw=true\" alt=\"alt text\"></p>\n<h3 id=\"训练函数的定义\"><a href=\"#训练函数的定义\" class=\"headerlink\" title=\"训练函数的定义\"></a>训练函数的定义</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">mselect</span>): </span><br><span class=\"line\">    EPOCHS = <span class=\"number\">10</span></span><br><span class=\"line\">    BS = <span class=\"number\">16</span>  </span><br><span class=\"line\">    <span class=\"keyword\">if</span>(mselect==<span class=\"string\">&#x27;UNet&#x27;</span>):</span><br><span class=\"line\">        model = unet()</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        model = unet_new()</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(mselect==<span class=\"string\">&#x27;UNet&#x27;</span>):</span><br><span class=\"line\">        modelcheck = ModelCheckpoint(<span class=\"string\">&#x27;model.h5&#x27;</span>,monitor=<span class=\"string\">&#x27;val_acc&#x27;</span>,save_best_only=<span class=\"literal\">True</span>,mode=<span class=\"string\">&#x27;max&#x27;</span>)  </span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        modelcheck = ModelCheckpoint(<span class=\"string\">&#x27;/root/data/Lab4/Lab4-data/models/model_new_&#123;epoch:02d&#125;_&#123;val_accuracy:.2f&#125;.h5&#x27;</span>,monitor=<span class=\"string\">&#x27;val_acc&#x27;</span>,save_best_only=<span class=\"literal\">False</span>,mode=<span class=\"string\">&#x27;max&#x27;</span>,save_freq=<span class=\"string\">&#x27;epoch&#x27;</span>) </span><br><span class=\"line\">    <span class=\"built_in\">callable</span> = [modelcheck]  </span><br><span class=\"line\">    train_set,val_set = get_train_val()</span><br><span class=\"line\">    train_numb = <span class=\"built_in\">len</span>(train_set)  </span><br><span class=\"line\">    valid_numb = <span class=\"built_in\">len</span>(val_set)  </span><br><span class=\"line\">    <span class=\"built_in\">print</span> (<span class=\"string\">&quot;the number of train data is&quot;</span>,train_numb)  </span><br><span class=\"line\">    <span class=\"built_in\">print</span> (<span class=\"string\">&quot;the number of val data is&quot;</span>,valid_numb)</span><br><span class=\"line\">    H = model.fit_generator(generator=generateData(BS,train_set),steps_per_epoch=train_numb//BS,epochs=EPOCHS,verbose=<span class=\"number\">1</span>,  </span><br><span class=\"line\">                    validation_data=generateValidData(BS,val_set),validation_steps=valid_numb//BS,callbacks=<span class=\"built_in\">callable</span>,max_queue_size=<span class=\"number\">1</span>)  </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># plot the training loss and accuracy</span></span><br><span class=\"line\">    plt.style.use(<span class=\"string\">&quot;ggplot&quot;</span>)</span><br><span class=\"line\">    plt.figure()</span><br><span class=\"line\">    N = EPOCHS</span><br><span class=\"line\">    plt.plot(np.arange(<span class=\"number\">0</span>, N), H.history[<span class=\"string\">&quot;loss&quot;</span>], label=<span class=\"string\">&quot;train_loss&quot;</span>)</span><br><span class=\"line\">    plt.plot(np.arange(<span class=\"number\">0</span>, N), H.history[<span class=\"string\">&quot;val_loss&quot;</span>], label=<span class=\"string\">&quot;val_loss&quot;</span>)</span><br><span class=\"line\">    plt.plot(np.arange(<span class=\"number\">0</span>, N), H.history[<span class=\"string\">&quot;accuracy&quot;</span>], label=<span class=\"string\">&quot;train_acc&quot;</span>)</span><br><span class=\"line\">    plt.plot(np.arange(<span class=\"number\">0</span>, N), H.history[<span class=\"string\">&quot;val_accuracy&quot;</span>], label=<span class=\"string\">&quot;val_acc&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(mselect==<span class=\"string\">&#x27;UNet&#x27;</span>):</span><br><span class=\"line\">        plt.title(<span class=\"string\">&quot;Training Loss and Accuracy on U-Net Satellite Seg&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        plt.title(<span class=\"string\">&quot;Training Loss and Accuracy on U-Net_new Satellite Seg&quot;</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">&quot;Epoch #&quot;</span>)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">&quot;Loss/Accuracy&quot;</span>)</span><br><span class=\"line\">    plt.legend(loc=<span class=\"string\">&quot;lower left&quot;</span>)</span><br><span class=\"line\">    plt.savefig(<span class=\"string\">&#x27;/root/data/Lab4/Lab4-data/plot.png&#x27;</span>)</span><br><span class=\"line\">    plt.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"unet网络的定义\"><a href=\"#unet网络的定义\" class=\"headerlink\" title=\"unet网络的定义\"></a>unet网络的定义</h3><p>池化层采用最大池化，激活函数采用relu，上采样的线性插值方式采取keras的UpSampling2D方法</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">unet</span>():</span><br><span class=\"line\">    inputs = Input((<span class=\"number\">3</span>, width, height))</span><br><span class=\"line\"></span><br><span class=\"line\">    conv1 = Conv2D(<span class=\"number\">32</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(inputs)</span><br><span class=\"line\">    conv1 = Conv2D(<span class=\"number\">32</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(conv1)</span><br><span class=\"line\">    pool1 = MaxPooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv1)</span><br><span class=\"line\"></span><br><span class=\"line\">    conv2 = Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(pool1)</span><br><span class=\"line\">    conv2 = Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(conv2)</span><br><span class=\"line\">    pool2 = MaxPooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv2)</span><br><span class=\"line\"></span><br><span class=\"line\">    conv3 = Conv2D(<span class=\"number\">128</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(pool2)</span><br><span class=\"line\">    conv3 = Conv2D(<span class=\"number\">128</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(conv3)</span><br><span class=\"line\">    pool3 = MaxPooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv3)</span><br><span class=\"line\"></span><br><span class=\"line\">    conv4 = Conv2D(<span class=\"number\">256</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(pool3)</span><br><span class=\"line\">    conv4 = Conv2D(<span class=\"number\">256</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(conv4)</span><br><span class=\"line\">    pool4 = MaxPooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv4)</span><br><span class=\"line\"></span><br><span class=\"line\">    conv5 = Conv2D(<span class=\"number\">512</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(pool4)</span><br><span class=\"line\">    conv5 = Conv2D(<span class=\"number\">512</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(conv5)</span><br><span class=\"line\"></span><br><span class=\"line\">    up6 = concatenate([UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv5), conv4], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    conv6 = Conv2D(<span class=\"number\">256</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(up6)</span><br><span class=\"line\">    conv6 = Conv2D(<span class=\"number\">256</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(conv6)</span><br><span class=\"line\"></span><br><span class=\"line\">    up7 = concatenate([UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv6), conv3], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    conv7 = Conv2D(<span class=\"number\">128</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(up7)</span><br><span class=\"line\">    conv7 = Conv2D(<span class=\"number\">128</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(conv7)</span><br><span class=\"line\"></span><br><span class=\"line\">    up8 = concatenate([UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv7), conv2], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    conv8 = Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(up8)</span><br><span class=\"line\">    conv8 = Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(conv8)</span><br><span class=\"line\"></span><br><span class=\"line\">    up9 = concatenate([UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv8), conv1], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    conv9 = Conv2D(<span class=\"number\">32</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(up9)</span><br><span class=\"line\">    conv9 = Conv2D(<span class=\"number\">32</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&quot;relu&quot;</span>, padding=<span class=\"string\">&quot;same&quot;</span>)(conv9)</span><br><span class=\"line\"></span><br><span class=\"line\">    conv10 = Conv2D(<span class=\"number\">1</span>, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">&quot;sigmoid&quot;</span>)(conv9)</span><br><span class=\"line\">    <span class=\"comment\">#conv10 = Conv2D(n_label, (1, 1), activation=&quot;softmax&quot;)(conv9)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    model = Model(inputs=inputs, outputs=conv10)</span><br><span class=\"line\">    model.<span class=\"built_in\">compile</span>(optimizer=<span class=\"string\">&#x27;Adam&#x27;</span>, loss=<span class=\"string\">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])</span><br><span class=\"line\"><span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure>\n<h3 id=\"训练unet网络\"><a href=\"#训练unet网络\" class=\"headerlink\" title=\"训练unet网络\"></a>训练unet网络</h3><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-3.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-4.png?raw=true\" alt=\"alt text\"></p>\n<h3 id=\"模型改进\"><a href=\"#模型改进\" class=\"headerlink\" title=\"模型改进\"></a>模型改进</h3><p>我们对原来的unet网络增加了下采样层，并且设定了权重的随机初始化，池化方式更改为平均池化，并且在网络中加入了批规范化BatchNormalization的处理，基于梯度的训练过程可以更加有效的进行，即加快收敛速度，减轻梯度消失或爆炸导致的无法训练的问题，并且增加了Dropout的机制，有效的防止过拟合。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">unet_new</span>():</span><br><span class=\"line\"> </span><br><span class=\"line\">    inputs = Input((width, height, <span class=\"number\">3</span>))<span class=\"comment\">#改进</span></span><br><span class=\"line\">    conv1 = Conv2D(<span class=\"number\">8</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(inputs)</span><br><span class=\"line\">    pool1 = AveragePooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv1)  <span class=\"comment\"># 16</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv2 = BatchNormalization(momentum=<span class=\"number\">0.99</span>)(pool1)</span><br><span class=\"line\">    conv2 = Conv2D(<span class=\"number\">64</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv2)</span><br><span class=\"line\">    conv2 = BatchNormalization(momentum=<span class=\"number\">0.99</span>)(conv2)</span><br><span class=\"line\">    conv2 = Conv2D(<span class=\"number\">64</span>, <span class=\"number\">1</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv2)</span><br><span class=\"line\">    conv2 = Dropout(<span class=\"number\">0.02</span>)(conv2)</span><br><span class=\"line\">    pool2 = AveragePooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv2)  <span class=\"comment\"># 8</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv3 = BatchNormalization(momentum=<span class=\"number\">0.99</span>)(pool2)</span><br><span class=\"line\">    conv3 = Conv2D(<span class=\"number\">128</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv3)</span><br><span class=\"line\">    conv3 = BatchNormalization(momentum=<span class=\"number\">0.99</span>)(conv3)</span><br><span class=\"line\">    conv3 = Conv2D(<span class=\"number\">128</span>, <span class=\"number\">1</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv3)</span><br><span class=\"line\">    conv3 = Dropout(<span class=\"number\">0.02</span>)(conv3)</span><br><span class=\"line\">    pool3 = AveragePooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv3)  <span class=\"comment\"># 4</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv4 = BatchNormalization(momentum=<span class=\"number\">0.99</span>)(pool3)</span><br><span class=\"line\">    conv4 = Conv2D(<span class=\"number\">256</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv4)</span><br><span class=\"line\">    conv4 = BatchNormalization(momentum=<span class=\"number\">0.99</span>)(conv4)</span><br><span class=\"line\">    conv4 = Conv2D(<span class=\"number\">256</span>, <span class=\"number\">1</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv4)</span><br><span class=\"line\">    conv4 = Dropout(<span class=\"number\">0.02</span>)(conv4)</span><br><span class=\"line\">    pool4 = AveragePooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv4)</span><br><span class=\"line\"></span><br><span class=\"line\">    conv5 = BatchNormalization(momentum=<span class=\"number\">0.99</span>)(pool4)</span><br><span class=\"line\">    conv5 = Conv2D(<span class=\"number\">512</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv5)</span><br><span class=\"line\">    conv5 = BatchNormalization(momentum=<span class=\"number\">0.99</span>)(conv5)</span><br><span class=\"line\">    conv5 = Conv2D(<span class=\"number\">512</span>, <span class=\"number\">1</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv5)</span><br><span class=\"line\">    conv5 = Dropout(<span class=\"number\">0.02</span>)(conv5)</span><br><span class=\"line\">    pool4 = AveragePooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv4)</span><br><span class=\"line\"><span class=\"comment\"># conv5 = Conv2D(35, 3, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;, kernel_initializer=&#x27;he_normal&#x27;)(conv4)</span></span><br><span class=\"line\"><span class=\"comment\"># drop4 = Dropout(0.02)(conv5)</span></span><br><span class=\"line\">    pool4 = AveragePooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(pool3)  <span class=\"comment\"># 2</span></span><br><span class=\"line\">    pool5 = AveragePooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(pool4)  <span class=\"comment\"># 1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    conv6 = BatchNormalization(momentum=<span class=\"number\">0.99</span>)(pool5)</span><br><span class=\"line\">    conv6 = Conv2D(<span class=\"number\">256</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv6)</span><br><span class=\"line\"></span><br><span class=\"line\">    conv7 = Conv2D(<span class=\"number\">256</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv6)</span><br><span class=\"line\">    up7 = (UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv7))  <span class=\"comment\"># 2</span></span><br><span class=\"line\">    conv7 = Conv2D(<span class=\"number\">256</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(up7)</span><br><span class=\"line\">    merge7 = concatenate([pool4, conv7], axis=<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    conv8 = Conv2D(<span class=\"number\">128</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(merge7)</span><br><span class=\"line\">    up8 = (UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv8))  <span class=\"comment\"># 4</span></span><br><span class=\"line\">    conv8 = Conv2D(<span class=\"number\">128</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(up8)</span><br><span class=\"line\">    merge8 = concatenate([pool3, conv8], axis=<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    conv9 = Conv2D(<span class=\"number\">64</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(merge8)</span><br><span class=\"line\">    up9 = (UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv9))  <span class=\"comment\"># 8</span></span><br><span class=\"line\">    conv9 = Conv2D(<span class=\"number\">64</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(up9)</span><br><span class=\"line\">    merge9 = concatenate([pool2, conv9], axis=<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    conv10 = Conv2D(<span class=\"number\">32</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(merge9)</span><br><span class=\"line\">    up10 = (UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv10))  <span class=\"comment\"># 16</span></span><br><span class=\"line\">    conv10 = Conv2D(<span class=\"number\">32</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(up10)</span><br><span class=\"line\"></span><br><span class=\"line\">    conv11 = Conv2D(<span class=\"number\">16</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv10)</span><br><span class=\"line\">    up11 = (UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>))(conv11))  <span class=\"comment\"># 32</span></span><br><span class=\"line\">    conv11 = Conv2D(<span class=\"number\">8</span>, <span class=\"number\">3</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(up11)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># conv12 = Conv2D(3, 1, activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;, kernel_initializer=&#x27;he_normal&#x27;)(conv11)</span></span><br><span class=\"line\">    conv12 = Conv2D(<span class=\"number\">1</span>, <span class=\"number\">1</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, kernel_initializer=<span class=\"string\">&#x27;he_normal&#x27;</span>)(conv11)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = Model(<span class=\"built_in\">input</span>=inputs, output=conv12)</span><br><span class=\"line\">    <span class=\"comment\">#print(model.summary())</span></span><br><span class=\"line\">    model.<span class=\"built_in\">compile</span>(optimizer=<span class=\"string\">&#x27;adam&#x27;</span>, loss=<span class=\"string\">&#x27;mse&#x27;</span>, metrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])</span><br><span class=\"line\">      </span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure>\n<h3 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h3><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-5.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-6.png?raw=true\" alt=\"alt text\"></p>\n<h3 id=\"训练改进后的模型\"><a href=\"#训练改进后的模型\" class=\"headerlink\" title=\"训练改进后的模型\"></a>训练改进后的模型</h3><p>可以看到，经过改进后，训练函数的收敛迅速，并且准确率也得到了明显的提高</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-7.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-8.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-9.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"模型使用\"><a href=\"#模型使用\" class=\"headerlink\" title=\"模型使用\"></a>模型使用</h2><h3 id=\"引入组件包\"><a href=\"#引入组件包\" class=\"headerlink\" title=\"引入组件包\"></a>引入组件包</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> cv2</span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.preprocessing.image <span class=\"keyword\">import</span> img_to_array</span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.models <span class=\"keyword\">import</span> load_model</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> LabelEncoder  </span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class=\"string\">&quot;0&quot;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"设置测试数据的格式和标签\"><a href=\"#设置测试数据的格式和标签\" class=\"headerlink\" title=\"设置测试数据的格式和标签\"></a>设置测试数据的格式和标签</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">image_size = <span class=\"number\">512</span></span><br><span class=\"line\">classes = [<span class=\"number\">0.</span> ,  <span class=\"number\">1.</span>,  <span class=\"number\">2.</span>,   <span class=\"number\">3.</span>  , <span class=\"number\">4.</span>]</span><br><span class=\"line\">TEST_SET = [<span class=\"string\">&#x27;/root/data/Lab4/Lab4-data/test.png&#x27;</span>]</span><br><span class=\"line\">labelencoder = LabelEncoder()  </span><br><span class=\"line\">labelencoder.fit(classes) </span><br></pre></td></tr></table></figure>\n<h3 id=\"定义预测函数，加载预测模型\"><a href=\"#定义预测函数，加载预测模型\" class=\"headerlink\" title=\"定义预测函数，加载预测模型\"></a>定义预测函数，加载预测模型</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>():</span><br><span class=\"line\">    <span class=\"comment\"># load the trained convolutional neural network</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;[INFO] loading network...&quot;</span>)</span><br><span class=\"line\">    model = load_model(<span class=\"string\">&#x27;/root/data/Lab4/Lab4-data/model.h5&#x27;</span>)</span><br><span class=\"line\">    stride = <span class=\"number\">512</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> n <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(TEST_SET)):</span><br><span class=\"line\">        path = TEST_SET[n]</span><br><span class=\"line\">        <span class=\"comment\">#load the image</span></span><br><span class=\"line\">        image = cv2.imread(path)</span><br><span class=\"line\">        </span><br><span class=\"line\">        h,w,_ = image.shape</span><br><span class=\"line\">        </span><br><span class=\"line\">        padding_h = (h//stride + <span class=\"number\">1</span>) * stride </span><br><span class=\"line\">        padding_w = (w//stride + <span class=\"number\">1</span>) * stride</span><br><span class=\"line\">        padding_img = np.zeros((padding_h,padding_w,<span class=\"number\">3</span>),dtype=np.uint8)</span><br><span class=\"line\">        padding_img[<span class=\"number\">0</span>:h,<span class=\"number\">0</span>:w,:] = image[:,:,:]</span><br><span class=\"line\">        <span class=\"comment\">#padding_img = padding_img.astype(&quot;float&quot;) / 255.0</span></span><br><span class=\"line\">        padding_img = img_to_array(padding_img)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"built_in\">print</span> (<span class=\"string\">&#x27;src:&#x27;</span>,padding_img.shape)</span><br><span class=\"line\">        mask_whole = np.zeros((padding_h,padding_w),dtype=np.uint8)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(padding_h//stride):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(padding_w//stride):</span><br><span class=\"line\">                crop = padding_img[i*stride:i*stride+image_size,j*stride:j*stride+image_size,:<span class=\"number\">3</span>]<span class=\"comment\">#trick</span></span><br><span class=\"line\">               </span><br><span class=\"line\">                <span class=\"comment\">#_,ch,cw = crop.shape</span></span><br><span class=\"line\"><span class=\"comment\">#                 if ch != 512 or cw != 512:</span></span><br><span class=\"line\"><span class=\"comment\">#                     print (&#x27;invalid size!&#x27;)</span></span><br><span class=\"line\"><span class=\"comment\">#                     continue</span></span><br><span class=\"line\">                </span><br><span class=\"line\">                crop = np.expand_dims(crop, axis=<span class=\"number\">0</span>) </span><br><span class=\"line\">                </span><br><span class=\"line\">                <span class=\"comment\">#crop.transpose(0,3,2,1)</span></span><br><span class=\"line\">                </span><br><span class=\"line\">               <span class=\"comment\"># crop.reshape(1,3,512,512)</span></span><br><span class=\"line\">                <span class=\"comment\">#print(crop.shape)</span></span><br><span class=\"line\">                pred = model.predict(crop.transpose(<span class=\"number\">0</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>),verbose=<span class=\"number\">2</span>)</span><br><span class=\"line\">                <span class=\"comment\">#print (np.unique(pred))  </span></span><br><span class=\"line\">                pred = pred.reshape((<span class=\"number\">512</span>,<span class=\"number\">512</span>)).astype(np.uint8)</span><br><span class=\"line\">                <span class=\"comment\">#print &#x27;pred:&#x27;,pred.shape</span></span><br><span class=\"line\">                mask_whole[i*stride:i*stride+image_size,j*stride:j*stride+image_size] = pred[:,:]</span><br><span class=\"line\"></span><br><span class=\"line\">        </span><br><span class=\"line\">        cv2.imwrite(<span class=\"string\">&#x27;pre&#x27;</span>+<span class=\"built_in\">str</span>(n+<span class=\"number\">1</span>)+<span class=\"string\">&#x27;.png&#x27;</span>,mask_whole[<span class=\"number\">0</span>:h,<span class=\"number\">0</span>:w])</span><br></pre></td></tr></table></figure>\n<h3 id=\"生成预测结果\"><a href=\"#生成预测结果\" class=\"headerlink\" title=\"生成预测结果\"></a>生成预测结果</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> backend <span class=\"keyword\">as</span> K</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># K.set_image_dim_ordering(&#x27;th&#x27;)#trick</span></span><br><span class=\"line\">predict()</span><br></pre></td></tr></table></figure>\n<h3 id=\"图像分割结果可视化\"><a href=\"#图像分割结果可视化\" class=\"headerlink\" title=\"图像分割结果可视化\"></a>图像分割结果可视化</h3><p>加载原始图像</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">img_org = cv2.imread(<span class=\"string\">&#x27;/root/data/Lab4/Lab4-data/test.png&#x27;</span>, cv2.IMREAD_GRAYSCALE)</span><br><span class=\"line\">plt.imshow(img_org)</span><br><span class=\"line\">plt.savefig(<span class=\"string\">&#x27;img_org.png&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-10.png?raw=true\" alt=\"alt text\"><br>预测结果可视化</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 加载模型预测结果</span></span><br><span class=\"line\">img_label = cv2.imread(<span class=\"string\">&#x27;/root/data/Lab4/Lab4-data/pre1.png&#x27;</span>, cv2.IMREAD_GRAYSCALE)</span><br><span class=\"line\"><span class=\"comment\"># 像素放大</span></span><br><span class=\"line\">img_label*=<span class=\"number\">30</span></span><br><span class=\"line\">plt.imshow(img_label)</span><br><span class=\"line\">plt.savefig(<span class=\"string\">&#x27;img_label.png&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-11.png?raw=true\" alt=\"alt text\"></p>\n<p>合并模型结果与原始图像，可以看到模型比较成功地将植被与背景环境分割开来，基本达到了预期效果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">img_show = cv2.addWeighted(img_org, <span class=\"number\">0.1</span>, img_label, <span class=\"number\">0.8</span>, <span class=\"number\">0</span>,dtype = cv2.CV_32F) </span><br><span class=\"line\">plt.imshow(img_show)</span><br><span class=\"line\">plt.savefig(<span class=\"string\">&#x27;weight.png&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-12.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"实验总结\"><a href=\"#实验总结\" class=\"headerlink\" title=\"实验总结\"></a>实验总结</h1><p>我们可以看到，在unet中除最后一个层激活函数为sigmoid，其余均为relu，而默认的初始化方法为Xavier初始化，显然不满足神经元激活均值为0的假设，而unet_new的激活函数均为relu，采用了He初始化</p>\n<p>Unet采用的是2*2最大池化，对邻域内特征点取最大。</p>\n<p>正向传播：取邻域内最大，并记住最大值的索引位置。</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-14.png?raw=true\" alt=\"alt text\"></p>\n<p>反向传播：将特征值填充到正向传播中值最大的索引位置，其他位置补0</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-15.png?raw=true\" alt=\"alt text\"></p>\n<p>unet_new采用的是2*2的平均池化，邻域内特征点只求平均。</p>\n<p>正向传播：邻域内取平均。</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-16.png?raw=true\" alt=\"alt text\"></p>\n<p>反向传播：特征值根据邻域大小被平均，然后传给每个索引位置。</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-17.png?raw=true\" alt=\"alt text\"></p>\n<p>unet_new还采用了drop-out，对于一次迭代中的某一层神经网络，先随机选择中的一些神经元并将其临时隐藏(丢弃)，然后再进行本次训练和优化。在下一次迭代中，继续随机隐藏一些神经元，如此直至训练结束。由于是随机丢弃，故而每一个mini-batch都在训练不同的网络，可以有效防止过拟合。</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/ml-18.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"Xavier初始化\"><a href=\"#Xavier初始化\" class=\"headerlink\" title=\"Xavier初始化\"></a>Xavier初始化</h2><p>Xavier初始化，由Xavier Glorot 在2010年的论文 Understanding the difficulty of training deep feedforward neural networks 提出。为了避免梯度爆炸或者梯度消失，有两个经验性的准则:</p>\n<ol>\n<li>每一层神经元激活值的均值要保持为 0</li>\n<li>每一层激活的方差应该保持不变。</li>\n</ol>\n<p>在正向传播时，每层的激活值的方差保持不变；在反向传播时，每层的梯度值的方差保持不变。<br>基于上述的准则，初始的权值参数 $W^l$ ( $l$ 为网络的第 $l$ 层) 要符合以下公式<br>$$<br>\\begin{aligned}<br>W^{[l]} &amp; \\sim \\mathcal{N}\\left(\\mu&#x3D;0, \\sigma^2&#x3D;\\frac{1}{n^{[l-1]}}\\right) \\<br>b^{[l]} &amp; &#x3D;0<br>\\end{aligned}<br>$$</p>\n<p>其中 $n^{n-1}$ 是第 $l-1$ 层的神经元的个数。也就是说，初始的权值 $w$ 可以从均值 $\\mu&#x3D;0$ ，方差为 $\\sigma^2&#x3D;\\frac{1}{n^{l-1}}$ 的正态分布 $\\mathrm{Q}$ 中随机选取。</p>\n<h2 id=\"He初始化-MSRA\"><a href=\"#He初始化-MSRA\" class=\"headerlink\" title=\"He初始化 (MSRA)\"></a>He初始化 (MSRA)</h2><p>由 Kaiming 在论文Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification提出，由于Xavier的假设条件是激活函数是关于0对称的，而常用的ReLU激活函数并不能满足该条件。</p>\n<p>只考虑输入的个数，MSRA的初始化是一个均值为 0 ，方差为 $\\sqrt{\\frac{2}{F_{\\text {in }}}}$ 的高斯分布<br>$$<br>w \\sim G\\left[0, \\sqrt{\\frac{2}{F_{i n}}}\\right]<br>$$</p>\n","categories":["Machine Learning"]},{"title":"神经网络实验","url":"/2023/11/27/mllab2/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h1><ol>\n<li>   熟悉和掌握感知机神经网络</li>\n<li>   熟悉和掌握随机梯度下降算法</li>\n<li>   了解和掌握第三方机器学习库Scikit-learn中的模型调用</li>\n</ol>\n<h1 id=\"实验要求\"><a href=\"#实验要求\" class=\"headerlink\" title=\"实验要求\"></a>实验要求</h1><ol>\n<li>   采用Python、Matlab等高级语言进行编程，推荐优先选用Python语言</li>\n<li>   核心模型和算法需自主编程实现，不得直接调用Scikit-learn、PyTorch等成熟框架的第三方实现（除非实验内容明确指定调用）</li>\n<li>   代码可读性强：变量、函数、类等命名可读性强，包含必要的注释</li>\n</ol>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理<span id=\"more\"></span></h1><p>感知机（perceptron）是二分类的线性分类模型，属于监督学习算法。输入为实例的特征向量，输出为实例的类别（取+1和-1）。感知机旨在求出将输入空间中的实例划分为两类的分离超平面。为求得超平面，感知机导入了基于误分类的损失函数，利用梯度下降法对损失函数进行最优化求解。</p>\n<p>如果训练数据集是线性可分的，则感知机一定能求得分离超平面。如果是非线性可分的数据，则无法获得超平面。</p>\n<p>感知机具有简单而易于实现的优点，分为原始形式和对偶形式。感知机预测是用学习得到的感知机模型对新的实例进行预测的，因此属于判别模型。感知机是神经网络和支持向量机的基础。</p>\n<p>二分类模型: $f(x)&#x3D;\\operatorname{sign}(w * x+b)$</p>\n<p>损失函数: $L(w, b)&#x3D;-\\Sigma y_i(w *+b)$<br>算法: </p>\n<p>随即梯度下降法 Stochastic Gradient Descent </p>\n<p>随机抽取一个误分类点使其梯度下降。</p>\n<p>$w&#x3D;w+\\eta y i x i$</p>\n<p>$b&#x3D;b+\\eta y i$</p>\n<p>当实例点被误分类, 即位于分离超平面的错误侧, 则调整 $\\mathrm{w}, \\mathrm{b}$ 的值, 使分离超平面向该无分类点的一一侧移动，直 至误分类点被正确分类拿出 iris 数据集中两个分类的数据和[sepal length, sepal width]作为特征</p>\n<h1 id=\"实验内容\"><a href=\"#实验内容\" class=\"headerlink\" title=\"实验内容\"></a>实验内容</h1><h2 id=\"感知机神经网络分类\"><a href=\"#感知机神经网络分类\" class=\"headerlink\" title=\"感知机神经网络分类\"></a>感知机神经网络分类</h2><ol>\n<li>   从iris数据集中取出[sepal length，sepal width]两个属性作为样本特征，保持类别标记不变，训练单隐层感知机网络进行二分类实验。注意取前100个样本作为训练集，剩余的50个样本作为测试集。<ul>\n<li>Iris数据集介绍详见：<a href=\"https://archive.ics.uci.edu/dataset/53/iris\">https://archive.ics.uci.edu/dataset/53/iris</a></li>\n<li>Scikit-learn库中预装了Iris数据集，安装库后采用 “from sklearn.datasets import load_iris” 可以直接读取，参考<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html</a></li>\n</ul>\n</li>\n<li>   借助matplotlib 画出原始训练数据分布的散点图（x&#x3D;“sepal length”，y&#x3D;“sepal width”，点的颜色代表不同类别）</li>\n<li>按照下述模型和优化目标, 构造感知机模型和损失函数：<ul>\n<li>模型: $f(x)&#x3D;\\operatorname{sign}(w \\cdot x+b)$</li>\n<li>优化目标: $\\min <em>{w, b} L(w, b)&#x3D;-\\sum</em>{x_i \\in M} y_i\\left(w \\cdot x_i+b\\right)$</li>\n</ul>\n</li>\n<li>   编写适用于感知机的随机梯度下降算法（Stochastic Gradient Descent，SGD），对单隐层的感知机进行梯度下降<ul>\n<li>此数据集线性可分，可以设置迭代的停止条件为“直到训练集内没有误分类样本为止”</li>\n<li>学习率设置为0.1, 偏置初始化0，权重均初始化为1</li>\n<li>SGD 更新:<br>$$<br>\\begin{aligned}<br>&amp; w&#x3D;w+\\eta y_i x_i \\\\<br>&amp; b&#x3D;b+\\eta y_i<br>\\end{aligned}<br>$$<br>结果展示</li>\n</ul>\n</li>\n</ol>\n<ul>\n<li>将模型拟合的分类边界与上述原始数据点画到同一图中，观察训练效果</li>\n<li>用训练的模型对测试数据进行分类，得到测试错误率</li>\n<li>将模型拟合的分类边界与测试数据点画到同一图中，观察效果</li>\n</ul>\n<h2 id=\"神经网络调参\"><a href=\"#神经网络调参\" class=\"headerlink\" title=\"神经网络调参\"></a>神经网络调参</h2><p>调整学习率参数取值分别为[0.01, 0.05, 0.1, 0.5]运行模型，比较模型最后的测试正确率以及模型收敛所需要的训练轮数（epoch）数。</p>\n<h2 id=\"Sciki-learn机器学习库的调用\"><a href=\"#Sciki-learn机器学习库的调用\" class=\"headerlink\" title=\"Sciki-learn机器学习库的调用\"></a>Sciki-learn机器学习库的调用</h2><ol>\n<li>   直接调用机器学习库Scikit-learn中感知机模型（<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html</a> ），用上述训练和测试集训练进行训练， 采用上述类似的方法可视化训练集和验证集上模型的分类结果。（调用时保持模型超参数tol的取值为默认值0.001）</li>\n<li>   比较自己实现的模型和调用的Scikit-learn中模型在训练集上的可视化结果图，观察有何不同，分析产生不同的原因。</li>\n</ol>\n<h1 id=\"实验代码和结果\"><a href=\"#实验代码和结果\" class=\"headerlink\" title=\"实验代码和结果\"></a>实验代码和结果</h1><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Model</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.w = np.ones(<span class=\"built_in\">len</span>(data[<span class=\"number\">0</span>]) - <span class=\"number\">1</span>, dtype=np.float32)</span><br><span class=\"line\">        self.b = <span class=\"number\">0</span>  <span class=\"comment\"># 初始w/b的值</span></span><br><span class=\"line\">        self.epoch=<span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">sign</span>(<span class=\"params\">self, x, w, b</span>):</span><br><span class=\"line\">        y = np.dot(x, w) + b  <span class=\"comment\"># 求w，b的值</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 随机梯度下降法</span></span><br><span class=\"line\">    <span class=\"comment\"># 随机梯度下降法（SGD），随机抽取一个误分类点使其梯度下降。根据损失函数的梯度，对w，b进行更新</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">fit</span>(<span class=\"params\">self, X_train, y_train,lr</span>):  <span class=\"comment\"># 将参数拟合 X_train数据集矩阵 y_train特征向量</span></span><br><span class=\"line\">        is_wrong = <span class=\"literal\">False</span></span><br><span class=\"line\">        <span class=\"comment\"># 误分类点的意思就是开始的时候，超平面并没有正确划分，做了错误分类的数据。</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> <span class=\"keyword\">not</span> is_wrong:</span><br><span class=\"line\">            self.epoch +=<span class=\"number\">1</span>;</span><br><span class=\"line\">            wrong_count = <span class=\"number\">0</span>  <span class=\"comment\"># 误分为0，就不用循环，得到w，b</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(X_train)):</span><br><span class=\"line\">                X = X_train[d]</span><br><span class=\"line\">                y = y_train[d]</span><br><span class=\"line\">                <span class=\"keyword\">if</span> y * self.sign(X, self.w, self.b) &lt;= <span class=\"number\">0</span>:</span><br><span class=\"line\">                    <span class=\"comment\"># 如果某个样本出现分类错误，即位于分离超平面的错误侧，则调整参数，使分离超平面开始移动，直至误分类点被正确分类。</span></span><br><span class=\"line\">                    self.w = self.w + lr * np.dot(y, X)  <span class=\"comment\"># 调整w和b</span></span><br><span class=\"line\">                    self.b = self.b + lr * y</span><br><span class=\"line\">                    wrong_count += <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> wrong_count == <span class=\"number\">0</span>:</span><br><span class=\"line\">                is_wrong = <span class=\"literal\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&#x27;Perceptron Model!&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">test</span>(<span class=\"params\">self,X_test,y_test</span>):</span><br><span class=\"line\">        <span class=\"built_in\">all</span>=<span class=\"built_in\">len</span>(X_test)</span><br><span class=\"line\">        error=<span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">all</span>):</span><br><span class=\"line\">            X=X_test[d]</span><br><span class=\"line\">            y=y_test[d]</span><br><span class=\"line\">            <span class=\"keyword\">if</span> y * self.sign(X, self.w, self.b) &lt;= <span class=\"number\">0</span>:</span><br><span class=\"line\">                error+=<span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">return</span>(error/<span class=\"built_in\">all</span>)</span><br><span class=\"line\">iris = load_iris()</span><br><span class=\"line\">df = pd.DataFrame(iris.data, columns=iris.feature_names) </span><br><span class=\"line\">df[<span class=\"string\">&#x27;label&#x27;</span>] = iris.target</span><br><span class=\"line\">df.columns = [<span class=\"string\">&#x27;sepal length&#x27;</span>, <span class=\"string\">&#x27;sepal width&#x27;</span>, <span class=\"string\">&#x27;petal length&#x27;</span>, <span class=\"string\">&#x27;petal width&#x27;</span>, <span class=\"string\">&#x27;label&#x27;</span>]</span><br><span class=\"line\">first_40_rows = df.head(<span class=\"number\">40</span>)</span><br><span class=\"line\">rows_50_to_90 = df[<span class=\"number\">50</span>:<span class=\"number\">90</span>]</span><br><span class=\"line\">dt = pd.concat([first_40_rows, rows_50_to_90])</span><br><span class=\"line\">data = np.array(dt.iloc[:<span class=\"number\">90</span>, [<span class=\"number\">0</span>, <span class=\"number\">1</span>, -<span class=\"number\">1</span>]])</span><br><span class=\"line\">test1=df[<span class=\"number\">40</span>:<span class=\"number\">50</span>]</span><br><span class=\"line\">test2=df[<span class=\"number\">90</span>:<span class=\"number\">100</span>]</span><br><span class=\"line\">test=pd.concat([test1,test1])</span><br><span class=\"line\">test=np.array(test.iloc[:<span class=\"number\">100</span>, [<span class=\"number\">0</span>, <span class=\"number\">1</span>, -<span class=\"number\">1</span>]])</span><br><span class=\"line\">TX, Ty = test[:,:-<span class=\"number\">1</span>], test[:,-<span class=\"number\">1</span>]</span><br><span class=\"line\">Ty = np.array([<span class=\"number\">1</span> <span class=\"keyword\">if</span> i == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> Ty])</span><br><span class=\"line\"><span class=\"comment\">#print(len(Ty))</span></span><br><span class=\"line\"><span class=\"comment\">#print(data)</span></span><br><span class=\"line\">plt.scatter(df[:<span class=\"number\">40</span>][<span class=\"string\">&#x27;sepal length&#x27;</span>], df[:<span class=\"number\">40</span>][<span class=\"string\">&#x27;sepal width&#x27;</span>], label=<span class=\"string\">&#x27;0&#x27;</span>) </span><br><span class=\"line\">plt.scatter(df[<span class=\"number\">50</span>:<span class=\"number\">90</span>][<span class=\"string\">&#x27;sepal length&#x27;</span>], df[<span class=\"number\">50</span>:<span class=\"number\">90</span>][<span class=\"string\">&#x27;sepal width&#x27;</span>], label=<span class=\"string\">&#x27;1&#x27;</span>) </span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;sepal length&#x27;</span>)<span class=\"comment\">#给x坐标命名</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;sepal width&#x27;</span>)<span class=\"comment\">#给y坐标命名</span></span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.title(<span class=\"string\">&quot;train&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">X, y = data[:,:-<span class=\"number\">1</span>], data[:,-<span class=\"number\">1</span>]</span><br><span class=\"line\">y = np.array([<span class=\"number\">1</span> <span class=\"keyword\">if</span> i == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> y])</span><br><span class=\"line\"></span><br><span class=\"line\">perceptron = Model()</span><br><span class=\"line\">perceptron.fit(X, y,<span class=\"number\">0.1</span>)</span><br><span class=\"line\">x_points = np.linspace(<span class=\"number\">4</span>, <span class=\"number\">7</span>,<span class=\"number\">10</span>)</span><br><span class=\"line\">y_ = -(perceptron.w[<span class=\"number\">0</span>]*x_points + perceptron.b)/perceptron.w[<span class=\"number\">1</span>]</span><br><span class=\"line\">plt.plot(x_points, y_)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"><span class=\"comment\">#print(perceptron.epoch)</span></span><br><span class=\"line\">er=perceptron.test(TX,Ty)</span><br><span class=\"line\">plt.scatter(df[<span class=\"number\">40</span>:<span class=\"number\">50</span>][<span class=\"string\">&#x27;sepal length&#x27;</span>], df[<span class=\"number\">40</span>:<span class=\"number\">50</span>][<span class=\"string\">&#x27;sepal width&#x27;</span>], label=<span class=\"string\">&#x27;0&#x27;</span>)</span><br><span class=\"line\">plt.scatter(df[<span class=\"number\">90</span>:<span class=\"number\">100</span>][<span class=\"string\">&#x27;sepal length&#x27;</span>], df[<span class=\"number\">90</span>:<span class=\"number\">100</span>][<span class=\"string\">&#x27;sepal width&#x27;</span>], label=<span class=\"string\">&#x27;1&#x27;</span>)</span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.title(<span class=\"string\">&quot;test&quot;</span>)</span><br><span class=\"line\">plt.plot(x_points, y_)</span><br><span class=\"line\"><span class=\"comment\">#plt.show()</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(er)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-3.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-4.png?raw=true\" alt=\"alt text\"></p>\n<p>学习率为0.1时，训练了13次，错误率为0.1</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-5.png?raw=true\" alt=\"alt text\"></p>\n<p>学习率为0.01时，训练了62次，错误率为0.1</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-6.png?raw=true\" alt=\"alt text\"></p>\n<p>学习率为0.05时，训练了16次，错误率为0.1</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-7.png?raw=true\" alt=\"alt text\"></p>\n<p>学习率为0. 5时，训练了13次，错误率为0.1</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class=\"number\">0.8</span>, random_state=<span class=\"number\">42</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用Scikit-learn中的Perceptron模型进行训练</span></span><br><span class=\"line\">perceptron = Perceptron(tol=<span class=\"number\">0.001</span>)</span><br><span class=\"line\">perceptron.fit(X_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 预测测试集</span></span><br><span class=\"line\">y_pred = perceptron.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算测试集准确率</span></span><br><span class=\"line\">accuracy = accuracy_score(y_test, y_pred)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;测试集的准确率: <span class=\"subst\">&#123;accuracy&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">plt.scatter(X_train[y_train == <span class=\"number\">1</span>][:, <span class=\"number\">0</span>], X_train[y_train == <span class=\"number\">1</span>][:, <span class=\"number\">1</span>], label=<span class=\"string\">&#x27;0&#x27;</span>)</span><br><span class=\"line\">plt.scatter(X_train[y_train == -<span class=\"number\">1</span>][:, <span class=\"number\">0</span>], X_train[y_train == -<span class=\"number\">1</span>][:, <span class=\"number\">1</span>], label=<span class=\"string\">&#x27;1&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;sepal length&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;sepal width&#x27;</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;Training Set&#x27;</span>)</span><br><span class=\"line\">x_min, x_max = X_train[:, <span class=\"number\">0</span>].<span class=\"built_in\">min</span>() - <span class=\"number\">1</span>, X_train[:, <span class=\"number\">0</span>].<span class=\"built_in\">max</span>() + <span class=\"number\">1</span></span><br><span class=\"line\">x_points = np.linspace(x_min, x_max, <span class=\"number\">10</span>)</span><br><span class=\"line\">y_ = -(perceptron.coef_[<span class=\"number\">0</span>][<span class=\"number\">0</span>] * x_points + perceptron.intercept_) / perceptron.coef_[<span class=\"number\">0</span>][<span class=\"number\">1</span>]</span><br><span class=\"line\">plt.plot(x_points, y_, color=<span class=\"string\">&#x27;blue&#x27;</span>)</span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-8.png?raw=true\" alt=\"alt text\"><br>准确率为0.9875</p>\n<p>结果不同可能是由于学习率设置不同，以及随机种子的差异</p>\n<h1 id=\"小结或讨论\"><a href=\"#小结或讨论\" class=\"headerlink\" title=\"小结或讨论\"></a>小结或讨论</h1><p>感知器是人工神经网络中的一种典型结构， 它的主要的特点是结构简单。它是一种分类学习器，是很多复杂算法的基础。其“赏罚概念”在机器学习算法在中广为应用。在分类正确时，对正确的权重向量w奖赏，即w不变；当分类错误时，对权重向量惩罚，即将权重向量w向着争取的方向转变</p>\n<p>Perceptron 由 Frank Rosenblatt 升发, 并于 1962 年出版的“神经动力学原理：感知器和脑机制理论”一文中所提出。当时, Rosenblatt 的文章被 Marvin Minksy 和 Seymour Papert 反对，认为神经网络有缺陷，只能解决线性分类问题。然而, 这种限制仅发生在单层神经网络中。Perceptron 可用于解决二分类问题。在传统线性可分的二分类情况下, 可以使 $w^T x&gt;&#x3D;0$ 时分类为正样本, $w^T x&lt;0$ 分类为负样本。</p>\n<p>算法步骤为对所有负样本乘以 -1 以方便算法流程, 即使 $w^T x&gt;&#x3D;0$ 时判断为分类正确。</p>\n<p>随机生成初始权重向量 $w 0$, 在每轮迭代中, 若样本 $i$ 分类正确即 $w^T x_i&gt;&#x3D;0$ 时,不对 $w$ 进行修改; 当本轮迭代中, 针对样本 $i$ 出现分类错误, 即  $w^T xi&lt;0$ 时, 对权重向量 $w$ 惩罚, 使之朝着正确的趋势改进。 $\\eta$ 为学习率。如果无错误分类，则迭代结束</p>\n","categories":["Machine Learning"]},{"title":"支持向量机实验","url":"/2023/12/04/mllab3/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h1><ol>\n<li>   熟悉和掌握支持向量机</li>\n<li>   熟悉和掌握核函数处理非线性性问题</li>\n<li>   了解和掌握第三方机器学习库Scikit-learn中的模型调用</li>\n</ol>\n<h1 id=\"实验要求\"><a href=\"#实验要求\" class=\"headerlink\" title=\"实验要求\"></a>实验要求</h1><ol>\n<li>   采用Python、Matlab等高级语言进行编程，推荐优先选用Python语言</li>\n<li>   本次实验可以直接调用Scikit-learn、PyTorch等成熟框架的第三方实现</li>\n<li>   代码可读性强：变量、函数、类等命名可读性强，包含必要的注释</li>\n</ol>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理<span id=\"more\"></span></h1><p>支持向量机(Support Vector Machine)是Cortes和Vapnik于1995年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。</p>\n<p>小样本，并不是说样本的绝对数量少（实际上，对任何算法来说，更多的样本几乎总是能带来更好的效果），而是说与问题的复杂度比起来，SVM算法要求的样本数是相对比较少的。</p>\n<p>非线性，是指SVM擅长应付样本数据线性不可分的情况，主要通过松弛变量（也有人叫惩罚变量）和核函数技术来实现，这一部分是SVM的精髓，以后会详细讨论。</p>\n<p>高维模式识别是指样本维数很高，例如文本的向量表示，如果没有经过另一系列文章（《文本分类入门》）中提到过的降维处理，出现几万维的情况很正常，其他算法基本就没有能力应付了，SVM却可以，主要是因为SVM 产生的分类器很简洁，用到的样本信息很少（仅仅用到那些称之为“支持向量”的样本），使得即使样本维数很高，也不会给存储和计算带来大麻烦</p>\n<p>目前在引入SVM概述时主要有两个方式：</p>\n<p>一是：</p>\n<p>支持向量机方法是建立在统计学习理论的VC 维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性（即对特定训练样本的学习精度，Accuracy）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折衷，以期获得最好的推广能力（或称泛化能力）。</p>\n<p>所谓VC维是对函数类的一种度量，可以简单的理解为问题的复杂程度，VC维越高，一个问题就越复杂。正是因为SVM关注的是VC维，后面我们可以看到，SVM解决问题的时候，和样本的维数是无关的（甚至样本是上万维的都可以，这使得SVM很适合用来解决文本分类的问题，当然，有这样的能力也因为引入了核函数）。</p>\n<p>结构风险是什么呢？机器学习本质上就是一种对问题真实模型的逼近，但毫无疑问，真实模型一定是不知道的。我们选择了一个假设之后，真实误差无从得知，但我们可以用某些可以掌握的量来逼近它。最直观的想法就是使用分类器在样本数据上的分类的结果与真实结果之间的差值来表示。这个差值叫做经验风险Remp(w)。此时的情况便是选择了一个足够复杂的分类函数，能够精确的记住每一个样本，但对样本之外的数据一律分类错误。统计学习因此而引入了泛化误差界的概念，就是指真实风险应该由两部分内容刻画，一是经验风险，代表了分类器在给定样本上的误差；二是置信风险，代表了我们在多大程度上可以信任分类器在未知数据分类的结果。很显然，第二部分是没有办法精确计算的，因此只能给出一个估计的区间，也使得整个误差只能计算上界，而无法计算准确的值（所以叫做泛化误差界，而不叫泛化误差）。</p>\n<p>置信风险与两个量有关，一是样本数量，显然给定的样本数量越大，我们的学习结果越有可能正确，此时置信风险越小；二是分类函数的VC维，显然VC维越大，推广能力越差，置信风险会变大。</p>\n<p>泛化误差界的公式为: $\\mathrm{R}(\\mathrm{w}) \\leqslant \\operatorname{Remp}(\\mathrm{w})+\\Phi(\\mathrm{n} &#x2F; \\mathrm{h})$ 。公式中 $\\mathrm{R}(\\mathrm{w})$ 就是真实风险, $\\operatorname{Remp}(\\mathrm{w})$ 就是经验风险, $\\Phi(\\mathrm{n} &#x2F; \\mathrm{h})$ 就是置信风险。统计学习的目标从经验风险最小化变为了寻求经验风险与置信风险的和最小, 即结构风险最小。SVM 正是这样一种努力最小化结构风险的算法。</p>\n<p>从logistic回归出发，引出了SVM，既揭示了模型间的联系，也让人觉得过渡更自然。</p>\n<p>Logistic回归目的是从特征学习出一个0&#x2F;1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y&#x3D;1的概率。</p>\n<h1 id=\"实验内容\"><a href=\"#实验内容\" class=\"headerlink\" title=\"实验内容\"></a>实验内容</h1><h2 id=\"iris数据集支持向量机分类实验\"><a href=\"#iris数据集支持向量机分类实验\" class=\"headerlink\" title=\"iris数据集支持向量机分类实验\"></a>iris数据集支持向量机分类实验</h2><ol>\n<li>   从iris数据集中取出[sepal length，sepal width]两个属性作为样本特征，取前两类样本训练支持向量机进行二分类实验。注意每一类取前30个样本作为训练集，剩余的20个样本作为测试集。<ul>\n<li>Iris数据集介绍详见：<a href=\"https://archive.ics.uci.edu/dataset/53/iris\">https://archive.ics.uci.edu/dataset/53/iris</a> </li>\n<li>Scikit-learn库中预装了Iris数据集，安装库后采用 “from sklearn.datasets import load_iris” 可以直接读取，参考：<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html</a></li>\n</ul>\n</li>\n<li>   借助matplotlib 画出原始训练数据分布的散点图（x&#x3D;“sepal length”，y&#x3D;“sepal width”，点的颜色代表不同类别）</li>\n<li>   调用sklearn的SVM模型包，实现分类算法</li>\n</ol>\n<p>结果展示：</p>\n<ul>\n<li>用训练的模型对测试数据进行分类，得到测试错误率</li>\n<li>使用不同的误差惩罚系数取值（分别为0.01、0.1、1.0、10和100）和核函数（分别为“linear”和“poly”）来组合，运行模型，并比较模型在最后的测试正确率以及可视化测试数据的SVM决策间隔。（提示：可调用sklearn库中的DecisionBoundaryDisplay）。</li>\n</ul>\n<h2 id=\"基于核函数的SVM非线性分类实验\"><a href=\"#基于核函数的SVM非线性分类实验\" class=\"headerlink\" title=\"基于核函数的SVM非线性分类实验\"></a>基于核函数的SVM非线性分类实验</h2><ol>\n<li>   利用sklearn生成非线性数据<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_moons</span><br><span class=\"line\">x, y = make_moons(n_samples=<span class=\"number\">100</span>, noise=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></li>\n<li>   借助matplotlib 画出生成非线性数据的散点图</li>\n<li>   分别选取核函数（“linear”,“poly”,”rbf”）,根据API手册了解不同核函数对应的超参数，通过调节超参数展示绘制的决策平面。参考<a href=\"sklearn.svm.SVR-scikit-learn\">sklearn API</a></li>\n</ol>\n<p>结果展示：展示使用三种不同核函数，在不同超参数下的决策平面</p>\n<h2 id=\"手写SVM模型\"><a href=\"#手写SVM模型\" class=\"headerlink\" title=\"手写SVM模型\"></a>手写SVM模型</h2><p>尝试手写SVM模型，实现鸢尾花数据集分类</p>\n<h1 id=\"实验代码和结果\"><a href=\"#实验代码和结果\" class=\"headerlink\" title=\"实验代码和结果\"></a>实验代码和结果</h1><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC, LinearSVC</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\">iris = load_iris()</span><br><span class=\"line\">df = pd.DataFrame(iris.data, columns=iris.feature_names)</span><br><span class=\"line\">df[<span class=\"string\">&#x27;label&#x27;</span>] = iris.target</span><br><span class=\"line\">df.columns = [<span class=\"string\">&#x27;sepal length&#x27;</span>, <span class=\"string\">&#x27;sepal width&#x27;</span>, <span class=\"string\">&#x27;petal length&#x27;</span>, <span class=\"string\">&#x27;petal width&#x27;</span>, <span class=\"string\">&#x27;label&#x27;</span>]</span><br><span class=\"line\">first_40_rows = df.head(<span class=\"number\">30</span>)</span><br><span class=\"line\">rows_50_to_90 = df[<span class=\"number\">50</span>:<span class=\"number\">80</span>]</span><br><span class=\"line\">dt = pd.concat([first_40_rows, rows_50_to_90])</span><br><span class=\"line\">data = np.array(dt.iloc[:<span class=\"number\">80</span>, [<span class=\"number\">0</span>, <span class=\"number\">1</span>, -<span class=\"number\">1</span>]])</span><br><span class=\"line\">test1=df[<span class=\"number\">30</span>:<span class=\"number\">50</span>]</span><br><span class=\"line\">test2=df[<span class=\"number\">80</span>:<span class=\"number\">100</span>]</span><br><span class=\"line\">test=pd.concat([test1,test1])</span><br><span class=\"line\">test=np.array(test.iloc[:<span class=\"number\">100</span>, [<span class=\"number\">0</span>, <span class=\"number\">1</span>, -<span class=\"number\">1</span>]])</span><br><span class=\"line\">TX, Ty = test[:,:-<span class=\"number\">1</span>], test[:,-<span class=\"number\">1</span>]</span><br><span class=\"line\">Ty = np.array([<span class=\"number\">1</span> <span class=\"keyword\">if</span> i == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> Ty])</span><br><span class=\"line\">X, y = data[:,:-<span class=\"number\">1</span>], data[:,-<span class=\"number\">1</span>]</span><br><span class=\"line\">y = np.array([<span class=\"number\">1</span> <span class=\"keyword\">if</span> i == <span class=\"number\">1</span> <span class=\"keyword\">else</span> -<span class=\"number\">1</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> y])</span><br><span class=\"line\">ker=[<span class=\"string\">&#x27;linear&#x27;</span>,<span class=\"string\">&#x27;poly&#x27;</span>]</span><br><span class=\"line\">C=[<span class=\"number\">0.01</span>,<span class=\"number\">0.1</span>,<span class=\"number\">1</span>,<span class=\"number\">10</span>,<span class=\"number\">100</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> ker:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> C:</span><br><span class=\"line\">     modelSVM = SVC(kernel=i, C=j)  <span class=\"comment\"># SVC 建模：使用 SVC类，线性核函数</span></span><br><span class=\"line\"><span class=\"comment\"># modelSVM = LinearSVC(C=100)  # SVC 建模：使用 LinearSVC类，运行结果同上</span></span><br><span class=\"line\">     modelSVM.fit(X, y)  <span class=\"comment\"># 用样本集 X,y 训练 SVM 模型</span></span><br><span class=\"line\">     <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;分类准确度：&#123;:.4f&#125;&#x27;</span>.<span class=\"built_in\">format</span>(modelSVM.score(TX, Ty)))  <span class=\"comment\"># 对训练集的分类准确度</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制分割超平面和样本集分类结果</span></span><br><span class=\"line\">     plt.scatter(X[:,<span class=\"number\">0</span>], X[:,<span class=\"number\">1</span>], c=y, s=<span class=\"number\">30</span>, cmap=plt.cm.Paired)  <span class=\"comment\"># 散点图，根据 y值设置不同颜色</span></span><br><span class=\"line\">     ax = plt.gca()  <span class=\"comment\"># 移动坐标轴</span></span><br><span class=\"line\">     xlim = ax.get_xlim()  <span class=\"comment\"># 获得Axes的 x坐标范围</span></span><br><span class=\"line\">     ylim = ax.get_ylim()  <span class=\"comment\"># 获得Axes的 y坐标范围</span></span><br><span class=\"line\">     xx = np.linspace(xlim[<span class=\"number\">0</span>], xlim[<span class=\"number\">1</span>], <span class=\"number\">30</span>)  <span class=\"comment\"># 创建等差数列，从 start 到 stop，共 num 个</span></span><br><span class=\"line\">     yy = np.linspace(ylim[<span class=\"number\">0</span>], ylim[<span class=\"number\">1</span>], <span class=\"number\">30</span>)</span><br><span class=\"line\">     YY, XX = np.meshgrid(yy, xx)</span><br><span class=\"line\">     xy = np.vstack([XX.ravel(), YY.ravel()]).T  <span class=\"comment\"># 将网格矩阵展平后重构为数组</span></span><br><span class=\"line\">     Z = modelSVM.decision_function(xy).reshape(XX.shape)</span><br><span class=\"line\">     ax.contour(XX, YY, Z, colors=<span class=\"string\">&#x27;k&#x27;</span>, levels=[-<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>], alpha=<span class=\"number\">0.5</span>,</span><br><span class=\"line\">           linestyles=[<span class=\"string\">&#x27;--&#x27;</span>, <span class=\"string\">&#x27;-&#x27;</span>, <span class=\"string\">&#x27;--&#x27;</span>])  <span class=\"comment\"># 绘制决策边界和分隔</span></span><br><span class=\"line\">     ax.scatter(modelSVM.support_vectors_[:, <span class=\"number\">0</span>], modelSVM.support_vectors_[:, <span class=\"number\">1</span>], s=<span class=\"number\">100</span>,</span><br><span class=\"line\">           linewidth=<span class=\"number\">1</span>, facecolors=<span class=\"string\">&#x27;none&#x27;</span>, edgecolors=<span class=\"string\">&#x27;k&#x27;</span>)  <span class=\"comment\"># 绘制 支持向量</span></span><br><span class=\"line\">     plt.title(<span class=\"string\">f&quot;SVM Decision Boundary with kernel=<span class=\"subst\">&#123;i&#125;</span> and C=<span class=\"subst\">&#123;j&#125;</span>&quot;</span>)</span><br><span class=\"line\">     plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-9.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-10.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-11.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-12.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-13.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-14.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-15.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-16.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-17.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-18.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-19.png?raw=true\" alt=\"alt text\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_moons</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> accuracy_score</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 生成非线性数据</span></span><br><span class=\"line\">x, y = make_moons(n_samples=<span class=\"number\">100</span>, noise=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 画出散点图</span></span><br><span class=\"line\">plt.scatter(x[:, <span class=\"number\">0</span>], x[:, <span class=\"number\">1</span>], c=y, cmap=plt.cm.Paired)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将数据集划分为训练集和测试集</span></span><br><span class=\"line\">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">42</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据标准化</span></span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\">x_train = scaler.fit_transform(x_train)</span><br><span class=\"line\">x_test = scaler.transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 不同核函数的超参数设置</span></span><br><span class=\"line\">kernel_params = [&#123;<span class=\"string\">&#x27;kernel&#x27;</span>: <span class=\"string\">&#x27;poly&#x27;</span>, <span class=\"string\">&#x27;degree&#x27;</span>: <span class=\"number\">3</span>, <span class=\"string\">&#x27;C&#x27;</span>: <span class=\"number\">1.0</span>&#125;,</span><br><span class=\"line\">    &#123;<span class=\"string\">&#x27;kernel&#x27;</span>: <span class=\"string\">&#x27;rbf&#x27;</span>, <span class=\"string\">&#x27;gamma&#x27;</span>: <span class=\"number\">0.5</span>, <span class=\"string\">&#x27;C&#x27;</span>: <span class=\"number\">1.0</span>&#125;,</span><br><span class=\"line\">    &#123;<span class=\"string\">&#x27;kernel&#x27;</span>: <span class=\"string\">&#x27;linear&#x27;</span>, <span class=\"string\">&#x27;C&#x27;</span>: <span class=\"number\">1.0</span>&#125;,</span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制决策平面</span></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">15</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, params <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(kernel_params, <span class=\"number\">1</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 创建SVM模型</span></span><br><span class=\"line\">    svm_model = SVC(**params)</span><br><span class=\"line\">    svm_model.fit(x_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 在训练集上绘制决策平面</span></span><br><span class=\"line\"></span><br><span class=\"line\">    plt.scatter(x_train[:, <span class=\"number\">0</span>], x_train[:, <span class=\"number\">1</span>], c=y_train, cmap=plt.cm.Paired)</span><br><span class=\"line\">    plt.title(<span class=\"string\">f&quot;SVM Decision Boundary (<span class=\"subst\">&#123;params[<span class=\"string\">&#x27;kernel&#x27;</span>]&#125;</span> kernel)&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 绘制决策平面</span></span><br><span class=\"line\">    h = <span class=\"number\">0.02</span></span><br><span class=\"line\">    x_min, x_max = x_train[:, <span class=\"number\">0</span>].<span class=\"built_in\">min</span>() - <span class=\"number\">1</span>, x_train[:, <span class=\"number\">0</span>].<span class=\"built_in\">max</span>() + <span class=\"number\">1</span></span><br><span class=\"line\">    y_min, y_max = x_train[:, <span class=\"number\">1</span>].<span class=\"built_in\">min</span>() - <span class=\"number\">1</span>, x_train[:, <span class=\"number\">1</span>].<span class=\"built_in\">max</span>() + <span class=\"number\">1</span></span><br><span class=\"line\">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class=\"line\"></span><br><span class=\"line\">    Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class=\"line\">    Z = Z.reshape(xx.shape)</span><br><span class=\"line\"></span><br><span class=\"line\">    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=<span class=\"number\">0.6</span>)</span><br><span class=\"line\">    plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-20.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-21.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-22.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-23.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"小结或讨论\"><a href=\"#小结或讨论\" class=\"headerlink\" title=\"小结或讨论\"></a>小结或讨论</h1><p>SVM的优缺点：</p>\n<p>优点:</p>\n<p>可用于线性&#x2F;非线性分类，也可以用于回归，泛化错误率低，也就是说具有良好的学习能力，且学到的结果具有很好的推广性。</p>\n<p>可以解决小样本情况下的机器学习问题，可以解决高维问题，可以避免神经网络结构选择和局部极小点问题。</p>\n<p>SVM是最好的现成的分类器，现成是指不加修改可直接使用。并且能够得到较低的错误率，SVM可以对训练集之外的数据点做很好的分类决策。</p>\n<p>缺点：</p>\n<p>对参数调节和和函数的选择敏感。</p>\n","categories":["Machine Learning"]},{"title":"集成学习实验","url":"/2023/12/11/mllab4/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h1><ol>\n<li>   熟悉和掌握集成学习的基本原理</li>\n<li>   熟悉和掌握集成学习的并行策略，Bagging与随机森林解决复杂分类问题</li>\n<li>   熟悉和掌握集成学习的串行策略，Boosting与Adaboost解决复杂分类问题</li>\n<li>   了解和掌握第三方机器学习库Scikit-learn中的模型调用</li>\n</ol>\n<h1 id=\"实验要求\"><a href=\"#实验要求\" class=\"headerlink\" title=\"实验要求\"></a>实验要求</h1><ol>\n<li>   采用Python、Matlab等高级语言进行编程，推荐优先选用Python语言</li>\n<li>   本次实验可以直接调用Scikit-learn、PyTorch等成熟框架的第三方实现<span id=\"more\"></span></li>\n<li>   代码可读性强：变量、函数、类等命名可读性强，包含必要的注释</li>\n</ol>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理</h1><h2 id=\"概念\"><a href=\"#概念\" class=\"headerlink\" title=\"概念\"></a>概念</h2><p>集成学习(Ensemble learning)通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统、基于委员会的学习等，集成学习通常可以获得比单一学习模型更好的泛化性。</p>\n<h2 id=\"基本结构\"><a href=\"#基本结构\" class=\"headerlink\" title=\"基本结构\"></a>基本结构</h2><p>先产生多个“个体学习器”，也叫做“基学习器”，再用某种策略将它们结合起来，集成中只包含同种类型的个体学习器叫做同质，包含不同类型的个体学习器叫做异质。要获得好的集成就应该找到“好而不同”的个体学习器，这就是集成学习的研究核心。</p>\n<h2 id=\"Boosting\"><a href=\"#Boosting\" class=\"headerlink\" title=\"Boosting\"></a>Boosting</h2><p>Boosting是一簇可将弱学习器提升为强学习器的算法。其工作机制为：<br>先从初始训练集训练出一个基学习器<br>再根据基学习器的表现对样本分布进行调整，使得先前的基学习器做错的训练样本在后续收到更多的关注<br>然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到实现指定的值T，或整个集成结果达到退出条件<br>然后将这些学习器进行加权结合。</p>\n<h2 id=\"AdaBoost\"><a href=\"#AdaBoost\" class=\"headerlink\" title=\"AdaBoost\"></a>AdaBoost</h2><p>AdaBoost是Boosting中的经典算法，其主要应用于二分类问题。<br>Adaboost 算法采用调整样本权重的方式来对样本分布进行调整，即提高前一轮个体学习器错误分类的样本的权重，而降低那些正确分类的样本的权重，这样就能使得错误分类的样本可以受到更多的关注，从而在下一轮中可以正确分类，使得分类问题被一系列的弱分类器“分而治之”。对于组合方式，AdaBoost采用加权多数表决的方法，具体地，加大分类误差率小的若分类器的权值，减小分类误差率大的若分类器的权值，从而调整他们在表决中的作用。</p>\n<h2 id=\"随机森林\"><a href=\"#随机森林\" class=\"headerlink\" title=\"随机森林\"></a>随机森林</h2><p>随机森林(Random Forest，简称RF)是Bagging集成学习方法的一个扩展变体。随机森林以决策树作为Bagging集成学习的基学习器，与经典的决策树不同，随机森林在构建决策树模型的时候不是在所有的属性上去选择最优划分属性，而是在数据集属性的一个随机子集上进行最优划分属性的选择。由于基学习器的训练数据以及特征属性都不完全相同，随机森林构造的基学习器具有较大的差异，使得随机森林不仅简单、计算开销小，而且在很多实际任务中展现出强大的性能。</p>\n<h1 id=\"实验内容\"><a href=\"#实验内容\" class=\"headerlink\" title=\"实验内容\"></a>实验内容</h1><h2 id=\"数据集准备\"><a href=\"#数据集准备\" class=\"headerlink\" title=\"数据集准备\"></a>数据集准备</h2><ol>\n<li>   利用sklearn生成非线性数据（用于主体实验数据）<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_moons</span><br><span class=\"line\">x, y = make_moons(n_samples=<span class=\"number\">1000</span>, noise=<span class=\"number\">0.4</span>, random_state=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></li>\n</ol>\n<ul>\n<li>生成的数据按照按照3:1切分训练集与测试集</li>\n<li>数据切分的随机种子调成0（即random_state&#x3D;0）</li>\n</ul>\n<ol start=\"2\">\n<li>   利用sklearn自带.的Iris数据集（主要完成随机森林的特征可视化）</li>\n</ol>\n<h2 id=\"基于不同分类器的集成学习（仅在make-moons数据集上实现）\"><a href=\"#基于不同分类器的集成学习（仅在make-moons数据集上实现）\" class=\"headerlink\" title=\"基于不同分类器的集成学习（仅在make_moons数据集上实现）\"></a>基于不同分类器的集成学习（仅在make_moons数据集上实现）</h2><ol>\n<li>   利调用sklearn中的knn，logistic回归和高斯朴素贝叶斯三种分类器</li>\n<li>   模型分别学习make_moons训练集中的数据</li>\n<li>   自己编程实现基于多数投票的集成选择（不得使用sklean实现）</li>\n<li>   尝试采用sklean中的VotingClassifier实现基于“硬投票”和“软投票”的分类</li>\n</ol>\n<p>结果展示：</p>\n<ul>\n<li>分别展示三种分类器的分类结果以及手写多数投票的集成实验结果</li>\n<li>展示基于硬投票和软投票集成的分类结果</li>\n</ul>\n<h2 id=\"基于Bagging的随机森林算法实现\"><a href=\"#基于Bagging的随机森林算法实现\" class=\"headerlink\" title=\"基于Bagging的随机森林算法实现\"></a>基于Bagging的随机森林算法实现</h2><ol>\n<li>   尝试基于sklearn中的BaggingClassifier和DecisionTreeClassifier构建随机森林，并实现make_moons数据集的分类</li>\n<li>   尝试基于sklearn中的RandomForestClassifier构建随机森林，并利用feature_importances_分析属性的重要性（属性的重要性可在make_moons和iris数据集中分别尝试）<br>结果展示：</li>\n</ol>\n<ul>\n<li>分别展示两种构造随机森林方法在make_moons测试集上的分类结果</li>\n<li>展示随机森林对make_moons和iris特征重要性的比例</li>\n</ul>\n<h2 id=\"基于Boosting的算法实现\"><a href=\"#基于Boosting的算法实现\" class=\"headerlink\" title=\"基于Boosting的算法实现\"></a>基于Boosting的算法实现</h2><ol>\n<li>   尝试基于sklearn中的AdaBoostClassifier实现make_moons数据集的分类</li>\n<li>   尝试基于sklearn中的GradientBoostingClassifier实现make_moons数据集的分类</li>\n</ol>\n<p>结果展示：分别展示两种boosting方法在make_moons测试集上的分类结果</p>\n<h1 id=\"实验代码和结果\"><a href=\"#实验代码和结果\" class=\"headerlink\" title=\"实验代码和结果\"></a>实验代码和结果</h1><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_moons</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.naive_bayes <span class=\"keyword\">import</span> GaussianNB</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> VotingClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> accuracy_score</span><br><span class=\"line\">x, y = make_moons(n_samples=<span class=\"number\">1000</span>, noise=<span class=\"number\">0.4</span>, random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(x,y,train_size=<span class=\"number\">0.75</span>,test_size=<span class=\"number\">0.25</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">knn = KNeighborsClassifier()    <span class=\"comment\">#实例化KNN模型</span></span><br><span class=\"line\">knn.fit(X_train, y_train)   <span class=\"comment\">#放入训练数据进行训练</span></span><br><span class=\"line\">pk=knn.predict(X_test)</span><br><span class=\"line\">lstr=LogisticRegression()</span><br><span class=\"line\">lstr.fit(X_train,y_train)</span><br><span class=\"line\">pl=lstr.predict(X_test)</span><br><span class=\"line\">gnb=GaussianNB()</span><br><span class=\"line\">gnb.fit(X_train,y_train)</span><br><span class=\"line\">pg=gnb.predict(X_test)</span><br><span class=\"line\">pre=(pk+pl+pg)/<span class=\"number\">3</span></span><br><span class=\"line\">count=<span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(pre)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(pre[i]&lt;<span class=\"number\">0.5</span>): pre[i]=<span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:pre[i]=<span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;KNN分类准确率:&quot;</span>,accuracy_score(y_test,pk))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;logistic regression分类准确率:&quot;</span>,accuracy_score(y_test,pl))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;高斯朴素贝叶斯分类准确率:&quot;</span>,accuracy_score(y_test,pg))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;手写投票测试准确率:&quot;</span> + <span class=\"built_in\">str</span>(accuracy_score(y_test,pre)))</span><br><span class=\"line\">voting_clf = VotingClassifier(estimators=[</span><br><span class=\"line\">    (<span class=\"string\">&#x27;knn&#x27;</span>, knn),</span><br><span class=\"line\">    (<span class=\"string\">&#x27;lstr&#x27;</span>, lstr),</span><br><span class=\"line\">    (<span class=\"string\">&#x27;gnb&#x27;</span>, gnb),</span><br><span class=\"line\">], voting=<span class=\"string\">&#x27;hard&#x27;</span>)</span><br><span class=\"line\">voting_clf.fit(X_train,y_train)</span><br><span class=\"line\">vh=voting_clf.predict(X_test)</span><br><span class=\"line\">votings = VotingClassifier(estimators=[</span><br><span class=\"line\">    (<span class=\"string\">&#x27;knn&#x27;</span>, knn),</span><br><span class=\"line\">    (<span class=\"string\">&#x27;lstr&#x27;</span>, lstr),</span><br><span class=\"line\">    (<span class=\"string\">&#x27;gnb&#x27;</span>, gnb),</span><br><span class=\"line\">], voting=<span class=\"string\">&#x27;soft&#x27;</span>)</span><br><span class=\"line\">votings.fit(X_train,y_train)</span><br><span class=\"line\">vs=votings.predict(X_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;硬投票测试准确率:&quot;</span> + <span class=\"built_in\">str</span>(accuracy_score(y_test,vh)))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;软投票测试准确率:&quot;</span> + <span class=\"built_in\">str</span>(accuracy_score(y_test,vs)))</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> VotingClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> accuracy_score</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> BaggingClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\">iris=load_iris()</span><br><span class=\"line\">Xi=iris.data</span><br><span class=\"line\">Xi=Xi[<span class=\"number\">0</span>:<span class=\"number\">100</span>]</span><br><span class=\"line\">yi=iris.target</span><br><span class=\"line\">yi=yi[<span class=\"number\">0</span>:<span class=\"number\">100</span>]</span><br><span class=\"line\">Xi_train,Xi_test,yi_train,yi_test=train_test_split(Xi,yi,train_size=<span class=\"number\">0.75</span>,test_size=<span class=\"number\">0.25</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">x, y = make_moons(n_samples=<span class=\"number\">1000</span>, noise=<span class=\"number\">0.4</span>, random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">X_train,X_test,y_train,y_test=train_test_split(x,y,train_size=<span class=\"number\">0.75</span>,test_size=<span class=\"number\">0.25</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">clf = DecisionTreeClassifier()</span><br><span class=\"line\">rfc1 = BaggingClassifier(clf,n_estimators=<span class=\"number\">50</span>)</span><br><span class=\"line\">rfc1.fit(X_train,y_train)</span><br><span class=\"line\">pr=rfc1.predict(X_test)</span><br><span class=\"line\">rfc2=RandomForestClassifier(n_estimators=<span class=\"number\">50</span>)</span><br><span class=\"line\">rfc2.fit(X_train,y_train)</span><br><span class=\"line\">pr2=rfc2.predict(X_test)</span><br><span class=\"line\">rfc3=RandomForestClassifier(n_estimators=<span class=\"number\">50</span>)</span><br><span class=\"line\">rfc3.fit(Xi_train,yi_train)</span><br><span class=\"line\">pr3=rfc3.predict(Xi_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;BaggingClassifier的准确率:&quot;</span>,accuracy_score(y_test,pr))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;RandomForestClassifier的准确率:&quot;</span>,accuracy_score(y_test,pr2))</span><br><span class=\"line\">feai=rfc2.feature_importances_</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;特征的重要性:&quot;</span>,feai)</span><br></pre></td></tr></table></figure>\n<p>make moon的结果:</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-24.png?raw=true\" alt=\"alt text\"></p>\n<p>iris的结果：</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-25.png?raw=true\" alt=\"alt text\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">adb = AdaBoostClassifier(n_estimators=<span class=\"number\">100</span>)</span><br><span class=\"line\">grb=GradientBoostingClassifier(n_estimators=<span class=\"number\">100</span>)</span><br><span class=\"line\">adb.fit(X_train,y_train)</span><br><span class=\"line\">grb.fit(X_train,y_train)</span><br><span class=\"line\">pa=adb.predict(X_test)</span><br><span class=\"line\">pg=grb.predict(X_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;AdaBoostClassifier的准确率为&quot;</span>,accuracy_score(y_test,pa))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;GradientBoostingClassifier的准确率为&quot;</span>,accuracy_score(y_test,pg))</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-26.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"小结或讨论\"><a href=\"#小结或讨论\" class=\"headerlink\" title=\"小结或讨论\"></a>小结或讨论</h1><p>集成方法有很多种，一种叫做bagging，bagging的思想是，我把我的数据做一点微小的调整，就得到了一个跟原来不一样的数据集，我就能多训练一个模型出来，模型的数量多了，解释力自然就增强了。比如说我原来有100个人的数据，其中有两个分别叫Tony和Lily，我把Tony这条数据删掉，用Lily的数据来替换，这样就得到了一个跟原来不一样的船新的数据集，这个过程叫做Bootstrap</p>\n<p>每一个Bootstrap数据集都能用来训练一次模型，所以我们重复这个过程，比如重复1000次，一次是Tony替代Cici，一次是Ivy替代Yuki，这样每一次都是不一样的数据，也就可以训练1000次，得到了1000个决策树，我们把这1000个决策树打包到一起作为我们最终的模型，这个打包就叫做bagging</p>\n<p>一般我们会把bagging跟随机森林一起叠加使用，在数据点的处理上，我们使用bagging来创造许多组（比如说1000组）bootstrap数据，对于每一组数据，我们使用随机森林来训练模型，最后再把所有模型的预测结果bagging起来</p>\n<p>第二种集成的方法是boosting，boosting跟bagging一样都属于集成的思想，本质上都是训练很多模型，用数量堆积出质量。还是举1000个model，100个variable的例子，bagging是训练1000个等价的模型，比如说用随机森林，这些模型都是同样随机从100个里面选10个variable出来训练，每一个模型之间是同一级别的、互不干扰的</p>\n<p>但boosting的思路和bagging不同，boosting里每一个模型都是基于上一个模型来进行优化，它的核心思想是训练1000个模型，每一个模型在上一个模型的基础上再好一点点</p>\n","categories":["Machine Learning"]},{"title":"聚类实验","url":"/2023/12/18/mllab5/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h1><ol>\n<li>   熟悉和掌握k-means聚类算法</li>\n<li>   熟悉和掌握DBSCAN聚类算法</li>\n</ol>\n<h1 id=\"实验要求\"><a href=\"#实验要求\" class=\"headerlink\" title=\"实验要求\"></a>实验要求</h1><ol>\n<li>   采用Python、Matlab等高级语言进行编程，推荐优先选用Python语言</li>\n<li>   核心模型和算法需自主编程实现，不得直接调用Scikit-learn、PyTorch等成熟框架的第三方实现</li>\n<li>   代码可读性强：变量、函数、类等命名可读性强，包含必要的注释</li>\n</ol>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理<span id=\"more\"></span></h1><p>K-means算法是最简单的一种聚类算法。算法的目的是使各个样本与所在类均值的误差平方和达到最小（这也是评价K-means算法最后聚类效果的评价标准）<br>K-means聚类算法的一般步骤：<br>1.\t初始化。输入基因表达矩阵作为对象集X，输入指定聚类类数N，并在X中随机选取N个对象作为初始聚类中心。设定迭代中止条件，比如最大循环次数或者聚类中心收敛误差容限。<br>2.\t进行迭代。根据相似度准则将数据对象分配到最接近的聚类中心，从而形成一类。初始化隶属度矩阵。<br>3.\t更新聚类中心。然后以每一类的平均向量作为新的聚类中心，重新分配数据对象。<br>4.\t反复执行第二步和第三步直至满足中止条件。</p>\n<p>举一个简单的例子来说明问题：</p>\n<p>设有一组数据集x1&#x3D;(2,1),x2&#x3D;(1,3),x3&#x3D;(6,7),x4&#x3D;(4,7)</p>\n<p>(1)选取聚类中心，该中心可以任意选取，也可以通过直方图进行选取，还可以通过取前2个值进行选取。我们选择两个聚类中心。</p>\n<p>(2)计算每一个样本值到聚类中心的距离；并划分新的聚类中心；<br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-27.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-28.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-29.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-30.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-31.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-32.png?raw=true\" alt=\"alt text\"></p>\n<p>评价标准：<br>$$<br>J(c, \\mu)&#x3D;\\sum_{i&#x3D;1}^m\\left|x^{(i)}-\\mu_{c^{(i)}}\\right|^2<br>$$<br>假设有 $\\mathrm{M}$ 个数据源， $\\mathrm{C}$ 个聚类中心。 $\\mu \\mathrm{c}$ 为聚类中心。该公式的意思也就是将每个类中的数据与每个聚类中心做差的平方和, $\\mathrm{J}$ 最小, 意味着分割的效果最好。</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-33.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"实验内容\"><a href=\"#实验内容\" class=\"headerlink\" title=\"实验内容\"></a>实验内容</h1><h2 id=\"K-means聚类算法\"><a href=\"#K-means聚类算法\" class=\"headerlink\" title=\"K-means聚类算法\"></a>K-means聚类算法</h2><p>采用数据集 “data&#x2F;clustering1.csv”进行k-means聚类算法实验。<br>数据集简介：本数据集来自UCI数据库Sales_Transactions_Dataset_Weekly数据，经过简单筛选后维度为666\\times52，对应666种商品在52周的销量。</p>\n<ol>\n<li>将每一种商品看作一个样本, 每一周看作一个属性, 对其进行聚类。</li>\n<li>对数据进行 min-max 归一化至 $[0,1]$ 区间内:</li>\n<li>随机挑选 $\\mathrm{k}$ 个初始聚类中心, 使用欧氏距离度量各样本与聚类中心的距离,划分聚类。</li>\n<li>更新聚类中心, 重新划分聚类。</li>\n<li>重复第 4 步, 直到各类包含的样本不再改变或达到最大迭代次数, 输出结果。</li>\n<li>对原样本数据计算均值和方差, 进行标准化, 使用 PCA 降维方法降至二维,打印散点图至二维平面中, 以不同的颜色区分不同类别的样本。</li>\n<li>更换随机聚类中心, 重复上述第 3-5 步并打印散点图。</li>\n<li>参数设置: k 分别取 2、3、5，最大迭代次数 maxiter 取 100</li>\n</ol>\n<p>代码输出：样本二维图像，共6幅放在一起，三行两列，分别对应3个不同的k值，每个k值对应两次不同的初始聚类中心。<br>##\tDBSCAN聚类算法<br>采用数据集 “data&#x2F;clustering2.csv”进行DBSCAN聚类算法实验。</p>\n<p>数据集简介: 本数据来自 DIP 数据库, 为蛋白质相互作用网络数据, 网络 $G-$<br>$(E, V), \\mathrm{E}$ 是网络中的节点的集合, 即蛋白质, $\\mathrm{V}$ 是网络中的边的集合, 即蛋白质 -蛋白质相互作用。网络保存为邻接矩阵 $\\mathrm{A}$ 的形式, 矩阵中元素 $A_{i j}$ 代表第 $\\mathrm{i}$ 个蛋白质和第 $\\mathrm{j}$ 个蛋白质是否存在相互作用, 若值为 1 , 则存在相互作用, 并有一条边连接这两个节点。矩阵中 $A_{i j}$ 和 $A_{j i}$ 对应同一条边。矩阵维度为 $1274 \\times 1274$, 即共有 $1274$ 个蛋白质。存在相互作用的两个蛋白质之间的距离定为 $1$ , 无须计算。</p>\n<ol>\n<li>本实验中, 邻域半径 $\\varepsilon$ 设为 1 , 即密度直达的两个节点就是存在相互作用的两个蛋白质, 也就是说邻接矩阵中值为 1 的元素所在的行和列所对应的两个节点是密度直达的。</li>\n<li>密度阈值 MinPts 设为 $10 、 15 、 20$ 。</li>\n<li>依据密度聚类流程, 首先根据 MinPts 构造核心对象子集。</li>\n<li>依次从中取出一个核心对象 (同时从子集中删去该节点), 将它的邻居加入待访问节点子集。依次从待访问节点子集中取出一个节点 $v$ (同时从子集中删去该节点), 若它是核心对象, 将它的邻居也放入待访问节点子集。重复这一过程, 直到待访问节点子集为空集, 此时所有访问过的结点构成一个聚类。</li>\n<li>重复第 5 步, 直到核心对象子集为空集。</li>\n<li>计算各个聚类的聚类系数：<br>$$coef&#x3D;\\frac{n_{edge}}{2\\times n_{node}\\times\\left(n_{node-1}\\right)}$$</li>\n<li>   保存所有聚类的聚类系数。</li>\n</ol>\n<p>代码输出:不同密度阈值MinPts取值时对应的聚类数量和相应各个聚类的聚类系数</p>\n<h1 id=\"实验代码和结果\"><a href=\"#实验代码和结果\" class=\"headerlink\" title=\"实验代码和结果\"></a>实验代码和结果</h1><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">KMeans</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">distEclud</span>(<span class=\"params\">self, vecA, vecB</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.linalg.norm(vecA - vecB)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">initCentroids</span>(<span class=\"params\">self, dataSet, k</span>):</span><br><span class=\"line\">        numSamples, dim = dataSet.shape</span><br><span class=\"line\">        centroids = np.zeros((k, dim))</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(k):</span><br><span class=\"line\">            index = <span class=\"built_in\">int</span>(np.random.uniform(<span class=\"number\">0</span>, numSamples))</span><br><span class=\"line\">            centroids[i, :] = dataSet[index, :]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> centroids</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">kmeans</span>(<span class=\"params\">self, dataSet, k</span>):</span><br><span class=\"line\">        numSamples = dataSet.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        clusterAssment = np.mat(np.zeros((numSamples, <span class=\"number\">2</span>)))</span><br><span class=\"line\">        clusterChanged = <span class=\"literal\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\">        centroids = self.initCentroids(dataSet, k)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">while</span> clusterChanged:</span><br><span class=\"line\">            clusterChanged = <span class=\"literal\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(numSamples):</span><br><span class=\"line\">                minDist = <span class=\"number\">100000.0</span></span><br><span class=\"line\">                minIndex = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(k):</span><br><span class=\"line\">                    distance = self.distEclud(centroids[j, :], dataSet[i, :])</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> distance &lt; minDist:</span><br><span class=\"line\">                        minDist = distance</span><br><span class=\"line\">                        minIndex = j</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> clusterAssment[i, <span class=\"number\">0</span>] != minIndex:</span><br><span class=\"line\">                    clusterChanged = <span class=\"literal\">True</span></span><br><span class=\"line\">                    clusterAssment[i, :] = minIndex, minDist ** <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(k):</span><br><span class=\"line\">                pointsInCluster = dataSet[np.nonzero(clusterAssment[:, <span class=\"number\">0</span>].A == j)[<span class=\"number\">0</span>]]</span><br><span class=\"line\">                centroids[j, :] = np.mean(pointsInCluster, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> centroids, clusterAssment</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">pca</span>(<span class=\"params\">data, k=<span class=\"number\">2</span></span>):</span><br><span class=\"line\">    mean_vals = np.mean(data, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    mean_removed = data - mean_vals</span><br><span class=\"line\">    cov_mat = np.cov(mean_removed, rowvar=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    eig_vals, eig_vecs = np.linalg.eig(np.mat(cov_mat))</span><br><span class=\"line\">    eig_val_index = np.argsort(eig_vals)[::-<span class=\"number\">1</span>]</span><br><span class=\"line\">    eig_val_index = eig_val_index[:k]</span><br><span class=\"line\">    reduced_data = mean_removed * eig_vecs[:, eig_val_index]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> reduced_data</span><br><span class=\"line\"></span><br><span class=\"line\">data = pd.read_csv(<span class=\"string\">r&#x27;E:\\Google\\data\\clustering1.csv&#x27;</span>,sep=<span class=\"string\">&#x27;,&#x27;</span>,header=<span class=\"literal\">None</span>)</span><br><span class=\"line\">data1=data.values</span><br><span class=\"line\">Max=data1.<span class=\"built_in\">max</span>()</span><br><span class=\"line\">Min=data1.<span class=\"built_in\">min</span>()</span><br><span class=\"line\">data1=(data1-Min)/(Max-Min)</span><br><span class=\"line\">kmeans_instance = KMeans()</span><br><span class=\"line\">centroids, cluster_assignment = kmeans_instance.kmeans(data1, k=<span class=\"number\">5</span>)</span><br><span class=\"line\">data=data.values</span><br><span class=\"line\">Mean=data.mean()</span><br><span class=\"line\">Std=data.std()</span><br><span class=\"line\">data=(data-Mean)/Std</span><br><span class=\"line\">rd=pca(data)</span><br><span class=\"line\">rd=np.column_stack((rd,cluster_assignment[:, <span class=\"number\">0</span>]))</span><br><span class=\"line\">color=[<span class=\"string\">&#x27;r&#x27;</span>,<span class=\"string\">&#x27;g&#x27;</span>,<span class=\"string\">&#x27;b&#x27;</span>,<span class=\"string\">&#x27;c&#x27;</span>,<span class=\"string\">&#x27;m&#x27;</span>,<span class=\"string\">&#x27;y&#x27;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(rd)):</span><br><span class=\"line\">    plt.scatter(rd[i,<span class=\"number\">0</span>],rd[i,<span class=\"number\">1</span>],color=color[<span class=\"built_in\">int</span>(rd[i,<span class=\"number\">2</span>])])</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>k&#x3D;2</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-34.png?raw=true\" alt=\"alt text\"></p>\n<p>k&#x3D;3</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-35.png?raw=true\" alt=\"alt text\"></p>\n<p>k&#x3D;5</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-36.png?raw=true\" alt=\"alt text\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">DBSCAN</span>(<span class=\"params\">Matrix, eps, min_pts</span>):</span><br><span class=\"line\">    Cluster = np.full(Matrix.shape[<span class=\"number\">0</span>], -<span class=\"number\">1</span>, dtype=<span class=\"built_in\">int</span>)</span><br><span class=\"line\">    cluster_id = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Matrix.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> Cluster[i] != -<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">        neighbors = Neighbors(Matrix, i, eps)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(neighbors) &lt; min_pts:</span><br><span class=\"line\">            Cluster[i] = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            Expand(Matrix, Cluster, i, neighbors, cluster_id, eps, min_pts)</span><br><span class=\"line\">            cluster_id += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> Cluster</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">Neighbors</span>(<span class=\"params\">Matrix, center, eps</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.where(Matrix[center] == <span class=\"number\">1</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">Expand</span>(<span class=\"params\">Matrix, Cluster, center, neighbors, cluster_id, eps, min_pts</span>):</span><br><span class=\"line\">    Cluster[center] = cluster_id</span><br><span class=\"line\"></span><br><span class=\"line\">    i = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> i &lt; <span class=\"built_in\">len</span>(neighbors):</span><br><span class=\"line\">        n = neighbors[i]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> Cluster[n] == -<span class=\"number\">1</span>:</span><br><span class=\"line\">            Cluster[n] = cluster_id</span><br><span class=\"line\">            new_neighbors = Neighbors(Matrix, n, eps)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(new_neighbors) &gt;= min_pts:</span><br><span class=\"line\">                neighbors = np.concatenate((neighbors, new_neighbors))</span><br><span class=\"line\"></span><br><span class=\"line\">        i += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">Coefficient</span>(<span class=\"params\">Matrix, Cluster, cluster_id</span>):</span><br><span class=\"line\">    cluster_nodes = np.where(Cluster == cluster_id)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(cluster_nodes) &lt; <span class=\"number\">2</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    internal_edges = np.<span class=\"built_in\">sum</span>(Matrix[np.ix_(cluster_nodes, cluster_nodes)])</span><br><span class=\"line\"></span><br><span class=\"line\">    cluster_coefficient = <span class=\"number\">2.0</span> * internal_edges / (<span class=\"built_in\">len</span>(cluster_nodes) * (<span class=\"built_in\">len</span>(cluster_nodes) - <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> cluster_coefficient</span><br><span class=\"line\">A = pd.read_csv(<span class=\"string\">r&#x27;E:\\Google\\data\\clustering2.csv&#x27;</span>,sep=<span class=\"string\">&#x27;,&#x27;</span>,header=<span class=\"literal\">None</span>)</span><br><span class=\"line\">A=A.to_numpy()</span><br><span class=\"line\">min_pts_values = [<span class=\"number\">10</span>, <span class=\"number\">15</span>, <span class=\"number\">20</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> min_pts <span class=\"keyword\">in</span> min_pts_values:</span><br><span class=\"line\">    Cluster = DBSCAN(A, eps=<span class=\"number\">1</span>, min_pts=min_pts)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;MinPts=<span class=\"subst\">&#123;min_pts&#125;</span>\\n&#x27;</span>)</span><br><span class=\"line\">    unique_Cluster= np.unique(Cluster)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> cluster <span class=\"keyword\">in</span> unique_Cluster:</span><br><span class=\"line\">        cluster_members = np.where(Cluster == cluster)[<span class=\"number\">0</span>]</span><br><span class=\"line\">        coefficient = Coefficient(A, Cluster, cluster)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Cluster <span class=\"subst\">&#123;cluster&#125;</span>: <span class=\"subst\">&#123;coefficient&#125;</span>\\n&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-37.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-38.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-39.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"小结或讨论\"><a href=\"#小结或讨论\" class=\"headerlink\" title=\"小结或讨论\"></a>小结或讨论</h1><p>k均值简单并且可以用于各种数据类型。它相当有效，尽管常常多次运行。k均值的某些变种（包括二分K均值）甚至更有效，并且不太受初始化问题的影响。然而，k均值并不适合所有的数据类型。它不能出来非球形簇、不同尺寸和不同密度的簇。最后，k均值仅限于具有中心（质心）概念的数据。</p>\n<p>DBSCAN(Density-Based Spatial Clustering of Applications with Noise)是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类（笔者认为是因为他不是基于距离的，基于距离的发现的是球状簇）。</p>\n<p>该算法利用基于密度的聚类的概念，即要求聚类空间中的一定区域内所包含对象（点或其他空间对象）的数目不小于某一给定阈值。DBSCAN算法的显著优点是聚类速度快且能够有效处理噪声点和发现任意形状的空间聚类。但是由于它直接对整个数据库进行操作且进行聚类时使用了一个全局性的表征密度的参数，因此也具有两个比较明显的弱点：</p>\n<p>（1）当数据量增大时，要求较大的内存支持I&#x2F;O消耗也很大；</p>\n<p>（2）当空间聚类的密度不均匀、聚类间距差相差很大时，聚类质量较差（有些簇内距离较小，有些簇内距离很大，但是Eps是确定的，所以，大的点可能被误判断为离群点或者边界点，如果Eps太大，那么小距离的簇内，可能会包含一些离群点或者边界点，KNN的k也存在同样的问题）。</p>\n<p>（1）与K-MEANS比较起来，不需要输入要划分的聚类个数；</p>\n<p>（2）聚类簇的形状没有偏倚；</p>\n<p>（3）可以在需要时输入过滤噪声的参数；</p>\n","categories":["Machine Learning"]},{"title":"神经网络分类任务综合实验","url":"/2023/12/25/mllab6/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h1><ol>\n<li>   熟悉和掌握机器学习的完整流程</li>\n<li>   熟悉和掌握神经网络的构建</li>\n</ol>\n<h1 id=\"实验要求\"><a href=\"#实验要求\" class=\"headerlink\" title=\"实验要求\"></a>实验要求</h1><ol>\n<li>   采用Python、Matlab等高级语言进行编程，推荐优先选用Python语言</li>\n<li>   代码可读性强：变量、函数、类等命名可读性强，包含必要的注释</li>\n</ol>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理<span id=\"more\"></span></h1><p>人工神经网络（ANN），简称神经网络，是一种模仿生物神经网络的结构和功能的数学模型或计算模型。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。现代神经网络是一种非线性统计性数据建模工具，常用来对输入和输出间复杂的关系进行建模，或用来探索数据的模式。<br>人工神经网络从以下四个方面去模拟人的智能行为:</p>\n<ul>\n<li>物理结构：人工神经元将模拟生物神经元的功能</li>\n<li>计算模拟：人脑的神经元有局部计算和存储的功能，通过连接构成一个系统。人工神经网络中也有大量有局部处理能力的神经元，也能够将信息进行大规模并行处理</li>\n<li>存储与操作：人脑和人工神经网络都是通过神经元的连接强度来实现记忆存储功能，同时为概括、类比、推广提供有力的支持</li>\n<li>训练：同人脑一样，人工神经网络将根据自己的结构特性，使用不同的训练、学习过程，自动从实践中获得相关知识</li>\n</ul>\n<p>神经网络是一种运算模型，由大量的节点（或称“神经元”，或“单元”）和之间相互联接构成。每个节点代表一种特定的输出函数，称为激励函数。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。网络的输出则依网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。</p>\n<h2 id=\"感知器\"><a href=\"#感知器\" class=\"headerlink\" title=\"感知器\"></a>感知器</h2><p>历史上，科学家一直希望模拟人的大脑，造出可以思考的机器。人为什么能够思考？科学家发现，原因在于人体的神经网络。</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-40.png?raw=true\" alt=\"alt text\"></p>\n<p>既然思考的基础是神经元，如果能够”人造神经元”（artificial neuron），就能组成人工神经网络，模拟思考。上个世纪六十年代，提出了最早的”人造神经元”模型，叫做”感知器”（perceptron），直到今天还在用。</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-41.png?raw=true\" alt=\"alt text\"></p>\n<p>上图的圆圈就代表一个感知器。它接受多个输入（x1，x2，x3…），产生一个输出（output），好比神经末梢感受各种外部环境的变化，最后产生电信号。</p>\n<p>为了简化模型，我们约定每种输入只有两种可能：1 或 0。如果所有输入都是1，表示各种条件都成立，输出就是1；如果所有输入都是0，表示条件都不成立，输出就是0</p>\n<h2 id=\"决策模型\"><a href=\"#决策模型\" class=\"headerlink\" title=\"决策模型\"></a>决策模型</h2><p>单个的感知器构成了一个简单的决策模型，已经可以拿来用了。真实世界中，实际的决策模型则要复杂得多，是由多个感知器组成的多层网络。</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-42.png?raw=true\" alt=\"alt text\"></p>\n<p>上图中，底层感知器接收外部输入，做出判断以后，再发出信号，作为上层感知器的输入，直至得到最后的结果。（注意：感知器的输出依然只有一个，但是可以发送给多个目标。）</p>\n<p>一个神经网络的搭建，需要满足三个条件。</p>\n<ul>\n<li>输入和输出</li>\n<li>权重（W）和阈值（b）</li>\n<li>多层感知机的结构</li>\n</ul>\n<p>神经网络的运作过程如下：</p>\n<p>确定输入和输出<br>1.\t找到一种或多种算法，可以从输入得到输出<br>2.\t找到一组已知答案的数据集，用来训练模型，估算W和b<br>3.\t一旦新的数据产生，输入模型，就可以得到结果，同时对W和b进行校正</p>\n<p>可以看到，整个过程需要海量计算。所以，神经网络直到最近这几年才有实用价值，而且一般的 CPU 还不行，要使用专门为机器学习定制的 GPU 来计算。</p>\n<h1 id=\"实验内容\"><a href=\"#实验内容\" class=\"headerlink\" title=\"实验内容\"></a>实验内容</h1><h2 id=\"数据简介\"><a href=\"#数据简介\" class=\"headerlink\" title=\"数据简介\"></a>数据简介</h2><p>采用数据集 “data&#x2F;positive.csv”和“data&#x2F;negative.csv”进行本次实验。</p>\n<h2 id=\"原始数据可视化\"><a href=\"#原始数据可视化\" class=\"headerlink\" title=\"原始数据可视化\"></a>原始数据可视化</h2><p>调用可视化工具，将原始数据可视化输出至二维平面内，以颜色区分不同类别。（python可使用UMAP，conda install umap-learn；matlab可使用t-sne函数）。</p>\n<h2 id=\"模型设计\"><a href=\"#模型设计\" class=\"headerlink\" title=\"模型设计\"></a>模型设计</h2><ol>\n<li>   本实验为二分类问题，正负样本各200个。</li>\n<li>   使用BP神经网络进行二分类（Python可使用Pytorch，MATLAB可使用神经网络工具箱newff函数。</li>\n<li>   设计包含3个隐藏层的BP神经网络，输入层维度为样本维度；隐藏层维度分别是128，32，2，隐藏层使用relu激活函数；输出层维度1，使用sigmoid激活函数。</li>\n</ol>\n<h2 id=\"模型训练与性能评估\"><a href=\"#模型训练与性能评估\" class=\"headerlink\" title=\"模型训练与性能评估\"></a>模型训练与性能评估</h2><ol>\n<li>   使用五折交叉验证（5-fold cross-validation）评估神经网络在该数据上的性能。</li>\n<li>   五折交叉验证：将正负样本各分为数量相等的5份：1-40，41-80，81-120，121-160，161-200，并把1份正样本与1份负样本合并构成1个子集，则原数据分成了5个子集，且子集中正负样本比例1：1保持不变。对模型进行5次训练测试：<ul>\n<li>第i次时，选择第i份子集作为测试集，其余4份数据子集作为训练集。使用训练集训练数据，并使用测试集验证&#x2F;测试性能，计算测试集的AUROC值。</li>\n<li>模型倒数第2层维度为2，导出测试集数据在这一层上的数值，输出成二维散点图，包含两个子图：子图1的标签为测试样本的预测标签（取决于模型输出，将输出大于0.5的样本预测为正样本，否则预测为负样本），以不同颜色区分不同的预测结果；子图2的标签为测试样本的真实标签，以不同颜色区分不同的真实结果。</li>\n<li>完成5次，使得每一个子集都作为测试集1次，作为训练集4次。计算整体的AUROC。</li>\n</ul>\n</li>\n<li>   打断正负样本排列，重新划分训练集与测试集，重新训练模型，查看性能变化。</li>\n</ol>\n<h2 id=\"特征选择和特征提取\"><a href=\"#特征选择和特征提取\" class=\"headerlink\" title=\"特征选择和特征提取\"></a>特征选择和特征提取</h2><ol>\n<li>   特征选择：针对原始数据，计算每个特征的方差，选取两个方差最大的特征。将正负样本点输出为二维散点图，以颜色区分。</li>\n<li>   特征提取：针对原始数据，调用PCA降维函数将数据降到二维，将正负样本点输出为二维散点图，以颜色区分。</li>\n</ol>\n<p>代码输出：</p>\n<ul>\n<li>原始数据的散点图；</li>\n<li>测试集的散点图；</li>\n<li>经过特征选择后的散点图；</li>\n<li>经过PCA降维后的散点图。</li>\n</ul>\n<h1 id=\"实验代码和结果\"><a href=\"#实验代码和结果\" class=\"headerlink\" title=\"实验代码和结果\"></a>实验代码和结果</h1><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">po = pd.read_csv(<span class=\"string\">r&#x27;E:\\firefox download\\positive.csv&#x27;</span>,sep=<span class=\"string\">&#x27;,&#x27;</span>,header=<span class=\"literal\">None</span>)</span><br><span class=\"line\">ne=pd.read_csv(<span class=\"string\">r&#x27;E:\\firefox download\\negative.csv&#x27;</span>,sep=<span class=\"string\">&#x27;,&#x27;</span>,header=<span class=\"literal\">None</span>)</span><br><span class=\"line\">poe = umap.UMAP(n_components=<span class=\"number\">2</span>).fit_transform(po)</span><br><span class=\"line\">nee= umap.UMAP(n_components=<span class=\"number\">2</span>).fit_transform(ne)</span><br><span class=\"line\">plt.scatter(poe[:, <span class=\"number\">0</span>], poe[:, <span class=\"number\">1</span>],label=<span class=\"string\">&quot;positive&quot;</span>)</span><br><span class=\"line\">plt.scatter(nee[:, <span class=\"number\">0</span>], nee[:, <span class=\"number\">1</span>],label=<span class=\"string\">&quot;negative&quot;</span>)</span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-43.png?raw=true\" alt=\"alt text\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> roc_auc_score</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> StratifiedKFold</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义神经网络模型</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">NeuralNetwork</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_dim, hidden_dims</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(NeuralNetwork, self).__init__()</span><br><span class=\"line\">        self.input_layer = nn.Linear(input_dim, hidden_dims[<span class=\"number\">0</span>])</span><br><span class=\"line\">        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dims[i], hidden_dims[i+<span class=\"number\">1</span>]) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(hidden_dims)-<span class=\"number\">1</span>)])</span><br><span class=\"line\">        self.output_layer = nn.Linear(hidden_dims[-<span class=\"number\">1</span>], <span class=\"number\">1</span>)</span><br><span class=\"line\">        self.relu = nn.ReLU()</span><br><span class=\"line\">        self.sigmoid = nn.Sigmoid()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        x = self.relu(self.input_layer(x))</span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> self.hidden_layers:</span><br><span class=\"line\">            x = self.relu(layer(x))</span><br><span class=\"line\">        x = self.sigmoid(self.output_layer(x))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读取数据</span></span><br><span class=\"line\">positive_data = pd.read_csv(<span class=\"string\">r&#x27;E:\\firefox download\\positive.csv&#x27;</span>,header=<span class=\"literal\">None</span>)</span><br><span class=\"line\">negative_data = pd.read_csv(<span class=\"string\">r&#x27;E:\\firefox download\\negative.csv&#x27;</span>,header=<span class=\"literal\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 合并数据</span></span><br><span class=\"line\">data = pd.concat([positive_data, negative_data], ignore_index=<span class=\"literal\">True</span>,axis=<span class=\"number\">1</span>).T.values</span><br><span class=\"line\">label_train = np.concatenate([np.ones(<span class=\"number\">200</span>), np.zeros(<span class=\"number\">200</span>)])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 存储每次的AUC值</span></span><br><span class=\"line\">auc_values = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化模型</span></span><br><span class=\"line\">input_dim = data.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">hidden_dims = [<span class=\"number\">128</span>, <span class=\"number\">32</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">model = NeuralNetwork(input_dim, hidden_dims)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义损失函数和优化器</span></span><br><span class=\"line\">criterion = nn.BCELoss()</span><br><span class=\"line\">optimizer = optim.Adam(model.parameters(), lr=<span class=\"number\">0.001</span>)</span><br><span class=\"line\">skf=StratifiedKFold(n_splits=<span class=\"number\">5</span>,shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># 进行5次测试</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> fold,(train_indices,test_indices) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(skf.split(data,label_train)):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;Fold <span class=\"subst\">&#123;fold + <span class=\"number\">1</span>&#125;</span>:&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    X_train, X_test = data[train_indices], data[test_indices]</span><br><span class=\"line\">    y_train, y_test = label_train[train_indices], label_train[test_indices]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check if there are both positive and negative samples in the test set</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">set</span>(y_test) == &#123;<span class=\"number\">0</span>&#125; <span class=\"keyword\">or</span> <span class=\"built_in\">set</span>(y_test) == &#123;<span class=\"number\">1</span>&#125;:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Warning: Test set contains only one class. Skipping evaluation for this fold.&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 数据标准化</span></span><br><span class=\"line\">    scaler = StandardScaler()</span><br><span class=\"line\">    X_train = scaler.fit_transform(X_train)</span><br><span class=\"line\">    X_test = scaler.transform(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 转换为PyTorch的Tensor</span></span><br><span class=\"line\">    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)</span><br><span class=\"line\">    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-<span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)</span><br><span class=\"line\">    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-<span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 训练模型</span></span><br><span class=\"line\">    num_epochs = <span class=\"number\">1000</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        model.train()</span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        outputs = model(X_train_tensor)</span><br><span class=\"line\">        loss = criterion(outputs, y_train_tensor)</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 在测试集上评估模型</span></span><br><span class=\"line\">    model.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        test_outputs = model(X_test_tensor)</span><br><span class=\"line\">        predicted_labels = (test_outputs &gt;= <span class=\"number\">0.5</span>).<span class=\"built_in\">float</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Check if there are both positive and negative predictions</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">set</span>(predicted_labels.numpy().flatten()) == &#123;<span class=\"number\">0</span>&#125; <span class=\"keyword\">or</span> <span class=\"built_in\">set</span>(predicted_labels.numpy().flatten()) == &#123;<span class=\"number\">1</span>&#125;:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Warning: Predictions contain only one class. Skipping evaluation for this fold.&quot;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 计算AUC值</span></span><br><span class=\"line\">        auc = roc_auc_score(y_test, predicted_labels.numpy())</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;AUC on test set: <span class=\"subst\">&#123;auc:<span class=\"number\">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">        auc_values.append(auc)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 绘制散点图</span></span><br><span class=\"line\">        plt.figure(figsize=(<span class=\"number\">8</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\">        plt.subplot(<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        plt.scatter(<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(predicted_labels)), predicted_labels.numpy(), label=<span class=\"string\">f&#x27;Fold <span class=\"subst\">&#123;fold + <span class=\"number\">1</span>&#125;</span>&#x27;</span>)</span><br><span class=\"line\">        plt.title(<span class=\"string\">&#x27;Predicted Labels&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        plt.subplot(<span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">        plt.scatter(<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(y_test)), y_test, label=<span class=\"string\">f&#x27;Fold <span class=\"subst\">&#123;fold + <span class=\"number\">1</span>&#125;</span>&#x27;</span>)</span><br><span class=\"line\">        plt.title(<span class=\"string\">&#x27;True Labels&#x27;</span>)</span><br><span class=\"line\">        plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算整体的平均AUC值</span></span><br><span class=\"line\"></span><br><span class=\"line\">mean_auc = <span class=\"built_in\">sum</span>(auc_values) / <span class=\"built_in\">len</span>(auc_values)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Mean AUC across folds: <span class=\"subst\">&#123;mean_auc:<span class=\"number\">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-44.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-45.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-46.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-47.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-48.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-49.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-50.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-51.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-52.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-53.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-54.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-55.png?raw=true\" alt=\"alt text\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">X = pd.read_csv(<span class=\"string\">r&#x27;E:\\firefox download\\positive.csv&#x27;</span>,sep=<span class=\"string\">&#x27;,&#x27;</span>,header=<span class=\"literal\">None</span>)</span><br><span class=\"line\">Y=pd.read_csv(<span class=\"string\">r&#x27;E:\\firefox download\\negative.csv&#x27;</span>,sep=<span class=\"string\">&#x27;,&#x27;</span>,header=<span class=\"literal\">None</span>)</span><br><span class=\"line\">X=X.values</span><br><span class=\"line\">Y=Y.values</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算每个特征的方差</span></span><br><span class=\"line\">X_variances = np.var(X, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">Y_variances = np.var(Y, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"comment\"># 选择两个方差最大的特征</span></span><br><span class=\"line\">X_features = X[:, np.argsort(X_variances)[-<span class=\"number\">2</span>:]]</span><br><span class=\"line\">Y_features = Y[:, np.argsort(Y_variances)[-<span class=\"number\">2</span>:]]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出二维散点图</span></span><br><span class=\"line\">plt.scatter(X_features[:,<span class=\"number\">0</span>], X_features[:,<span class=\"number\">1</span>], color=<span class=\"string\">&#x27;blue&#x27;</span>, label=<span class=\"string\">&#x27;Positive Samples&#x27;</span>)</span><br><span class=\"line\">plt.scatter(Y_features[:,<span class=\"number\">0</span>], Y_features[:,<span class=\"number\">1</span>], color=<span class=\"string\">&#x27;red&#x27;</span>, label=<span class=\"string\">&#x27;Negative Samples&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Feature 1&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Feature 2&#x27;</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;Feature Selection&#x27;</span>)</span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-56.png?raw=true\" alt=\"alt text\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">X = pd.read_csv(<span class=\"string\">r&#x27;E:\\firefox download\\positive.csv&#x27;</span>,sep=<span class=\"string\">&#x27;,&#x27;</span>,header=<span class=\"literal\">None</span>)</span><br><span class=\"line\">Y=pd.read_csv(<span class=\"string\">r&#x27;E:\\firefox download\\negative.csv&#x27;</span>,sep=<span class=\"string\">&#x27;,&#x27;</span>,header=<span class=\"literal\">None</span>)</span><br><span class=\"line\">X=X.values</span><br><span class=\"line\">Y=Y.values</span><br><span class=\"line\"></span><br><span class=\"line\">cov_matrix = np.cov(X, rowvar=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算特征值和特征向量</span></span><br><span class=\"line\">eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 选择前两个特征向量对应的特征值最大的两个特征</span></span><br><span class=\"line\">X_features = np.dot(X, eigenvectors[:, -<span class=\"number\">2</span>:])</span><br><span class=\"line\">Y_features = np.dot(Y, eigenvectors[:, -<span class=\"number\">2</span>:])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出二维散点图</span></span><br><span class=\"line\">plt.scatter(X_features[:,<span class=\"number\">0</span>], X_features[:,<span class=\"number\">1</span>], color=<span class=\"string\">&#x27;blue&#x27;</span>, label=<span class=\"string\">&#x27;Positive Samples&#x27;</span>)</span><br><span class=\"line\">plt.scatter(Y_features[:,<span class=\"number\">0</span>], Y_features[:,<span class=\"number\">1</span>], color=<span class=\"string\">&#x27;red&#x27;</span>, label=<span class=\"string\">&#x27;Negative Samples&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Principal Component 1&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Principal Component 2&#x27;</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;Feature Extraction (PCA)&#x27;</span>)</span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/mlu-57.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"小结或讨论\"><a href=\"#小结或讨论\" class=\"headerlink\" title=\"小结或讨论\"></a>小结或讨论</h1><p>显而易见的是，神经网络的分类方法能够适用于任何数据集，在模型搭建好之后，只需要进行训练，就可以得到不错的结果，训练的轮数越多，网络的层数越深，就能够得到约准确的分类，但是带来的时间开销也是巨大的。</p>\n","categories":["Machine Learning"]},{"title":"进程调度(上)","url":"/2023/11/24/oslab1/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h1><ul>\n<li>针对进程调度的特点，设计进程控制块PCB的数据结构</li>\n<li>选择合适的数据结构建立进程就绪队列，实现对进程的有序组织</li>\n<li>实现时间片轮转调度算法</li>\n<li>实现基于优先权调度算法</li>\n</ul>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理</h1><ul>\n<li>时间片轮转调度算法（round robin）<span id=\"more\"></span><ul>\n<li>该算法采取了非常公平的方式，即让就绪队列上的每个进程每次仅运行一个时间片。如果就绪队列上有N个进程，则每个进程每次大约都可获得1&#x2F;N的处理机时间。</li>\n<li>时间片的大小对于系统性能有很大的影响。若选择很小的时间片，将有利于短作业，但意味着会频繁地执行进程调度和进程上下文的切换，这无疑会增加系统的开销。反之，若时间片选择得太长，且为使每个进程都能在一个时间片内完成，RR算法便退化为FCFS算法，无法满足短作业和交互式用户的需求。</li>\n<li>进程的切换时机体现出RR算法的特点。若一个进程在时间片还没结束时就已完成，此时立即激活调度程序，将它从执行队列中删除。若一个进程在时间片结束时还未运行完毕，则调度程序将把它送往就绪队列的末尾，等待下一次执行。用C语言编程模拟调度程序时，将时间片，程序运行时间量化为整数</li>\n</ul>\n</li>\n<li>优先权调度算法<ul>\n<li>在时间片算法中，无法对进程的紧急程度加以区分。而优先级算法正好可以解决这一问题。</li>\n<li>进程优先级的确定同样重要。进程优先级可以分为静态优先级和动态优先级。静态优先级是在进程创建初期就被确定的值，此后不再更改。动态优先级指进程在创建时被赋予一个初值，此后其值会所进程的推进或等待时间的增加而改变。</li>\n<li>用C语言模拟调度程序时，可用<code>p-&gt;priority += 1; /*优先数加,若设为0则优先级不变*/ </code>这条语句控制静态动态优先级的切换。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"实验内容\"><a href=\"#实验内容\" class=\"headerlink\" title=\"实验内容\"></a>实验内容</h1><h2 id=\"PCB的数据结构及进程的基本操作\"><a href=\"#PCB的数据结构及进程的基本操作\" class=\"headerlink\" title=\"PCB的数据结构及进程的基本操作\"></a>PCB的数据结构及进程的基本操作</h2><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> _CRT_SECURE_NO_WARNINGS</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">pcb</span> &#123;</span> <span class=\"comment\">// 定义进程控制块（PCB）结构体</span></span><br><span class=\"line\">\t<span class=\"type\">char</span> name;</span><br><span class=\"line\">\t<span class=\"type\">char</span> state;</span><br><span class=\"line\">\t<span class=\"type\">int</span> arrivetime=<span class=\"number\">65535</span>;<span class=\"comment\">//到达时间</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> needtime=<span class=\"number\">65535</span>;<span class=\"comment\">//所需时间</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> finishtime=<span class=\"number\">65535</span>;<span class=\"comment\">//结束时间</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> servicetime = <span class=\"number\">0</span>; <span class=\"comment\">//服务时间</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> priority = <span class=\"number\">65535</span>; <span class=\"comment\">//优先数</span></span><br><span class=\"line\">\t<span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">pcb</span>* <span class=\"title\">next</span>=</span><span class=\"literal\">NULL</span>; <span class=\"comment\">//下一个节点</span></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">pcb</span> <span class=\"title\">PCB</span>;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">ReadyQueue</span>//就绪队列</span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">\tPCB* head;</span><br><span class=\"line\">\tPCB* tail;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">initQueue</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue* rq)</span> &#123;<span class=\"comment\">//初始化就绪队列</span></span><br><span class=\"line\">\trq-&gt;head = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">\trq-&gt;tail = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">EnQueue</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue* rq, PCB* p)</span> &#123;<span class=\"comment\">//入队</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (!rq-&gt;head) &#123;</span><br><span class=\"line\">\t\trq-&gt;head = p;</span><br><span class=\"line\">\t\trq-&gt;tail = p;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\trq-&gt;tail-&gt;next = p;</span><br><span class=\"line\">\t\trq-&gt;tail = p;</span><br><span class=\"line\">\t\tp-&gt;next = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Dequeue</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue* rq, PCB** p)</span> &#123;<span class=\"comment\">//出队</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (rq-&gt;head != <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">\t\t*p = rq-&gt;head;</span><br><span class=\"line\">\t\trq-&gt;head = rq-&gt;head-&gt;next;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (rq-&gt;head == <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">\t\t\trq-&gt;tail = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t(*p)-&gt;next = <span class=\"literal\">NULL</span>;  <span class=\"comment\">// Ensure the dequeued PCB points to NULL</span></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Creat</span><span class=\"params\">(<span class=\"type\">int</span> n, <span class=\"keyword\">struct</span> ReadyQueue* rq)</span> &#123;<span class=\"comment\">//创建就绪队列</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; n; i++) &#123;</span><br><span class=\"line\">\t\tPCB* temp1 = new PCB;</span><br><span class=\"line\">\t\t <span class=\"comment\">// Declare temp inside the loop to create a new object for each process</span></span><br><span class=\"line\">\t\t<span class=\"comment\">//printf(&quot;Enter process name,arrive time and execution time: \\n&quot;);</span></span><br><span class=\"line\">\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;Enter process name,arrive time,execution time and priority: \\n&quot;</span>);</span><br><span class=\"line\">\t\tgetchar();</span><br><span class=\"line\">\t\t<span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%c&quot;</span>, &amp;temp1-&gt;name);</span><br><span class=\"line\">\t\t<span class=\"comment\">//scanf(&quot;%d%d&quot;, &amp;temp1-&gt;arrivetime, &amp;temp1-&gt;needtime);</span></span><br><span class=\"line\">\t\t<span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d%d%d&quot;</span>, &amp;temp1-&gt;arrivetime,&amp;temp1-&gt;needtime, &amp;temp1-&gt;priority);</span><br><span class=\"line\">\t\ttemp1-&gt;state = <span class=\"string\">&#x27;W&#x27;</span>;</span><br><span class=\"line\">\t\tEnQueue(rq, temp1);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Processin</span><span class=\"params\">(PCB** p, ReadyQueue* rq1, <span class=\"type\">int</span> i)</span> &#123;<span class=\"comment\">//进程进入就绪队列</span></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (*p &amp;&amp; (*p)-&gt;arrivetime &lt;= i) &#123;</span><br><span class=\"line\">\t\tPCB* temp1 = new PCB;</span><br><span class=\"line\">\t\ttemp1-&gt;arrivetime = (*p)-&gt;arrivetime;</span><br><span class=\"line\">\t\ttemp1-&gt;name = (*p)-&gt;name;</span><br><span class=\"line\">\t\ttemp1-&gt;state = (*p)-&gt;state;</span><br><span class=\"line\">\t\ttemp1-&gt;needtime = (*p)-&gt;needtime;</span><br><span class=\"line\">\t\ttemp1-&gt;priority = (*p)-&gt;priority;</span><br><span class=\"line\">\t\ttemp1-&gt;next = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (rq1-&gt;head == <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">\t\t\trq1-&gt;head = temp1;</span><br><span class=\"line\">\t\t\trq1-&gt;tail = temp1;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\trq1-&gt;tail-&gt;next = temp1;</span><br><span class=\"line\">\t\t\trq1-&gt;tail = temp1;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t(*p) = (*p)-&gt;next;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">PCB* <span class=\"title function_\">Getprior</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue rq)</span>&#123; <span class=\"comment\">//获得优先级最高的进程</span></span><br><span class=\"line\">\tPCB* p = rq.head; PCB* p1 = rq.head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (p) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (p-&gt;priority &lt; p1-&gt;priority)p1 = p;</span><br><span class=\"line\">\t\tp = p-&gt;next;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> p1;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Delete</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue* rq, PCB* p)</span> &#123;<span class=\"comment\">//删除节点</span></span><br><span class=\"line\">\tPCB* p1 = rq-&gt;head;PCB* p2;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (p) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (p1-&gt;name == p-&gt;name) Dequeue(rq, &amp;p2);</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (p1-&gt;name != p-&gt;name) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> (p1-&gt;next-&gt;name != p-&gt;name)</span><br><span class=\"line\">\t\t\t&#123;</span><br><span class=\"line\">\t\t\t\tp1 = p1-&gt;next;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (p1-&gt;next)p1-&gt;next = p1-&gt;next-&gt;next;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Print</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue rq)</span> &#123;<span class=\"comment\">//打印就绪队列</span></span><br><span class=\"line\">\tPCB* p = rq.head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (p)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">//printf(&quot;进程名:%c\\t到达时间:%d\\t所需时间:%d\\t进程状态:%c\\t\\n&quot;, p-&gt;name,p-&gt;arrivetime,p-&gt;needtime, p-&gt;state);</span></span><br><span class=\"line\">\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;进程名:%c\\t到达时间:%d\\t所需时间:%d\\t优先数:%d\\t进程状态:%c\\t\\n\\</span></span><br><span class=\"line\"><span class=\"string\">&quot;</span>, p-&gt;name, p-&gt;arrivetime, p-&gt;needtime,p-&gt;priority,p-&gt;state);</span><br><span class=\"line\">\t\tp = p-&gt;next;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"算法实现\"><a href=\"#算法实现\" class=\"headerlink\" title=\"算法实现\"></a>算法实现</h2><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">RR</span><span class=\"params\">(<span class=\"type\">int</span> n, <span class=\"keyword\">struct</span> ReadyQueue rq, ReadyQueue* rq1, <span class=\"keyword\">struct</span> ReadyQueue* rq2)</span> &#123;<span class=\"comment\">//时间片轮转</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> i = <span class=\"number\">0</span>,j=<span class=\"number\">1</span>;</span><br><span class=\"line\">\tPCB* p = rq.head;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (rq1-&gt;head != <span class=\"literal\">NULL</span> || p) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!rq1-&gt;head &amp;&amp; p-&gt;arrivetime - i &lt; n&amp;&amp; p-&gt;arrivetime-i&gt;<span class=\"number\">0</span>) i = p-&gt;arrivetime;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!rq1-&gt;head &amp;&amp; p-&gt;arrivetime &gt;= i + n) &#123; i += n; <span class=\"keyword\">continue</span>; j++; &#125;</span><br><span class=\"line\">\t\tProcessin(&amp;p, rq1, i);</span><br><span class=\"line\">\t\tPCB* temp;</span><br><span class=\"line\">\t\trq1-&gt;head-&gt;state = <span class=\"string\">&#x27;R&#x27;</span>;</span><br><span class=\"line\">\t\t<span class=\"type\">int</span> t = rq1-&gt;head-&gt;needtime;</span><br><span class=\"line\">\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;第%d个时间片\\n&quot;</span>, j);</span><br><span class=\"line\">\t\tPrint(*rq1);</span><br><span class=\"line\">\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;******************************\\n&quot;</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (rq1-&gt;head-&gt;needtime &gt; n) &#123; rq1-&gt;head-&gt;needtime -= n; rq1-&gt;head-&gt;state = <span class=\"string\">&#x27;W&#x27;</span>; rq1-&gt;head-&gt;servicetime += n; &#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\trq1-&gt;head-&gt;finishtime = i + t; rq1-&gt;head-&gt;servicetime += t;</span><br><span class=\"line\">\t\t\trq1-&gt;head-&gt;needtime = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t\trq1-&gt;head-&gt;state = <span class=\"string\">&#x27;F&#x27;</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tDequeue(rq1, &amp;temp); <span class=\"comment\">//printf(&quot;%d&quot;, temp-&gt;needtime);</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (temp-&gt;needtime) &#123;</span><br><span class=\"line\">\t\t\ti += n;</span><br><span class=\"line\">\t\t\tProcessin(&amp;p, rq1, i);</span><br><span class=\"line\">\t\t\tEnQueue(rq1, temp);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\ti += t;</span><br><span class=\"line\">\t\t\tEnQueue(rq2, temp);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tj++;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Preemptive</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue rq, ReadyQueue* rq1, <span class=\"keyword\">struct</span> ReadyQueue* rq2)</span>&#123; <span class=\"comment\">//抢占式</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\">\tPCB* p = rq.head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (rq1-&gt;head != <span class=\"literal\">NULL</span>||p) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!rq1-&gt;head) i = p-&gt;arrivetime;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span>(p)Processin(&amp;p, rq1, i);</span><br><span class=\"line\">\t\tPCB* p1 = Getprior(*rq1);</span><br><span class=\"line\">\t\tp1-&gt;state = <span class=\"string\">&#x27;R&#x27;</span>;</span><br><span class=\"line\">\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;第%d个时刻\\n&quot;</span>, i);</span><br><span class=\"line\">\t\tPrint(*rq1);</span><br><span class=\"line\">\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;******************************\\n&quot;</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (p1-&gt;needtime&gt;<span class=\"number\">1</span>) &#123; p1-&gt;state = <span class=\"string\">&#x27;W&#x27;</span>; p1-&gt;priority += <span class=\"number\">1</span>;&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\tp1-&gt;finishtime =i+<span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t\tp1-&gt;state = <span class=\"string\">&#x27;F&#x27;</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tp1-&gt;needtime -= <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\tp1-&gt;servicetime += <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!p1-&gt;needtime) &#123; Delete(rq1, p1); EnQueue(rq2, p1); &#125;</span><br><span class=\"line\">\t\t<span class=\"comment\">//Print(*rq1);</span></span><br><span class=\"line\">\t\ti++;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">NonPreemptive</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue rq, ReadyQueue* rq1, <span class=\"keyword\">struct</span> ReadyQueue* rq2)</span> &#123;<span class=\"comment\">//非抢占式</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\">\tPCB* p = rq.head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (rq1-&gt;head != <span class=\"literal\">NULL</span> || p) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!rq1-&gt;head) i = p-&gt;arrivetime;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (p)Processin(&amp;p, rq1, i);</span><br><span class=\"line\">\t\tPCB* p1 = Getprior(*rq1);</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (p1-&gt;needtime) &#123;</span><br><span class=\"line\">\t\t\tp1-&gt;state = <span class=\"string\">&#x27;R&#x27;</span>;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;第%d个时刻\\n&quot;</span>, i);</span><br><span class=\"line\">\t\t\tPrint(*rq1);</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;******************************\\n&quot;</span>);</span><br><span class=\"line\">\t\t\ti++;</span><br><span class=\"line\">\t\t\tp1-&gt;servicetime += <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t\tp1-&gt;state = <span class=\"string\">&#x27;W&#x27;</span>; </span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (p)Processin(&amp;p, rq1, i);</span><br><span class=\"line\">\t\t\tp1-&gt;needtime -= <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tp1-&gt;finishtime = i;</span><br><span class=\"line\">\t\tp1-&gt;state = <span class=\"string\">&#x27;F&#x27;</span>;</span><br><span class=\"line\">\t\tDelete(rq1, p1); EnQueue(rq2, p1);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Show</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue rq)</span> &#123;<span class=\"comment\">//打印周转时间</span></span><br><span class=\"line\">\tPCB* p = rq.head;</span><br><span class=\"line\">\t<span class=\"type\">int</span> i = <span class=\"number\">0</span>; <span class=\"type\">float</span> t = <span class=\"number\">0</span>, w = <span class=\"number\">0</span>,m;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (p)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\tm = (p-&gt;finishtime - p-&gt;arrivetime)*<span class=\"number\">1.0</span> / (p-&gt;servicetime);</span><br><span class=\"line\">\t\tt += p-&gt;finishtime - p-&gt;arrivetime;</span><br><span class=\"line\">\t\tw += m;</span><br><span class=\"line\">\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;进程名:%c\\t完成时间:%d\\t周转时间:%d\\t带权周转时间:%f\\t\\n&quot;</span>, p-&gt;name, p-&gt;finishtime, p-&gt;finishtime - p-&gt;arrivetime,m);</span><br><span class=\"line\">\t\tp = p-&gt;next;</span><br><span class=\"line\">\t\ti++;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d个进程的平均周转时间为%f，平均带权周转时间为%f&quot;</span>, i, t/i , w/ i);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\tReadyQueue rq, rq2; ReadyQueue rq1; initQueue(&amp;rq1);</span><br><span class=\"line\">\tinitQueue(&amp;rq); initQueue(&amp;rq2);</span><br><span class=\"line\">\t<span class=\"type\">int</span> n1, n2;</span><br><span class=\"line\">\t<span class=\"comment\">//scanf(&quot;%d%d&quot;, &amp;n1, &amp;n2);</span></span><br><span class=\"line\">\t<span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>, &amp;n1);</span><br><span class=\"line\">\tCreat(n1, &amp;rq);</span><br><span class=\"line\">\tNonPreemptive(rq, &amp;rq1, &amp;rq2);</span><br><span class=\"line\">\t<span class=\"comment\">//RR(n2, rq, &amp;rq1, &amp;rq2);</span></span><br><span class=\"line\">\tShow(rq2);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"时间片轮转算法演示\"><a href=\"#时间片轮转算法演示\" class=\"headerlink\" title=\"时间片轮转算法演示\"></a>时间片轮转算法演示</h2><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-1.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-2.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"非抢占式优先权调度算法\"><a href=\"#非抢占式优先权调度算法\" class=\"headerlink\" title=\"非抢占式优先权调度算法\"></a>非抢占式优先权调度算法</h2><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-3.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"实验总结\"><a href=\"#实验总结\" class=\"headerlink\" title=\"实验总结\"></a>实验总结</h1><p>通过这次实验，我们可以看出优先权调度算法和时间片轮转调度算法各有优缺点。优先权调度算法能够保证高优先级的进程优先得到执行，但可能会导致低优先级的进程饥饿；而时间片轮转调度算法能够保证所有的进程都有机会得到执行，但其效率可能会受到时间片长度选择的影响。因此，在实际应用中，需要根据具体的需求和环境来选择合适的调度算法。</p>\n","categories":["Operating System"]},{"title":"进程调度(下)","url":"/2023/12/01/oslab2/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h1><ul>\n<li>针对作业调度问题，能够分析影响作业调度性能的主要因素，通过设计最优的方案实现作业调度算法</li>\n<li>针对不同作业的要求，选择不同的调度算法，满足不同作业，尤其短作业运行的需求</li>\n</ul>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理</h1><h2 id=\"先来先服务调度算法：\"><a href=\"#先来先服务调度算法：\" class=\"headerlink\" title=\"先来先服务调度算法：\"></a>先来先服务调度算法：<span id=\"more\"></span></h2><p>按作业提交的&#x2F;到达的（到达后备队列的时间）先后次序从外存后备队列中选择几个最先进入该队列的作业为他们分配资源、创建进程，然后再放入就绪队列。</p>\n<p>每个作业由一个作业控制块JCB表示，JCB可以包含如下信息：作业名、提交时间、所需的运行时间、作业状态等等。<br>作业的状态可以是等待W(Wait)、运行R(Run)和完成F(Finish)三种状态之一。每个作业的最初状态总是等待W。 各个等待的作业按照提交时刻的先后次序排队。</p>\n<p>每个作业完成后要输出该作业的开始运行时刻、完成时刻、周转时间和带权周转时间，这一组作业完成后计算并输出这组作业的平均周转时间、平均带权周转时间。</p>\n<h2 id=\"短作业优先调度算法；\"><a href=\"#短作业优先调度算法；\" class=\"headerlink\" title=\"短作业优先调度算法；\"></a>短作业优先调度算法；</h2><p>根据作业的估计运行时间的长短，从外存后备队列中选择若干个作业为他们分配资源、创建进程，然后再放入就绪队列。</p>\n<p>每个作业由一个作业控制块JCB表示，JCB可以包含如下信息：作业名、提交时间、所需的运行时间、作业状态等等。</p>\n<p>作业的状态可以是等待W(Wait)、运行R(Run)和完成F(Finish)三种状态之一。每个作业的最初状态总是等待W。 各个等待的作业按照提交时刻的先后次序排队。</p>\n<p>每个作业完成后要输出该作业的开始运行时刻、完成时刻、周转时间和带权周转时间，这一组作业完成后计算并输出这组作业的平均周转时间、平均带权周转时间。</p>\n<h1 id=\"实验内容\"><a href=\"#实验内容\" class=\"headerlink\" title=\"实验内容\"></a>实验内容</h1><p>大部分代码与进程调度（上）相同，只需添加和修改部分代码</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">PCB* <span class=\"title function_\">Getshort</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue rq)</span> &#123;</span><br><span class=\"line\">\tPCB* p = rq.head; PCB* p1 = rq.head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (p) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (p-&gt;needtime &lt; p1-&gt;needtime)p1 = p;</span><br><span class=\"line\">\t\tp = p-&gt;next;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> p1;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">FCFS</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue rq, ReadyQueue* rq1, <span class=\"keyword\">struct</span> ReadyQueue* rq2)</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\">\tPCB* p = rq.head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (rq1-&gt;head != <span class=\"literal\">NULL</span> || p) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!rq1-&gt;head) i = p-&gt;arrivetime;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (p)Processin(&amp;p, rq1, i);</span><br><span class=\"line\">\t\tPCB* p1 = rq1-&gt;head;</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (p1-&gt;needtime) &#123;</span><br><span class=\"line\">\t\t\tp1-&gt;state = <span class=\"string\">&#x27;R&#x27;</span>;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;第%d个时刻\\n&quot;</span>, i);</span><br><span class=\"line\">\t\t\tPrint(*rq1);</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;******************************\\n&quot;</span>);</span><br><span class=\"line\">\t\t\ti++;</span><br><span class=\"line\">\t\t\tp1-&gt;servicetime += <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t\tp1-&gt;state = <span class=\"string\">&#x27;W&#x27;</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (p)Processin(&amp;p, rq1, i);</span><br><span class=\"line\">\t\t\tp1-&gt;needtime -= <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tp1-&gt;finishtime = i;</span><br><span class=\"line\">\t\tp1-&gt;state = <span class=\"string\">&#x27;F&#x27;</span>;</span><br><span class=\"line\">\t\tDelete(rq1, p1); EnQueue(rq2, p1);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">SJF</span><span class=\"params\">(<span class=\"keyword\">struct</span> ReadyQueue rq, ReadyQueue* rq1, <span class=\"keyword\">struct</span> ReadyQueue* rq2)</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\">\tPCB* p = rq.head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (rq1-&gt;head != <span class=\"literal\">NULL</span> || p) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!rq1-&gt;head) i = p-&gt;arrivetime;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (p)Processin(&amp;p, rq1, i);</span><br><span class=\"line\">\t\tPCB* p1 = Getshort(*rq1);</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (p1-&gt;needtime) &#123;</span><br><span class=\"line\">\t\t\tp1-&gt;state = <span class=\"string\">&#x27;R&#x27;</span>;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;第%d个时刻\\n&quot;</span>, i);</span><br><span class=\"line\">\t\t\tPrint(*rq1);</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;******************************\\n&quot;</span>);</span><br><span class=\"line\">\t\t\ti++;</span><br><span class=\"line\">\t\t\tp1-&gt;servicetime += <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t\tp1-&gt;state = <span class=\"string\">&#x27;W&#x27;</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (p)Processin(&amp;p, rq1, i);</span><br><span class=\"line\">\t\t\tp1-&gt;needtime -= <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tp1-&gt;finishtime = i;</span><br><span class=\"line\">\t\tp1-&gt;state = <span class=\"string\">&#x27;F&#x27;</span>;</span><br><span class=\"line\">\t\tDelete(rq1, p1); EnQueue(rq2, p1);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\tReadyQueue rq, rq2; ReadyQueue rq1; initQueue(&amp;rq1);</span><br><span class=\"line\">\tinitQueue(&amp;rq); initQueue(&amp;rq2);</span><br><span class=\"line\">\t<span class=\"type\">int</span> n1, n2;</span><br><span class=\"line\">\t<span class=\"comment\">//scanf(&quot;%d%d&quot;, &amp;n1, &amp;n2);</span></span><br><span class=\"line\">\t<span class=\"built_in\">scanf</span>(<span class=\"string\">&quot;%d&quot;</span>, &amp;n1);</span><br><span class=\"line\">\tCreat(n1, &amp;rq);</span><br><span class=\"line\">\t<span class=\"comment\">//NonPreemptive(rq, &amp;rq1, &amp;rq2);</span></span><br><span class=\"line\">\t<span class=\"comment\">//RR(n2, rq, &amp;rq1, &amp;rq2);</span></span><br><span class=\"line\">\t<span class=\"comment\">//FCFS(rq, &amp;rq1, &amp;rq2);</span></span><br><span class=\"line\">\tSJF(rq, &amp;rq1, &amp;rq2);</span><br><span class=\"line\">\tShow(rq2);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"先来先服务演示\"><a href=\"#先来先服务演示\" class=\"headerlink\" title=\"先来先服务演示\"></a>先来先服务演示</h2><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-4.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-5.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"短作业优先演示\"><a href=\"#短作业优先演示\" class=\"headerlink\" title=\"短作业优先演示\"></a>短作业优先演示</h2><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-6.png?raw=true\" alt=\"alt text\"><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-7.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"实验总结\"><a href=\"#实验总结\" class=\"headerlink\" title=\"实验总结\"></a>实验总结</h1><p>通过实验，我们可以观察到这两种算法在不同情况下的表现。例如，对于长作业和短作业混合的工作负载，SJF通常会提供更好的服务时间，因为它首先调度短作业。然而，如果所有作业的长度都相同，那么FCFS和SJF的性能就会相同。</p>\n","categories":["Operating System"]},{"title":"避免死锁","url":"/2023/12/08/oslab3/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h1><ul>\n<li>理解银行家算法</li>\n<li>掌握进程安全性检查的方法与资源分配的方法</li>\n</ul>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理</h1><h2 id=\"银行家算法\"><a href=\"#银行家算法\" class=\"headerlink\" title=\"银行家算法\"></a>银行家算法</h2><p>银行家算法最初级为银行系统设计，以确保银行在发放现金贷款时，不会发生不能满足所有客户需要的情况。在OS设计中，用它来避免死锁。</p>\n<p>为实现银行家算法，每个新进程在进入系统时它必须申明在运行过程中，可能需要的每种资源类型的最大单元数目，其数目不应超过系统所拥有的资源总量。当某一进程请求时，系统会自动判断请求量是否小于进程最大所需，同时判断请求量是否小于当前系统资源剩余量。若两项均满足，则系统试分配资源并执行安全性检查算法。</p>\n<p>算法流程图如下<span id=\"more\"></span><br><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-8.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"安全性检查算法\"><a href=\"#安全性检查算法\" class=\"headerlink\" title=\"安全性检查算法\"></a>安全性检查算法</h2><p>.安全性检查算法用于检查系统进行资源分配后是否安全，若安全系统才可以执行此次分配；若不安全，则系统不执行此次分配。<br>安全性检查算法原理为：在系统试分配资源后，算法从现有进程列表寻找出一个可执行的进程进行执行，执行完成后回收进程占用资源；进而寻找下一个可执行进程。当进程需求量大于系统可分配量时，进程无法执行。当所有进程均可执行，则产生一个安全执行序列，系统资源分配成功。若进程无法全部执行，即无法找到一条安全序列，则说明系统在分配资源后会不安全，所以此次分配失败。</p>\n<p>算法流程图如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-9.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"实验内容\"><a href=\"#实验内容\" class=\"headerlink\" title=\"实验内容\"></a>实验内容</h1><h2 id=\"算法实现\"><a href=\"#算法实现\" class=\"headerlink\" title=\"算法实现\"></a>算法实现</h2><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> res, pro;</span><br><span class=\"line\"><span class=\"type\">int</span> need[<span class=\"number\">10</span>][<span class=\"number\">10</span>], maxr[<span class=\"number\">10</span>][<span class=\"number\">10</span>], allocation[<span class=\"number\">10</span>][<span class=\"number\">10</span>], available[<span class=\"number\">10</span>],request[<span class=\"number\">10</span>];</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Compare</span><span class=\"params\">(<span class=\"type\">int</span> m[], <span class=\"type\">int</span> n[])</span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; res; i++)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (m[i] &lt; n[i])<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Print</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;            allocation            need              avilable&quot;</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; pro; i++) &#123;</span><br><span class=\"line\">\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&#x27;\\n&#x27;</span> &lt;&lt; <span class=\"string\">&quot;进程&quot;</span> &lt;&lt; i&lt;&lt; <span class=\"string\">&#x27;\\t&#x27;</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; res; j++)</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;  %2d &quot;</span>, allocation[i][j]);</span><br><span class=\"line\">\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;     &quot;</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; res; j++)</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;  %2d &quot;</span>, need[i][j]);</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!i)</span><br><span class=\"line\">\t\t&#123; </span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;     &quot;</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">for</span>(<span class=\"type\">int</span> j=<span class=\"number\">0</span>;j&lt;res;j++)</span><br><span class=\"line\">\t\t\t   <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;  %2d &quot;</span>, available[j]);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">init</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;请输入最大需求矩阵maxr\\n&quot;</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; pro; i++) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; res; j++) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cin</span> &gt;&gt; maxr[i][j];</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;请输入分配矩阵allocation\\n&quot;</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; pro; i++) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; res; j++) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cin</span> &gt;&gt; allocation[i][j];</span><br><span class=\"line\">\t\t\tneed[i][j] = maxr[i][j] - allocation[i][j];</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;请输入可用资源向量available\\n&quot;</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; res; i++) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cin</span> &gt;&gt; available[i];</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Safetytest</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> work[<span class=\"number\">10</span>], finish[<span class=\"number\">10</span>] = &#123; <span class=\"number\">0</span> &#125;, seq[<span class=\"number\">10</span>];</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; res; i++)</span><br><span class=\"line\">\t\twork[i] = available[i];</span><br><span class=\"line\">\t<span class=\"type\">int</span> i, k = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (k &lt; pro) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>; i &lt; pro; i++)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (!finish[i] &amp;&amp; Compare(work,need[i])) <span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (i == pro)<span class=\"keyword\">break</span>;<span class=\"comment\">//不安全</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; res; j++)</span><br><span class=\"line\">\t\t\twork[j] +=  allocation[i][j];</span><br><span class=\"line\">\t\tfinish[i] = <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\tseq[k] = i; k++;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (k == pro) &#123;</span><br><span class=\"line\">\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;安全序列为:&quot;</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; pro; i++)</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cout</span> &lt;&lt; seq[i] &lt;&lt; <span class=\"string\">&#x27;\\t&#x27;</span>;</span><br><span class=\"line\">\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Banker</span><span class=\"params\">(<span class=\"type\">int</span> n)</span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (Compare(need[n],request)) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (Compare(request, available))<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;无足够资源&quot;</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>&#123;<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; res; i++) &#123;</span><br><span class=\"line\">\t\t\tavailable[i] -= request[i];</span><br><span class=\"line\">\t\t\tallocation[n][i] += request[i];</span><br><span class=\"line\">\t\t\tneed[n][i] -= request[i];</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!Safetytest()) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; res; i++) &#123;</span><br><span class=\"line\">\t\t\t\tavailable[i] += request[i];</span><br><span class=\"line\">\t\t\t\tallocation[n][i] -= request[i];</span><br><span class=\"line\">\t\t\t\tneed[n][i] += request[i];</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;不安全的状态&quot;</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;允许分配资源&quot;</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t\t\tPrint();</span><br><span class=\"line\">\t\t&#125;&#125;</span><br><span class=\"line\">\t&#125;<span class=\"keyword\">else</span> <span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;请求资源大于所需资源&quot;</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> n;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;输入进程数和资源数&quot;</span>&lt;&lt;<span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">cin</span> &gt;&gt; pro&gt;&gt;res;</span><br><span class=\"line\">\tinit();</span><br><span class=\"line\">\tPrint();</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;输入进程号:&quot;</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">cin</span> &gt;&gt; n;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;请求向量:&quot;</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; res; i++)</span><br><span class=\"line\">\t\t<span class=\"built_in\">cin</span> &gt;&gt; request[i];</span><br><span class=\"line\">\tBanker(n);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"内容演示\"><a href=\"#内容演示\" class=\"headerlink\" title=\"内容演示\"></a>内容演示</h2><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-10.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"实验总结\"><a href=\"#实验总结\" class=\"headerlink\" title=\"实验总结\"></a>实验总结</h1><p>银行家算法是一种用来避免操作系统死锁出现的有效算法。</p>\n<p>死锁：是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。</p>\n<p>死锁的发生必须具备以下四个必要条件：</p>\n<ul>\n<li>互斥条件：指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。</li>\n<li>请求和保持条件：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。</li>\n<li>不抢占条件：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。</li>\n<li>循环等待条件：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合$\\lbrace P_0，P_1，P_2，···，P_n\\rbrace$中的$P_0$正在等待一个$P_1$占用的资源；$P_1$正在等待$P_2$占用的资源，……，$P_n$正在等待已被$P_0$占用的资源。</li>\n<li>银行家算法是避免死锁的一种重要方法，防止死锁的机构只能确保上述四个条件之一不出现，则系统就不会发生死锁。</li>\n</ul>\n<p>为实现银行家算法，系统必须设置若干数据结构，同时要解释银行家算法，必须先解释操作系统安全状态和不安全状态。</p>\n<p>安全序列:是指一个进程序列$\\lbrace P_1，···，P_n\\rbrace$是安全的，即对于每一个进程$P_i (1≤i≤n)$，它以后尚需要的资源量不超过系统当前剩余资源量与所有进程$P_j (j &lt; i )$当前占有资源量之和。<br>安全状态：如果存在一个由系统中所有进程构成的安全序列$P_1，…，P_n$，则系统处于安全状态。</p>\n<p>安全状态一定是没有死锁发生。不安全状态：不存在一个安全序列。不安全状态不一定导致死锁</p>\n","categories":["Operating System"]},{"title":"虚拟存储","url":"/2023/12/14/oslab4/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h1><p>页面置换算法也称为页面淘汰算法，是用来选择换出页面的算法。</p>\n<ul>\n<li>解决：需要调入页面时，选择内存中哪个或哪些物理页面被置换</li>\n<li>目标：把未来不再使用的或在以后一段时间内较少使用的页面调出</li>\n</ul>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理</h1><h2 id=\"先进先出算法\"><a href=\"#先进先出算法\" class=\"headerlink\" title=\"先进先出算法\"></a>先进先出算法</h2><p>基本思想是淘汰最先进入内存的页面，即选择在内存驻留时间最长的页面予以淘汰。实现简单。按页面调入内存的先后链结为队列，设置一个替换指针，总是指向最先进入内存的页面。缺点在与进程实际运行规律不符，性能不好。</p>\n<p>算法流程图如下<span id=\"more\"></span></p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-11.png?raw=true\" alt=\"alt text\"></p>\n<h2 id=\"最佳页面置换算法\"><a href=\"#最佳页面置换算法\" class=\"headerlink\" title=\"最佳页面置换算法\"></a>最佳页面置换算法</h2><p>基本思想是所选择的被淘汰页面，将是以后永不使用的，或是在最长(未来)时间内不再被访问的页面。采用最佳置换算法，可保证获得最低的缺页率。</p>\n<p>算法流程图如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-12.png?raw=true\"></p>\n<h2 id=\"最近最久未使用页面置换算法\"><a href=\"#最近最久未使用页面置换算法\" class=\"headerlink\" title=\"最近最久未使用页面置换算法\"></a>最近最久未使用页面置换算法</h2><p>基本思想：最近未访问的页面，将来一段时间也不会访问。利用局部性原理，根据一个进程在执行过程中过去的页面访问踪迹来推测未来的行为。最近的过去 → 最近的将来     思想：选择最近最久未使用的页面予以淘汰。利用页表中的访问字段，记录页面自上次被访问以来所经历的时间t，需要淘汰页面时，选择在内存页面中t值最大的，即最近最久未使用的页面予以淘汰</p>\n<p>算法流程图如下</p>\n<p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-13.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"实验内容\"><a href=\"#实验内容\" class=\"headerlink\" title=\"实验内容\"></a>实验内容</h1><h2 id=\"算法实现\"><a href=\"#算法实现\" class=\"headerlink\" title=\"算法实现\"></a>算法实现</h2><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> n,j=<span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">page</span> &#123;</span></span><br><span class=\"line\">\t<span class=\"type\">int</span> num;</span><br><span class=\"line\">\t<span class=\"type\">int</span> visit=<span class=\"number\">65535</span>;</span><br><span class=\"line\">\t<span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">page</span>* <span class=\"title\">next</span> =</span> <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">page</span> <span class=\"title\">Page</span>;</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">PageQueue</span></span></span><br><span class=\"line\"><span class=\"class\">&#123;</span></span><br><span class=\"line\">\tPage* head;</span><br><span class=\"line\">\tPage* tail;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">initQueue</span><span class=\"params\">(<span class=\"keyword\">struct</span> PageQueue* rq)</span> &#123;</span><br><span class=\"line\">\trq-&gt;head = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">\trq-&gt;tail = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">EnQueue</span><span class=\"params\">(<span class=\"keyword\">struct</span> PageQueue* rq, Page* p)</span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (!rq-&gt;head) &#123;</span><br><span class=\"line\">\t\trq-&gt;head = p;</span><br><span class=\"line\">\t\trq-&gt;tail = p;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\trq-&gt;tail-&gt;next = p;</span><br><span class=\"line\">\t\trq-&gt;tail = p;</span><br><span class=\"line\">\t\tp-&gt;next = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Dequeue</span><span class=\"params\">(<span class=\"keyword\">struct</span> PageQueue* rq, Page** p)</span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (rq-&gt;head != <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">\t\t*p = rq-&gt;head;</span><br><span class=\"line\">\t\trq-&gt;head = rq-&gt;head-&gt;next;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (rq-&gt;head == <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">\t\t\trq-&gt;tail = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t(*p)-&gt;next = <span class=\"literal\">NULL</span>;  <span class=\"comment\">// Ensure the dequeued PCB points to NULL</span></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Find</span><span class=\"params\">(<span class=\"type\">int</span> n1, <span class=\"keyword\">struct</span> PageQueue rq)</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\">\tPage* p = rq.head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span>(p)&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (p-&gt;num == n1) <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\tp = p-&gt;next;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Getleast</span><span class=\"params\">(<span class=\"keyword\">struct</span> PageQueue* rq, <span class=\"type\">int</span> a[])</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> maxv = <span class=\"number\">0</span>, num=<span class=\"number\">0</span>;</span><br><span class=\"line\">\tPage* q = rq-&gt;head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (q) &#123;</span><br><span class=\"line\">\t\t<span class=\"type\">int</span> k = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (a[k] != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (q-&gt;num == a[k]) &#123; q-&gt;visit = k; <span class=\"keyword\">break</span>; &#125;</span><br><span class=\"line\">\t\t\tk++;</span><br><span class=\"line\">\t\t&#125;<span class=\"keyword\">if</span> (maxv &lt; q-&gt;visit) &#123; maxv = q-&gt;visit; num = q-&gt;num; &#125;</span><br><span class=\"line\">\t\tq = q-&gt;next;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> num;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Getleastr</span><span class=\"params\">(<span class=\"keyword\">struct</span> PageQueue* rq)</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> num = <span class=\"number\">0</span>,minv=<span class=\"number\">65535</span>;</span><br><span class=\"line\">\tPage* q = rq-&gt;head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (q) &#123;</span><br><span class=\"line\">\t\t<span class=\"type\">int</span> k = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (minv &gt; q-&gt;visit) &#123; minv = q-&gt;visit; num = q-&gt;num; &#125;</span><br><span class=\"line\">\t\tq = q-&gt;next;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> num;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Delete</span><span class=\"params\">(<span class=\"keyword\">struct</span> PageQueue* rq, <span class=\"type\">int</span> num)</span> &#123;</span><br><span class=\"line\">\tPage* p1 = rq-&gt;head; Page* p2;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (p1-&gt;num ==num) Dequeue(rq, &amp;p2);</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (p1-&gt;num != num) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> (p1-&gt;next-&gt;num != num)</span><br><span class=\"line\">\t\t\t&#123;</span><br><span class=\"line\">\t\t\t\tp1 = p1-&gt;next;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (p1-&gt;next == rq-&gt;tail)rq-&gt;tail = p1;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (p1-&gt;next)p1-&gt;next = p1-&gt;next-&gt;next;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Print</span><span class=\"params\">(<span class=\"keyword\">struct</span> PageQueue rq)</span> &#123;</span><br><span class=\"line\">\tPage* p = rq.head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (p) &#123;</span><br><span class=\"line\">\t\t<span class=\"built_in\">cout</span> &lt;&lt; p-&gt;num &lt;&lt; <span class=\"string\">&#x27;\\t&#x27;</span>;</span><br><span class=\"line\">\t\tp = p-&gt;next;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">FIFO</span><span class=\"params\">(<span class=\"keyword\">struct</span> PageQueue* rq, <span class=\"type\">int</span> a[])</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> i = <span class=\"number\">0</span>,count=<span class=\"number\">0</span>; </span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (a[i] != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">\t\tPage* p = new page;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!Find(a[i], *rq)) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;在&quot;</span> &lt;&lt; a[i] &lt;&lt; <span class=\"string\">&quot;处产生缺页中断&quot;</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(count&gt;=n)Dequeue(rq, &amp;p); </span><br><span class=\"line\">\t\t\tp-&gt;num = a[i]; p-&gt;next = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">\t\t\tEnQueue(rq, p);</span><br><span class=\"line\">\t\t\tPrint(*rq);</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t\t\tcount++;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\ti++;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;缺页次数&quot;</span> &lt;&lt; count &lt;&lt; <span class=\"string\">&quot;缺页率&quot;</span> &lt;&lt; count * <span class=\"number\">1.0</span> / j &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">OPT</span><span class=\"params\">(<span class=\"keyword\">struct</span> PageQueue* rq, <span class=\"type\">int</span> a[])</span>&#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> i = <span class=\"number\">0</span>, count = <span class=\"number\">0</span>,num1=<span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (a[i] != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">\t\tPage* p = new page;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!Find(a[i], *rq)) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;在&quot;</span> &lt;&lt; a[i] &lt;&lt; <span class=\"string\">&quot;处产生缺页中断&quot;</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (count &gt;= n) &#123;</span><br><span class=\"line\">\t\t\t\tnum1 = Getleast(rq, a);</span><br><span class=\"line\">\t\t\t\tDelete(rq, num1);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tp-&gt;num = a[i]; p-&gt;next = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">\t\t\tEnQueue(rq, p);</span><br><span class=\"line\">\t\t\tPrint(*rq);</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t\t\tcount++;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\ti++;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;缺页次数&quot;</span> &lt;&lt; count &lt;&lt; <span class=\"string\">&quot;缺页率&quot;</span> &lt;&lt; count * <span class=\"number\">1.0</span> / j &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">ChangeV</span><span class=\"params\">(<span class=\"keyword\">struct</span> PageQueue* rq, <span class=\"type\">int</span> a, <span class=\"type\">int</span> i)</span> &#123;</span><br><span class=\"line\">\tPage* q = rq-&gt;head;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (q-&gt;num != a)</span><br><span class=\"line\">\t\tq = q-&gt;next;</span><br><span class=\"line\">\tq-&gt;visit = i;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">LRU</span><span class=\"params\">(<span class=\"keyword\">struct</span> PageQueue* rq, <span class=\"type\">int</span> a[])</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> i = <span class=\"number\">0</span>, count = <span class=\"number\">0</span>,num;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (a[i] != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">\t\tPage* p = new page;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!Find(a[i], *rq)) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;在&quot;</span> &lt;&lt; a[i] &lt;&lt; <span class=\"string\">&quot;处产生缺页中断&quot;</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (count &gt;= n) &#123;</span><br><span class=\"line\">\t\t\t\tnum = Getleastr(rq);</span><br><span class=\"line\">\t\t\t\tDelete(rq, num);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tp-&gt;num = a[i]; p-&gt;next = <span class=\"literal\">NULL</span>; p-&gt;visit = i;</span><br><span class=\"line\">\t\t\tEnQueue(rq, p);</span><br><span class=\"line\">\t\t\tPrint(*rq);</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t\t\tcount++;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\tChangeV(rq, a[i], i);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\ti++;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;缺页次数&quot;</span> &lt;&lt; count &lt;&lt; <span class=\"string\">&quot;缺页率&quot;</span> &lt;&lt; count * <span class=\"number\">1.0</span> / j &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> a[<span class=\"number\">30</span>] = &#123; <span class=\"number\">0</span> &#125;,t,i=<span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;主存块数&quot;</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">cin</span> &gt;&gt; n;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;页面号&quot;</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (<span class=\"built_in\">cin</span>.get() != <span class=\"string\">&#x27;\\n&#x27;</span>||j==<span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">\t\t<span class=\"built_in\">cin</span> &gt;&gt; a[j];</span><br><span class=\"line\">\t\tj++;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tPageQueue pg;</span><br><span class=\"line\">\tinitQueue(&amp;pg);</span><br><span class=\"line\">\t<span class=\"comment\">//FIFO(&amp;pg, a);</span></span><br><span class=\"line\">\t<span class=\"comment\">//OPT(&amp;pg, a);</span></span><br><span class=\"line\">\tLRU(&amp;pg, a);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"内容演示\"><a href=\"#内容演示\" class=\"headerlink\" title=\"内容演示\"></a>内容演示</h2><h3 id=\"FIFO\"><a href=\"#FIFO\" class=\"headerlink\" title=\"FIFO\"></a>FIFO</h3><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-14.png?raw=true\" alt=\"alt text\"></p>\n<h3 id=\"最佳置换算法\"><a href=\"#最佳置换算法\" class=\"headerlink\" title=\"最佳置换算法\"></a>最佳置换算法</h3><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-15.png?raw=true\"></p>\n<h3 id=\"LRU\"><a href=\"#LRU\" class=\"headerlink\" title=\"LRU\"></a>LRU</h3><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-16.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"实验总结\"><a href=\"#实验总结\" class=\"headerlink\" title=\"实验总结\"></a>实验总结</h1><p>通过本次实验加深了我对页面置换算法的理解，本次实验中的页面置换算法有<br>FIFO（先进先出置换算法）、LRU（最近最少使用页面置换算法）、OPT（最佳置换算法）。</p>\n<p>FIFO算法的核心思想是置换最先调入内存的页面，即置换在内存中驻留时间最久的页面。按照进入内存的先后次序排列成队列，从队尾进入，从队首删除。但是该算法会淘汰经常访问的页面，不适应进程实际运行的规律，目前已经很少使用。</p>\n<p>LRU算法的核心思想是置换最近一段时间以来最长时间未访问过的页面。根据程序局部性原理，刚被访问的页面，可能马上又要被访问；而较长时间内没有被访问的页面，可能最近不会被访问。LRU算法普偏地适用于各种类型的程序，但是系统要时时刻刻对各页的访问历史情况加以记录和更新，开销太大，因此LRU算法必须要有硬件的支持。</p>\n<p>OPT算法的核心思想是置换以后不再被访问，或者在将来最迟才回被访问的页面，缺页中断率最低。缺点：该算法需要依据以后各业的使用情况，而当一个进程还未运行完成是，很难估计哪一个页面是以后不再使用或在最长时间以后才会用到的页面。</p>\n<p>这三种算法对比：OPT &gt; LRU &gt; FIFO 但是OPT算法是不能实现的，但该算法仍然有意义，可以作为其他算法优劣的一个标准。</p>\n","categories":["Operating System"]},{"title":"磁盘调度","url":"/2023/12/22/oslab5/","content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"\\assets\\css\\APlayer.min.css\"><script src=\"\\assets\\js\\APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"\\assets\\js\\Meting.min.js\"></script><h1 id=\"磁盘调度\"><a href=\"#磁盘调度\" class=\"headerlink\" title=\"磁盘调度\"></a>磁盘调度</h1><p>磁盘调度中寻道时间直接影响到数据访问的快慢，处理好磁盘寻道时间是关键</p>\n<h1 id=\"实验原理\"><a href=\"#实验原理\" class=\"headerlink\" title=\"实验原理\"></a>实验原理</h1><h2 id=\"先来先服务-FCFS\"><a href=\"#先来先服务-FCFS\" class=\"headerlink\" title=\"先来先服务(FCFS)\"></a>先来先服务(FCFS)</h2><p>基本思想：按照输入输出请求到达的顺序，逐一完成访问请求，它只考虑请求访问者的先后次序，而不考虑它们要访问的物理位置</p>\n<h2 id=\"最短寻道时间优先-SSTF\"><a href=\"#最短寻道时间优先-SSTF\" class=\"headerlink\" title=\"最短寻道时间优先(SSTF)\"></a>最短寻道时间优先(SSTF)</h2><p>基本思想：先对最靠近当前柱面位置的请求进行服务，即先对寻找时间最短的请求进行服务。SSTF算法总是让寻找时间最短的那个请求先服务，而不管请求访问者到来的先后次序。</p>\n<h1 id=\"实验内容\"><a href=\"#实验内容\" class=\"headerlink\" title=\"实验内容\"></a>实验内容</h1><h2 id=\"算法实现\"><a href=\"#算法实现\" class=\"headerlink\" title=\"算法实现\"></a>算法实现<span id=\"more\"></span></h2><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\">using namespace <span class=\"built_in\">std</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">Getmin</span><span class=\"params\">(<span class=\"type\">int</span> a[],<span class=\"type\">int</span> i,<span class=\"type\">int</span> temp)</span> &#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> min = <span class=\"number\">65535</span>,mini=<span class=\"number\">0</span>,j=<span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (i&gt;<span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (<span class=\"built_in\">abs</span>(a[j]-temp) &lt;= <span class=\"built_in\">abs</span>(min-temp)) &#123; min = a[j]; mini = j; &#125;</span><br><span class=\"line\">\t\tj++;</span><br><span class=\"line\">\t\ti--;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\ta[mini] = <span class=\"number\">65536</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> min;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">FCFS</span><span class=\"params\">(<span class=\"type\">int</span> a[])</span> &#123;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;被访问的下一个磁盘号&quot;</span> &lt;&lt; <span class=\"string\">&quot;\\t\\t&quot;</span> &lt;&lt; <span class=\"string\">&quot;移动距离&quot;</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t<span class=\"type\">int</span> i = <span class=\"number\">0</span>,temp=<span class=\"number\">100</span>,count=<span class=\"number\">0</span>,dis=<span class=\"number\">0</span>; </span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (a[i] != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">\t\tdis= <span class=\"built_in\">abs</span>(temp - a[i]);</span><br><span class=\"line\">\t\ttemp = a[i];</span><br><span class=\"line\">\t\t<span class=\"built_in\">cout</span> &lt;&lt; a[i] &lt;&lt; <span class=\"string\">&quot;\\t\\t\\t\\t&quot;</span> &lt;&lt; dis &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t\tcount += dis;</span><br><span class=\"line\">\t\ti++;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;平均寻道长度&quot;</span> &lt;&lt; count * <span class=\"number\">1.0</span> / i &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">SSTF</span><span class=\"params\">(<span class=\"type\">int</span> a[],<span class=\"type\">int</span> j)</span> &#123;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;被访问的下一个磁盘号&quot;</span> &lt;&lt;<span class=\"string\">&quot;\\t\\t&quot;</span> &lt;&lt; <span class=\"string\">&quot;移动距离&quot;</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t<span class=\"type\">int</span> i = j, temp = <span class=\"number\">100</span>, count = <span class=\"number\">0</span>, dis = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (j&gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">\t\t<span class=\"type\">int</span> ele = Getmin(a, i,temp);</span><br><span class=\"line\">\t\tdis = <span class=\"built_in\">abs</span>(temp - ele);</span><br><span class=\"line\">\t\ttemp = ele;</span><br><span class=\"line\">\t\t<span class=\"built_in\">cout</span> &lt;&lt; ele &lt;&lt; <span class=\"string\">&quot;\\t\\t\\t\\t&quot;</span> &lt;&lt; dis &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">\t\tcount += dis;</span><br><span class=\"line\">\t\tj--;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">&quot;平均寻道长度&quot;</span> &lt;&lt; count * <span class=\"number\">1.0</span> / i &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"type\">int</span> a[<span class=\"number\">30</span>] = &#123;<span class=\"number\">0</span> &#125;,i=<span class=\"number\">0</span>,j=<span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (j==<span class=\"number\">0</span>||<span class=\"built_in\">cin</span>.get() != <span class=\"string\">&#x27;\\n&#x27;</span>) &#123;</span><br><span class=\"line\">\t\t<span class=\"built_in\">cin</span> &gt;&gt; a[j];</span><br><span class=\"line\">\t\tj++;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"comment\">//FCFS(a);</span></span><br><span class=\"line\">\tSSTF(a,j);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"内容演示\"><a href=\"#内容演示\" class=\"headerlink\" title=\"内容演示\"></a>内容演示</h2><h3 id=\"FCFS\"><a href=\"#FCFS\" class=\"headerlink\" title=\"FCFS\"></a>FCFS</h3><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-17.png?raw=true\"></p>\n<h3 id=\"SSTF\"><a href=\"#SSTF\" class=\"headerlink\" title=\"SSTF\"></a>SSTF</h3><p><img src=\"https://github.com/KianaMeiZaychik/KianaMeiZaychik.github.io/blob/master/photo/os-18.png?raw=true\" alt=\"alt text\"></p>\n<h1 id=\"实验总结\"><a href=\"#实验总结\" class=\"headerlink\" title=\"实验总结\"></a>实验总结</h1><h2 id=\"先来先服务算法（FCFS）First-Come-First-Service\"><a href=\"#先来先服务算法（FCFS）First-Come-First-Service\" class=\"headerlink\" title=\"先来先服务算法（FCFS）First Come First Service\"></a>先来先服务算法（FCFS）First Come First Service</h2><p>这是一种比较简单的磁盘调度算法。它根据进程请求访问磁盘的先后次序进行调度。此算法的优点是公平、简单，且每个进程的请求都能依次得到处理，不会出现某一进程的请求长期得不到满足的情况。此算法由于未对寻道进行优化，在对磁盘的访问请求比较多的情况下，此算法将降低设备服务的吞吐量，致使平均寻道时间可能较长，但各进程得到服务的响应时间的变化幅度较小。</p>\n<h2 id=\"最短寻道时间优先算法（SSTF）-Shortest-Seek-Time-First\"><a href=\"#最短寻道时间优先算法（SSTF）-Shortest-Seek-Time-First\" class=\"headerlink\" title=\"最短寻道时间优先算法（SSTF） Shortest Seek Time First\"></a>最短寻道时间优先算法（SSTF） Shortest Seek Time First</h2><p>该算法选择这样的进程，其要求访问的磁道与当前磁头所在的磁道距离最近，以使每次的寻道时间最短，该算法可以得到比较好的吞吐量，但却不能保证平均寻道时间最短。其缺点是对用户的服务请求的响应机会不是均等的，因而导致响应时间的变化幅度很大。在服务请求很多的情况下，对内外边缘磁道的请求将会无限期的被延迟，有些请求的响应时间将不可预期。</p>\n","categories":["Operating System"]}]